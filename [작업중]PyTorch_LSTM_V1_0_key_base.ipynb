{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_LSTM_V1_0_key_base.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1NliHLfVy-a0N6Ok5K3HAny5_1nwBLTgJ","authorship_tag":"ABX9TyOB2hp5HIMSMt2cnKg9AK+s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALDi87Pc5OeI","executionInfo":{"status":"ok","timestamp":1655434219384,"user_tz":-540,"elapsed":10764,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"a9fb8cd1-73bb-45aa-e785-14c894125ee5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 12.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Collecting alembic\n","  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 64.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Collecting Mako\n","  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n","Collecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n","\u001b[K     |████████████████████████████████| 146 kB 68.8 MB/s \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.5 MB/s \n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 58.0 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.2.0)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=7816282617b2d026f33bbe9e277b35097fb2cabe288b5db89e71229bb0645514\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.0 alembic-1.8.0 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 optuna-2.10.1 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PJ8QC3wS1Rd9","executionInfo":{"status":"ok","timestamp":1655434233791,"user_tz":-540,"elapsed":5667,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","\n","from torch.autograd import Variable\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import joblib\n","import pickle\n","import optuna\n","\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/Colab Notebooks/실무인증/Data/crypto_currency_data_key_v2.pickle', 'rb') as f:\n","    data = pickle.load(f)\n","\n","\n","data = data[['date', 'trade_price']]\n","data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n","data = data.sort_values(by='date').reset_index(drop=True)\n","\n","scaler = MinMaxScaler()\n","scale_cols = ['trade_price']\n","\n","# 예측 데이터 사이즈\n","max_prediction_length = 20\n","\n","\n","# 학습용 데이터\n","# 전체 데이터의 70%만 사용\n","data_p = data.iloc[:int(data.index.max() * .7), :]\n","training_data = scaler.fit_transform(data_p[scale_cols])\n","\n","# # 학습용 데이터\n","# data_p = data.iloc[:data.index.max() - max_prediction_length +1, :]\n","# training_data = scaler.fit_transform(data_p[scale_cols])\n","\n","# max_prediction_length 만큼의 데이터는 예측 데이터와 비교를 위해 분리\n","actual_data = data.loc[~data.index.isin(data_p.index)][scale_cols]\n","actual_data.shape\n","\n"],"metadata":{"id":"6h3QCVaQ1oFF","executionInfo":{"status":"ok","timestamp":1655434358274,"user_tz":-540,"elapsed":408,"user":{"displayName":"이기","userId":"02906280205536551697"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"034fc979-1f2f-438d-c5f8-a08682917071"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(513, 1)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["training_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXhsBBf7qL-u","executionInfo":{"status":"ok","timestamp":1655435741783,"user_tz":-540,"elapsed":411,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"756681a2-386b-475f-8133-9f9e44371fb3"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1194, 1)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6Sviy2AqT80","executionInfo":{"status":"ok","timestamp":1655435774418,"user_tz":-540,"elapsed":716,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"d296b99d-4047-4b10-baf7-c8f848b7bb05"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1707, 2)"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["def sliding_windows(data, lookback_length, forecast_length):\n","\n","    x = []\n","    y = []\n","    \n","    for i in range(lookback_length, len(data) - forecast_length + 1):\n","        _x = data[(i-lookback_length) : i]\n","        _y = data[i : (i + forecast_length)]\n","        x.append(_x)\n","        y.append(_y)\n","    return np.array(x), np.array(y)\n","\n","\n","def get_data_loader(X, y, batch_size):\n","\n","    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n","\n","    train_ds = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n","    train_dl = DataLoader(train_ds, batch_size = batch_size)\n","\n","    val_ds = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n","    val_dl = DataLoader(val_ds, batch_size = batch_size)\n","\n","    input_size = x_train.shape[-1]\n","\n","    return train_dl, val_dl, input_size"],"metadata":{"id":"N7ckUYfEZ2tP","executionInfo":{"status":"ok","timestamp":1655434364362,"user_tz":-540,"elapsed":523,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n","        super(LSTM, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        \n","        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n","                            num_layers = num_layers, batch_first = True)\n","        \n","        self.fc = nn.Linear(hidden_size * num_layers , num_classes)\n","        \n","    def forward(self, x):\n","        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n","        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n","        \n","        # Propagate input through LSTM\n","        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n","        h_out = h_out.view(-1, self.hidden_size * self.num_layers)\n","        \n","        out = self.fc(h_out)\n","        \n","        return out"],"metadata":{"id":"oBWe5Qb9RIin","executionInfo":{"status":"ok","timestamp":1655434365417,"user_tz":-540,"elapsed":2,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(log_interval, model, train_dl, val_dl, optimizer, criterion, epoch):\n","\n","    best_loss = np.inf\n","    for epoch in range(epoch):\n","        train_loss = 0.0\n","        model.train()\n","        for data, target in train_dl:\n","\n","            if torch.cuda.is_available():\n","                data, target = data.cuda(), target.cuda()\n","                model = model.cuda()\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target) # mean-squared error for regression\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # validation\n","        valid_loss = 0.0\n","        model.eval()\n","        for data, target in val_dl:\n","\n","            if torch.cuda.is_available():\n","                data, target = data.cuda(), target.cuda()\n","\n","            output = model(data)\n","            loss = criterion(output, target)\n","            valid_loss += loss.item()\n","\n","        if ( epoch % log_interval == 0 ):\n","            print(f'\\n Epoch {epoch} \\t Training Loss: {train_loss / len(train_dl)} \\t Validation Loss: {valid_loss / len(val_dl)} \\n')\n","\n","        if best_loss > (valid_loss / len(val_dl)):\n","            print(f'Validation Loss Decreased({best_loss:.6f}--->{(valid_loss / len(val_dl)):.6f}) \\t Saving The Model')\n","            best_loss = (valid_loss / len(val_dl))\n","            torch.save(model.state_dict(), 'lstm_saved_model.pth')\n","\n","    return best_loss\n","\n","\n","def smape(a, f):\n","    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)"],"metadata":{"id":"R1qLb-u0eOj2","executionInfo":{"status":"ok","timestamp":1655434368597,"user_tz":-540,"elapsed":464,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def objective(trial):\n","\n","    cfg = { 'batch_size' : trial.suggest_categorical('batch_size',[64, 128, 256, 512]),\n","            'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-4, 1e-0), #trial.suggest_loguniform('learning_rate', 1e-2, 1e-1), # learning rate을 0.01-0.1까지 로그 uniform 분포로 사용          \n","            # 'activation': trial.suggest_categorical('activation',[ torch.nn.relu6, torch.nn.tanh ]),\n","            'hidden_size': trial.suggest_categorical('hidden_size', [16, 32, 64,128,256,512,1024]),\n","            'num_layers': trial.suggest_int('num_layers', 1, 5, 1) }\n","\n","    torch.manual_seed(42)\n","\n","    lookback_length = 90\n","    forecast_length = max_prediction_length\n","    log_interval = 10\n","    num_classes = 1 # parameter에서 빼서 상수로 설정\n","    num_epochs = 150 # parameter에서 빼서 상수로 설정\n","\n","    x, y = sliding_windows(training_data, lookback_length, forecast_length)\n","\n","    train_dl, val_dl, input_size = get_data_loader(x, y[:, 0, :],  cfg['batch_size'])\n","    model = LSTM(num_classes=num_classes, input_size=input_size, hidden_size=cfg['hidden_size'], num_layers=cfg['num_layers'])\n","\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n","    criterion = torch.nn.MSELoss()\n","    best_loss = train(log_interval, model, train_dl, val_dl, optimizer, criterion,  num_epochs)\n","\n","    # print('best loss for the trial = ', best_loss)\n","    predict_data = []\n","    actual_data = []\n","\n","    # x_pred = x[-1:, :, :]  # last observed input sequence\n","    # y_pred = y[-1]         # last observed target value\n","\n","    # 여기서 x는 (sample, lookback_length, 1)의 크기를 지님. 따라서, 제일 앞의 시점을 제거하려면, x[:, -1, :]이 되어야 함\n","    x_pred = x  # Inference에 사용할 lookback data를 x_pred로 지정. 앞으로 x_pred를 하나씩 옮겨 가면서 inference를 할 예정\n","\n","    # print('-----------------------y shape before loop = ', y.shape)\n","    for j, i in enumerate(range(max_prediction_length - 1)):\n","\n","        # feed the last forecast back to the model as an input\n","        # print(f'j = {j}')\n","        # print(f'y shape = {y.shape}')\n","        # print(f'Before update data = {x_pred.shape} & y_pred = {y_pred.shape}, expand_dim shape = {np.expand_dims(y[:, j, :], 1).shape}')\n","        x_pred = np.append(x_pred[:, 1:, :], np.expand_dims(y[:, j, :], 1), axis=1)\n","\n","        # print(f'After update data = {x_pred.shape}')\n","        xt_pred = torch.Tensor(x_pred)\n","\n","        if torch.cuda.is_available():\n","            xt_pred = xt_pred.cuda()\n","\n","        # generate the next forecast\n","        yt_pred = model(xt_pred)\n","\n","        # print(f'model result yt_pred = {yt_pred.shape}')\n","        # tensor to array\n","        # x_pred = xt_pred.cpu().detach().numpy()\n","        y_pred = yt_pred.cpu().detach().numpy()\n","\n","        # save the forecast\n","        predict_data.append(y_pred)\n","        # save actual data\n","        actual_data.append(y[:, j, :])\n","\n","     # print(f'After loop predict_data = {len(predict_data)} & predict_data = {len(predict_data)}')\n","    # transform the forecasts back to the original scale\n","    predict_data = np.array(predict_data).reshape(-1, 1)\n","    # predict_data = scaler.inverse_transform(predict_data) # actual_data는 scale되어 있는 데이터임\n","    actual_data = np.array(actual_data).reshape(-1, 1)\n","\n","    # print(f'predict_data = {predict_data[:3]}, actual data = {actual_data[:3]}')\n","    # print(f'actual size = {actual.shape}, predict = {predict_data.shape}')\n","    SMAPE = smape(actual_data, predict_data)\n","    print(f' \\nSMAPE : {SMAPE}')\n","\n","\n","    return SMAPE\n"],"metadata":{"id":"WYyF-qAseVWf","executionInfo":{"status":"ok","timestamp":1655434371284,"user_tz":-540,"elapsed":382,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["sampler = optuna.samplers.TPESampler()\n","#   sampler = optuna.samplers.SkoptSampler()\n","\n","# model.load_state_dict(torch.load('lstm_saved_model.pth'))\n","    \n","study = optuna.create_study(sampler=sampler, direction='minimize')\n","study.optimize(objective, n_trials=25)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SZ9QNcBj6qX","outputId":"7c13d128-745a-4843-9f68-6b1c0ead8eea","executionInfo":{"status":"ok","timestamp":1655434924877,"user_tz":-540,"elapsed":539834,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:53:03,196]\u001b[0m A new study created in memory with name: no-name-31aa652f-d5aa-45dd-b85a-99bb42c8bae2\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0 \t Training Loss: 0.029111374601987854 \t Validation Loss: 0.012294458458200097 \n","\n","Validation Loss Decreased(inf--->0.012294) \t Saving The Model\n","Validation Loss Decreased(0.012294--->0.010433) \t Saving The Model\n","Validation Loss Decreased(0.010433--->0.009167) \t Saving The Model\n","Validation Loss Decreased(0.009167--->0.008294) \t Saving The Model\n","Validation Loss Decreased(0.008294--->0.007687) \t Saving The Model\n","Validation Loss Decreased(0.007687--->0.007262) \t Saving The Model\n","Validation Loss Decreased(0.007262--->0.006962) \t Saving The Model\n","Validation Loss Decreased(0.006962--->0.006747) \t Saving The Model\n","Validation Loss Decreased(0.006747--->0.006593) \t Saving The Model\n","Validation Loss Decreased(0.006593--->0.006479) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.022568062837568244 \t Validation Loss: 0.006393431278411299 \n","\n","Validation Loss Decreased(0.006479--->0.006393) \t Saving The Model\n","Validation Loss Decreased(0.006393--->0.006328) \t Saving The Model\n","Validation Loss Decreased(0.006328--->0.006275) \t Saving The Model\n","Validation Loss Decreased(0.006275--->0.006231) \t Saving The Model\n","Validation Loss Decreased(0.006231--->0.006194) \t Saving The Model\n","Validation Loss Decreased(0.006194--->0.006160) \t Saving The Model\n","Validation Loss Decreased(0.006160--->0.006129) \t Saving The Model\n","Validation Loss Decreased(0.006129--->0.006100) \t Saving The Model\n","Validation Loss Decreased(0.006100--->0.006071) \t Saving The Model\n","Validation Loss Decreased(0.006071--->0.006042) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.021523320581763983 \t Validation Loss: 0.006012091937009245 \n","\n","Validation Loss Decreased(0.006042--->0.006012) \t Saving The Model\n","Validation Loss Decreased(0.006012--->0.005982) \t Saving The Model\n","Validation Loss Decreased(0.005982--->0.005950) \t Saving The Model\n","Validation Loss Decreased(0.005950--->0.005916) \t Saving The Model\n","Validation Loss Decreased(0.005916--->0.005880) \t Saving The Model\n","Validation Loss Decreased(0.005880--->0.005842) \t Saving The Model\n","Validation Loss Decreased(0.005842--->0.005800) \t Saving The Model\n","Validation Loss Decreased(0.005800--->0.005754) \t Saving The Model\n","Validation Loss Decreased(0.005754--->0.005703) \t Saving The Model\n","Validation Loss Decreased(0.005703--->0.005646) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.019320007122587413 \t Validation Loss: 0.005581894889473915 \n","\n","Validation Loss Decreased(0.005646--->0.005582) \t Saving The Model\n","Validation Loss Decreased(0.005582--->0.005509) \t Saving The Model\n","Validation Loss Decreased(0.005509--->0.005425) \t Saving The Model\n","Validation Loss Decreased(0.005425--->0.005327) \t Saving The Model\n","Validation Loss Decreased(0.005327--->0.005210) \t Saving The Model\n","Validation Loss Decreased(0.005210--->0.005069) \t Saving The Model\n","Validation Loss Decreased(0.005069--->0.004895) \t Saving The Model\n","Validation Loss Decreased(0.004895--->0.004679) \t Saving The Model\n","Validation Loss Decreased(0.004679--->0.004408) \t Saving The Model\n","Validation Loss Decreased(0.004408--->0.004091) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.00839943957232338 \t Validation Loss: 0.003885359088599216 \n","\n","Validation Loss Decreased(0.004091--->0.003885) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0043618682144109956 \t Validation Loss: 0.003942690396797843 \n","\n","Validation Loss Decreased(0.003885--->0.003879) \t Saving The Model\n","Validation Loss Decreased(0.003879--->0.003856) \t Saving The Model\n","Validation Loss Decreased(0.003856--->0.003848) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.003144311451511125 \t Validation Loss: 0.003833199225482531 \n","\n","Validation Loss Decreased(0.003848--->0.003833) \t Saving The Model\n","Validation Loss Decreased(0.003833--->0.003818) \t Saving The Model\n","Validation Loss Decreased(0.003818--->0.003804) \t Saving The Model\n","Validation Loss Decreased(0.003804--->0.003790) \t Saving The Model\n","Validation Loss Decreased(0.003790--->0.003776) \t Saving The Model\n","Validation Loss Decreased(0.003776--->0.003762) \t Saving The Model\n","Validation Loss Decreased(0.003762--->0.003748) \t Saving The Model\n","Validation Loss Decreased(0.003748--->0.003735) \t Saving The Model\n","Validation Loss Decreased(0.003735--->0.003722) \t Saving The Model\n","Validation Loss Decreased(0.003722--->0.003710) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0030707341102242935 \t Validation Loss: 0.0036977383570047095 \n","\n","Validation Loss Decreased(0.003710--->0.003698) \t Saving The Model\n","Validation Loss Decreased(0.003698--->0.003686) \t Saving The Model\n","Validation Loss Decreased(0.003686--->0.003674) \t Saving The Model\n","Validation Loss Decreased(0.003674--->0.003663) \t Saving The Model\n","Validation Loss Decreased(0.003663--->0.003652) \t Saving The Model\n","Validation Loss Decreased(0.003652--->0.003641) \t Saving The Model\n","Validation Loss Decreased(0.003641--->0.003631) \t Saving The Model\n","Validation Loss Decreased(0.003631--->0.003621) \t Saving The Model\n","Validation Loss Decreased(0.003621--->0.003611) \t Saving The Model\n","Validation Loss Decreased(0.003611--->0.003601) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0030176048463285304 \t Validation Loss: 0.0035913963365601376 \n","\n","Validation Loss Decreased(0.003601--->0.003591) \t Saving The Model\n","Validation Loss Decreased(0.003591--->0.003582) \t Saving The Model\n","Validation Loss Decreased(0.003582--->0.003573) \t Saving The Model\n","Validation Loss Decreased(0.003573--->0.003564) \t Saving The Model\n","Validation Loss Decreased(0.003564--->0.003555) \t Saving The Model\n","Validation Loss Decreased(0.003555--->0.003547) \t Saving The Model\n","Validation Loss Decreased(0.003547--->0.003538) \t Saving The Model\n","Validation Loss Decreased(0.003538--->0.003530) \t Saving The Model\n","Validation Loss Decreased(0.003530--->0.003522) \t Saving The Model\n","Validation Loss Decreased(0.003522--->0.003514) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0029746356589644813 \t Validation Loss: 0.003506249689962715 \n","\n","Validation Loss Decreased(0.003514--->0.003506) \t Saving The Model\n","Validation Loss Decreased(0.003506--->0.003499) \t Saving The Model\n","Validation Loss Decreased(0.003499--->0.003491) \t Saving The Model\n","Validation Loss Decreased(0.003491--->0.003484) \t Saving The Model\n","Validation Loss Decreased(0.003484--->0.003477) \t Saving The Model\n","Validation Loss Decreased(0.003477--->0.003469) \t Saving The Model\n","Validation Loss Decreased(0.003469--->0.003463) \t Saving The Model\n","Validation Loss Decreased(0.003463--->0.003456) \t Saving The Model\n","Validation Loss Decreased(0.003456--->0.003449) \t Saving The Model\n","Validation Loss Decreased(0.003449--->0.003442) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0029385571319185794 \t Validation Loss: 0.0034358331176918 \n","\n","Validation Loss Decreased(0.003442--->0.003436) \t Saving The Model\n","Validation Loss Decreased(0.003436--->0.003429) \t Saving The Model\n","Validation Loss Decreased(0.003429--->0.003423) \t Saving The Model\n","Validation Loss Decreased(0.003423--->0.003417) \t Saving The Model\n","Validation Loss Decreased(0.003417--->0.003411) \t Saving The Model\n","Validation Loss Decreased(0.003411--->0.003405) \t Saving The Model\n","Validation Loss Decreased(0.003405--->0.003399) \t Saving The Model\n","Validation Loss Decreased(0.003399--->0.003393) \t Saving The Model\n","Validation Loss Decreased(0.003393--->0.003388) \t Saving The Model\n","Validation Loss Decreased(0.003388--->0.003382) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0029074888448771424 \t Validation Loss: 0.003376527725777123 \n","\n","Validation Loss Decreased(0.003382--->0.003377) \t Saving The Model\n","Validation Loss Decreased(0.003377--->0.003371) \t Saving The Model\n","Validation Loss Decreased(0.003371--->0.003366) \t Saving The Model\n","Validation Loss Decreased(0.003366--->0.003361) \t Saving The Model\n","Validation Loss Decreased(0.003361--->0.003355) \t Saving The Model\n","Validation Loss Decreased(0.003355--->0.003350) \t Saving The Model\n","Validation Loss Decreased(0.003350--->0.003345) \t Saving The Model\n","Validation Loss Decreased(0.003345--->0.003340) \t Saving The Model\n","Validation Loss Decreased(0.003340--->0.003336) \t Saving The Model\n","Validation Loss Decreased(0.003336--->0.003331) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0028801980598862948 \t Validation Loss: 0.0033261444477830082 \n","\n","Validation Loss Decreased(0.003331--->0.003326) \t Saving The Model\n","Validation Loss Decreased(0.003326--->0.003322) \t Saving The Model\n","Validation Loss Decreased(0.003322--->0.003317) \t Saving The Model\n","Validation Loss Decreased(0.003317--->0.003313) \t Saving The Model\n","Validation Loss Decreased(0.003313--->0.003308) \t Saving The Model\n","Validation Loss Decreased(0.003308--->0.003304) \t Saving The Model\n","Validation Loss Decreased(0.003304--->0.003300) \t Saving The Model\n","Validation Loss Decreased(0.003300--->0.003295) \t Saving The Model\n","Validation Loss Decreased(0.003295--->0.003291) \t Saving The Model\n","Validation Loss Decreased(0.003291--->0.003287) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.002855807683122943 \t Validation Loss: 0.0032831238204380497 \n","\n","Validation Loss Decreased(0.003287--->0.003283) \t Saving The Model\n","Validation Loss Decreased(0.003283--->0.003279) \t Saving The Model\n","Validation Loss Decreased(0.003279--->0.003275) \t Saving The Model\n","Validation Loss Decreased(0.003275--->0.003271) \t Saving The Model\n","Validation Loss Decreased(0.003271--->0.003268) \t Saving The Model\n","Validation Loss Decreased(0.003268--->0.003264) \t Saving The Model\n","Validation Loss Decreased(0.003264--->0.003260) \t Saving The Model\n","Validation Loss Decreased(0.003260--->0.003257) \t Saving The Model\n","Validation Loss Decreased(0.003257--->0.003253) \t Saving The Model\n","Validation Loss Decreased(0.003253--->0.003250) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.0028336670189414042 \t Validation Loss: 0.0032462141680298373 \n","\n","Validation Loss Decreased(0.003250--->0.003246) \t Saving The Model\n","Validation Loss Decreased(0.003246--->0.003243) \t Saving The Model\n","Validation Loss Decreased(0.003243--->0.003239) \t Saving The Model\n","Validation Loss Decreased(0.003239--->0.003236) \t Saving The Model\n","Validation Loss Decreased(0.003236--->0.003233) \t Saving The Model\n","Validation Loss Decreased(0.003233--->0.003230) \t Saving The Model\n","Validation Loss Decreased(0.003230--->0.003227) \t Saving The Model\n","Validation Loss Decreased(0.003227--->0.003223) \t Saving The Model\n","Validation Loss Decreased(0.003223--->0.003220) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:53:51,251]\u001b[0m Trial 0 finished with value: 86.25027691734313 and parameters: {'batch_size': 64, 'learning_rate': 0.00027136717806644186, 'hidden_size': 16, 'num_layers': 5}. Best is trial 0 with value: 86.25027691734313.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.003220--->0.003217) \t Saving The Model\n"," \n","SMAPE : 86.25027691734313\n","\n"," Epoch 0 \t Training Loss: 0.04333077766932547 \t Validation Loss: 0.010701923631131649 \n","\n","Validation Loss Decreased(inf--->0.010702) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.01036230381578207 \t Validation Loss: 0.023058531805872917 \n","\n","\n"," Epoch 20 \t Training Loss: 0.006529561942443252 \t Validation Loss: 0.023228630423545837 \n","\n","\n"," Epoch 30 \t Training Loss: 0.0037690355093218386 \t Validation Loss: 0.03246232122182846 \n","\n","\n"," Epoch 40 \t Training Loss: 0.009051148488651961 \t Validation Loss: 0.013370991684496403 \n","\n","\n"," Epoch 50 \t Training Loss: 0.005988960852846503 \t Validation Loss: 0.01831463910639286 \n","\n","\n"," Epoch 60 \t Training Loss: 0.01904796389862895 \t Validation Loss: 0.03462719917297363 \n","\n","\n"," Epoch 70 \t Training Loss: 0.011528013739734888 \t Validation Loss: 0.01859818957746029 \n","\n","\n"," Epoch 80 \t Training Loss: 0.010632309364154935 \t Validation Loss: 0.018659837543964386 \n","\n","\n"," Epoch 90 \t Training Loss: 0.007664003409445286 \t Validation Loss: 0.020198868587613106 \n","\n","\n"," Epoch 100 \t Training Loss: 0.00510904670227319 \t Validation Loss: 0.021983277052640915 \n","\n","\n"," Epoch 110 \t Training Loss: 0.010182250756770372 \t Validation Loss: 0.03133615106344223 \n","\n","\n"," Epoch 120 \t Training Loss: 0.006258803070522845 \t Validation Loss: 0.027995137497782707 \n","\n","\n"," Epoch 130 \t Training Loss: 0.014083295420277864 \t Validation Loss: 0.013934711925685406 \n","\n","\n"," Epoch 140 \t Training Loss: 0.01078157452866435 \t Validation Loss: 0.020761976018548012 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:53:59,585]\u001b[0m Trial 1 finished with value: 64.87771720962412 and parameters: {'batch_size': 512, 'learning_rate': 0.00667022038898804, 'hidden_size': 64, 'num_layers': 4}. Best is trial 1 with value: 64.87771720962412.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 64.87771720962412\n","\n"," Epoch 0 \t Training Loss: 1112.5953434593976 \t Validation Loss: 2273.0830078125 \n","\n","Validation Loss Decreased(inf--->2273.083008) \t Saving The Model\n","Validation Loss Decreased(2273.083008--->466.305359) \t Saving The Model\n","Validation Loss Decreased(466.305359--->216.264465) \t Saving The Model\n","Validation Loss Decreased(216.264465--->96.766563) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 150.63068008422852 \t Validation Loss: 178.6921844482422 \n","\n","Validation Loss Decreased(96.766563--->92.160561) \t Saving The Model\n","Validation Loss Decreased(92.160561--->47.747375) \t Saving The Model\n","Validation Loss Decreased(47.747375--->36.184837) \t Saving The Model\n","Validation Loss Decreased(36.184837--->36.010284) \t Saving The Model\n","Validation Loss Decreased(36.010284--->32.359142) \t Saving The Model\n","Validation Loss Decreased(32.359142--->19.757984) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 21.207637786865234 \t Validation Loss: 14.929875373840332 \n","\n","Validation Loss Decreased(19.757984--->14.929875) \t Saving The Model\n","Validation Loss Decreased(14.929875--->0.640835) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 2.6972744464874268 \t Validation Loss: 2.2872514724731445 \n","\n","Validation Loss Decreased(0.640835--->0.492192) \t Saving The Model\n","Validation Loss Decreased(0.492192--->0.284946) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.5985976159572601 \t Validation Loss: 0.26053065061569214 \n","\n","Validation Loss Decreased(0.284946--->0.260531) \t Saving The Model\n","Validation Loss Decreased(0.260531--->0.184901) \t Saving The Model\n","Validation Loss Decreased(0.184901--->0.049561) \t Saving The Model\n","Validation Loss Decreased(0.049561--->0.030160) \t Saving The Model\n","Validation Loss Decreased(0.030160--->0.024654) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.06905674375593662 \t Validation Loss: 0.01023291889578104 \n","\n","Validation Loss Decreased(0.024654--->0.010233) \t Saving The Model\n","Validation Loss Decreased(0.010233--->0.008502) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.023683512001298368 \t Validation Loss: 0.014788016676902771 \n","\n","\n"," Epoch 70 \t Training Loss: 0.016744264401495457 \t Validation Loss: 0.02281828783452511 \n","\n","\n"," Epoch 80 \t Training Loss: 0.018491053953766823 \t Validation Loss: 0.02318580448627472 \n","\n","\n"," Epoch 90 \t Training Loss: 0.019744026474654675 \t Validation Loss: 0.020993931218981743 \n","\n","\n"," Epoch 100 \t Training Loss: 0.020869061816483736 \t Validation Loss: 0.020874498412013054 \n","\n","\n"," Epoch 110 \t Training Loss: 0.02164150308817625 \t Validation Loss: 0.021277887746691704 \n","\n","\n"," Epoch 120 \t Training Loss: 0.022457662038505077 \t Validation Loss: 0.02151046320796013 \n","\n","\n"," Epoch 130 \t Training Loss: 0.023374445736408234 \t Validation Loss: 0.021910885348916054 \n","\n","\n"," Epoch 140 \t Training Loss: 0.024373159743845463 \t Validation Loss: 0.022330617532134056 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:54:11,537]\u001b[0m Trial 2 finished with value: 114.14551986749865 and parameters: {'batch_size': 512, 'learning_rate': 0.5008245774923209, 'hidden_size': 128, 'num_layers': 4}. Best is trial 1 with value: 64.87771720962412.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 114.14551986749865\n","\n"," Epoch 0 \t Training Loss: 0.03461551550675982 \t Validation Loss: 0.007283719489350915 \n","\n","Validation Loss Decreased(inf--->0.007284) \t Saving The Model\n","Validation Loss Decreased(0.007284--->0.006995) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.017810583048100983 \t Validation Loss: 0.008714356925338507 \n","\n","\n"," Epoch 20 \t Training Loss: 0.02163096546428278 \t Validation Loss: 0.007724469993263483 \n","\n","\n"," Epoch 30 \t Training Loss: 0.017191265609913638 \t Validation Loss: 0.0074939909391105175 \n","\n","Validation Loss Decreased(0.006995--->0.006519) \t Saving The Model\n","Validation Loss Decreased(0.006519--->0.005586) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.010580029554798134 \t Validation Loss: 0.007960665505379438 \n","\n","\n"," Epoch 50 \t Training Loss: 0.009623845723191542 \t Validation Loss: 0.007404803531244397 \n","\n","\n"," Epoch 60 \t Training Loss: 0.008971908379213087 \t Validation Loss: 0.007533789845183492 \n","\n","\n"," Epoch 70 \t Training Loss: 0.007433711151991572 \t Validation Loss: 0.007715423125773668 \n","\n","Validation Loss Decreased(0.005586--->0.005385) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.004238369014014357 \t Validation Loss: 0.007229314185678959 \n","\n","\n"," Epoch 90 \t Training Loss: 0.004699303292519679 \t Validation Loss: 0.006371957482770085 \n","\n","\n"," Epoch 100 \t Training Loss: 0.003031155909411609 \t Validation Loss: 0.008134234230965376 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0027060759727776584 \t Validation Loss: 0.00784192094579339 \n","\n","\n"," Epoch 120 \t Training Loss: 0.002464033054171263 \t Validation Loss: 0.007644176483154297 \n","\n","\n"," Epoch 130 \t Training Loss: 0.00239050283562392 \t Validation Loss: 0.007631572429090738 \n","\n","\n"," Epoch 140 \t Training Loss: 0.0022034636099955867 \t Validation Loss: 0.00740084471181035 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:55:05,275]\u001b[0m Trial 3 finished with value: 82.35671996926985 and parameters: {'batch_size': 128, 'learning_rate': 0.0003126224215409772, 'hidden_size': 256, 'num_layers': 5}. Best is trial 1 with value: 64.87771720962412.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 82.35671996926985\n","\n"," Epoch 0 \t Training Loss: 0.02883132716773876 \t Validation Loss: 0.0045520864659920335 \n","\n","Validation Loss Decreased(inf--->0.004552) \t Saving The Model\n","Validation Loss Decreased(0.004552--->0.004083) \t Saving The Model\n","Validation Loss Decreased(0.004083--->0.002181) \t Saving The Model\n","Validation Loss Decreased(0.002181--->0.000309) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.0022979750350974166 \t Validation Loss: 0.001131705439547659 \n","\n","Validation Loss Decreased(0.000309--->0.000238) \t Saving The Model\n","Validation Loss Decreased(0.000238--->0.000219) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.0010843286298560478 \t Validation Loss: 0.0013642865233123302 \n","\n","Validation Loss Decreased(0.000219--->0.000208) \t Saving The Model\n","Validation Loss Decreased(0.000208--->0.000194) \t Saving The Model\n","Validation Loss Decreased(0.000194--->0.000183) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0009028060026789067 \t Validation Loss: 0.00042960439168382436 \n","\n","Validation Loss Decreased(0.000183--->0.000178) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0006926146771937576 \t Validation Loss: 0.000263868390902644 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0005409261996516891 \t Validation Loss: 0.00020588365987350699 \n","\n","Validation Loss Decreased(0.000178--->0.000173) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0004920740647581365 \t Validation Loss: 0.0003333150575599575 \n","\n","\n"," Epoch 70 \t Training Loss: 0.000450592627333078 \t Validation Loss: 0.00018540257588028908 \n","\n","\n"," Epoch 80 \t Training Loss: 0.00043733228631026577 \t Validation Loss: 0.00018085589090333087 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0004336313915181173 \t Validation Loss: 0.00019043483143832418 \n","\n","Validation Loss Decreased(0.000173--->0.000157) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0007370683302464645 \t Validation Loss: 0.0008223734871535271 \n","\n","\n"," Epoch 110 \t Training Loss: 0.00038482172515484436 \t Validation Loss: 0.00016641324327792972 \n","\n","\n"," Epoch 120 \t Training Loss: 0.00038178897063647 \t Validation Loss: 0.00016653888997097965 \n","\n","\n"," Epoch 130 \t Training Loss: 0.000383583080682521 \t Validation Loss: 0.00017273038065468427 \n","\n","\n"," Epoch 140 \t Training Loss: 0.00038356218751037626 \t Validation Loss: 0.00018657257260201732 \n","\n","Validation Loss Decreased(0.000157--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000145) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:55:18,894]\u001b[0m Trial 4 finished with value: 60.33916997603149 and parameters: {'batch_size': 64, 'learning_rate': 0.003073317106297369, 'hidden_size': 16, 'num_layers': 1}. Best is trial 4 with value: 60.33916997603149.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 60.33916997603149\n","\n"," Epoch 0 \t Training Loss: 0.030618557548483034 \t Validation Loss: 0.012990651783184148 \n","\n","Validation Loss Decreased(inf--->0.012991) \t Saving The Model\n","Validation Loss Decreased(0.012991--->0.011785) \t Saving The Model\n","Validation Loss Decreased(0.011785--->0.007447) \t Saving The Model\n","Validation Loss Decreased(0.007447--->0.007044) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.01756676605353797 \t Validation Loss: 0.00678622793930117 \n","\n","Validation Loss Decreased(0.007044--->0.006786) \t Saving The Model\n","Validation Loss Decreased(0.006786--->0.005463) \t Saving The Model\n","Validation Loss Decreased(0.005463--->0.004313) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.015177740815228649 \t Validation Loss: 0.009285737876780331 \n","\n","Validation Loss Decreased(0.004313--->0.002420) \t Saving The Model\n","Validation Loss Decreased(0.002420--->0.002082) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.004091787007512592 \t Validation Loss: 0.0028418732108548284 \n","\n","\n"," Epoch 40 \t Training Loss: 0.004549574942627389 \t Validation Loss: 0.0032537372899241745 \n","\n","\n"," Epoch 50 \t Training Loss: 0.004419198805408087 \t Validation Loss: 0.0032995299843605608 \n","\n","Validation Loss Decreased(0.002082--->0.001912) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.003954387097370012 \t Validation Loss: 0.0029728374065598473 \n","\n","Validation Loss Decreased(0.001912--->0.001904) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.003925578204091705 \t Validation Loss: 0.0019670499459607527 \n","\n","Validation Loss Decreased(0.001904--->0.001860) \t Saving The Model\n","Validation Loss Decreased(0.001860--->0.001417) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.00495442096975499 \t Validation Loss: 0.0016865176148712635 \n","\n","\n"," Epoch 90 \t Training Loss: 0.003010741101336732 \t Validation Loss: 0.002576783052063547 \n","\n","Validation Loss Decreased(0.001417--->0.001366) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0028178657521493733 \t Validation Loss: 0.002832304104231298 \n","\n","Validation Loss Decreased(0.001366--->0.001276) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0023903066690503954 \t Validation Loss: 0.002025806257734075 \n","\n","\n"," Epoch 120 \t Training Loss: 0.002880259870705361 \t Validation Loss: 0.0037698175583500415 \n","\n","\n"," Epoch 130 \t Training Loss: 0.003196904469015343 \t Validation Loss: 0.0022313266672426835 \n","\n","\n"," Epoch 140 \t Training Loss: 0.0022199608559146456 \t Validation Loss: 0.002829942648531869 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:55:49,922]\u001b[0m Trial 5 finished with value: 95.5909378573714 and parameters: {'batch_size': 64, 'learning_rate': 0.009970115340872564, 'hidden_size': 64, 'num_layers': 3}. Best is trial 4 with value: 60.33916997603149.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 95.5909378573714\n","\n"," Epoch 0 \t Training Loss: 976.8677785098553 \t Validation Loss: 941.504150390625 \n","\n","Validation Loss Decreased(inf--->941.504150) \t Saving The Model\n","Validation Loss Decreased(941.504150--->600.306763) \t Saving The Model\n","Validation Loss Decreased(600.306763--->55.543846) \t Saving The Model\n","Validation Loss Decreased(55.543846--->48.290039) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 70.38897514343262 \t Validation Loss: 160.84231567382812 \n","\n","Validation Loss Decreased(48.290039--->48.030502) \t Saving The Model\n","Validation Loss Decreased(48.030502--->36.927578) \t Saving The Model\n","Validation Loss Decreased(36.927578--->23.749586) \t Saving The Model\n","Validation Loss Decreased(23.749586--->14.629571) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 14.389606475830078 \t Validation Loss: 11.814665794372559 \n","\n","Validation Loss Decreased(14.629571--->11.814666) \t Saving The Model\n","Validation Loss Decreased(11.814666--->9.554485) \t Saving The Model\n","Validation Loss Decreased(9.554485--->2.811311) \t Saving The Model\n","Validation Loss Decreased(2.811311--->1.465531) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 2.7328505516052246 \t Validation Loss: 1.28848397731781 \n","\n","Validation Loss Decreased(1.465531--->1.288484) \t Saving The Model\n","Validation Loss Decreased(1.288484--->0.483251) \t Saving The Model\n","Validation Loss Decreased(0.483251--->0.435419) \t Saving The Model\n","Validation Loss Decreased(0.435419--->0.063138) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.39975300431251526 \t Validation Loss: 0.1500614583492279 \n","\n","Validation Loss Decreased(0.063138--->0.046115) \t Saving The Model\n","Validation Loss Decreased(0.046115--->0.038968) \t Saving The Model\n","Validation Loss Decreased(0.038968--->0.023676) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.03579617105424404 \t Validation Loss: 0.10169291496276855 \n","\n","Validation Loss Decreased(0.023676--->0.020127) \t Saving The Model\n","Validation Loss Decreased(0.020127--->0.017801) \t Saving The Model\n","Validation Loss Decreased(0.017801--->0.009092) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.02281180117279291 \t Validation Loss: 0.015184272080659866 \n","\n","\n"," Epoch 70 \t Training Loss: 0.02051633968949318 \t Validation Loss: 0.01879708096385002 \n","\n","\n"," Epoch 80 \t Training Loss: 0.018636414781212807 \t Validation Loss: 0.018190350383520126 \n","\n","\n"," Epoch 90 \t Training Loss: 0.02027049520984292 \t Validation Loss: 0.01978793740272522 \n","\n","\n"," Epoch 100 \t Training Loss: 0.021020117681473494 \t Validation Loss: 0.01906205527484417 \n","\n","\n"," Epoch 110 \t Training Loss: 0.021784750744700432 \t Validation Loss: 0.01899043284356594 \n","\n","\n"," Epoch 120 \t Training Loss: 0.02253920864313841 \t Validation Loss: 0.019227150827646255 \n","\n","\n"," Epoch 130 \t Training Loss: 0.02336218487471342 \t Validation Loss: 0.019459104165434837 \n","\n","\n"," Epoch 140 \t Training Loss: 0.024239882826805115 \t Validation Loss: 0.019736645743250847 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:55:59,994]\u001b[0m Trial 6 finished with value: 84.61692565703285 and parameters: {'batch_size': 512, 'learning_rate': 0.5148536349587238, 'hidden_size': 128, 'num_layers': 3}. Best is trial 4 with value: 60.33916997603149.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 84.61692565703285\n","\n"," Epoch 0 \t Training Loss: 0.20548953348770738 \t Validation Loss: 0.05845282971858978 \n","\n","Validation Loss Decreased(inf--->0.058453) \t Saving The Model\n","Validation Loss Decreased(0.058453--->0.004089) \t Saving The Model\n","Validation Loss Decreased(0.004089--->0.001282) \t Saving The Model\n","Validation Loss Decreased(0.001282--->0.000814) \t Saving The Model\n","Validation Loss Decreased(0.000814--->0.000089) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.0005451728611660656 \t Validation Loss: 0.0008941452251747251 \n","\n","Validation Loss Decreased(0.000089--->0.000086) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.00029372852623055223 \t Validation Loss: 7.650633779121563e-05 \n","\n","Validation Loss Decreased(0.000086--->0.000077) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0002797589436340786 \t Validation Loss: 8.211079693865031e-05 \n","\n","\n"," Epoch 40 \t Training Loss: 0.0002739250357990386 \t Validation Loss: 8.179077121894807e-05 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0002690790379347163 \t Validation Loss: 8.107855683192611e-05 \n","\n","\n"," Epoch 60 \t Training Loss: 0.00026469807971807313 \t Validation Loss: 8.043602429097518e-05 \n","\n","\n"," Epoch 70 \t Training Loss: 0.0002607988230920455 \t Validation Loss: 7.977726636454463e-05 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0002573571564425947 \t Validation Loss: 7.911302236607298e-05 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0002543393061387178 \t Validation Loss: 7.84313670010306e-05 \n","\n","\n"," Epoch 100 \t Training Loss: 0.00025169654145429377 \t Validation Loss: 7.77186724008061e-05 \n","\n","\n"," Epoch 110 \t Training Loss: 0.00024936724730650894 \t Validation Loss: 7.696892862441018e-05 \n","\n","Validation Loss Decreased(0.000077--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0002472826490702573 \t Validation Loss: 7.62023773859255e-05 \n","\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000076) \t Saving The Model\n","Validation Loss Decreased(0.000076--->0.000075) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0002455222806929669 \t Validation Loss: 7.6275784522295e-05 \n","\n","Validation Loss Decreased(0.000075--->0.000073) \t Saving The Model\n","Validation Loss Decreased(0.000073--->0.000072) \t Saving The Model\n","Validation Loss Decreased(0.000072--->0.000069) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.00043819415805046447 \t Validation Loss: 0.00029834723682142794 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:56:05,170]\u001b[0m Trial 7 finished with value: 33.45010680557603 and parameters: {'batch_size': 256, 'learning_rate': 0.0830586299557716, 'hidden_size': 32, 'num_layers': 1}. Best is trial 7 with value: 33.45010680557603.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 33.45010680557603\n","\n"," Epoch 0 \t Training Loss: 309.7156193293631 \t Validation Loss: 79.99197387695312 \n","\n","Validation Loss Decreased(inf--->79.991974) \t Saving The Model\n","Validation Loss Decreased(79.991974--->34.095425) \t Saving The Model\n","Validation Loss Decreased(34.095425--->27.788855) \t Saving The Model\n","Validation Loss Decreased(27.788855--->2.567224) \t Saving The Model\n","Validation Loss Decreased(2.567224--->0.399669) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.23625800386071205 \t Validation Loss: 1.5040653944015503 \n","\n","Validation Loss Decreased(0.399669--->0.016365) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.2998593896627426 \t Validation Loss: 0.8677756190299988 \n","\n","Validation Loss Decreased(0.016365--->0.006247) \t Saving The Model\n","Validation Loss Decreased(0.006247--->0.005501) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.17954180017113686 \t Validation Loss: 0.08702459931373596 \n","\n","Validation Loss Decreased(0.005501--->0.003297) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.03157886676490307 \t Validation Loss: 0.013160107657313347 \n","\n","\n"," Epoch 50 \t Training Loss: 0.01966720400378108 \t Validation Loss: 0.007478017359972 \n","\n","Validation Loss Decreased(0.003297--->0.003182) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.021525304298847914 \t Validation Loss: 0.0036798291839659214 \n","\n","\n"," Epoch 70 \t Training Loss: 0.020818825578317046 \t Validation Loss: 0.0046736858785152435 \n","\n","\n"," Epoch 80 \t Training Loss: 0.021075906231999397 \t Validation Loss: 0.004167231731116772 \n","\n","\n"," Epoch 90 \t Training Loss: 0.021084011532366276 \t Validation Loss: 0.004346081521362066 \n","\n","\n"," Epoch 100 \t Training Loss: 0.021092039300128818 \t Validation Loss: 0.004365243017673492 \n","\n","\n"," Epoch 110 \t Training Loss: 0.021130985813215375 \t Validation Loss: 0.004363826476037502 \n","\n","\n"," Epoch 120 \t Training Loss: 0.02116452227346599 \t Validation Loss: 0.004381009377539158 \n","\n","\n"," Epoch 130 \t Training Loss: 0.02119475556537509 \t Validation Loss: 0.004400188103318214 \n","\n","\n"," Epoch 140 \t Training Loss: 0.0212245958391577 \t Validation Loss: 0.004418063443154097 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:57:02,863]\u001b[0m Trial 8 finished with value: 69.83483972726661 and parameters: {'batch_size': 512, 'learning_rate': 0.03448117586412615, 'hidden_size': 1024, 'num_layers': 1}. Best is trial 7 with value: 33.45010680557603.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 69.83483972726661\n","\n"," Epoch 0 \t Training Loss: 12.012861364121948 \t Validation Loss: 2.5411624908447266 \n","\n","Validation Loss Decreased(inf--->2.541162) \t Saving The Model\n","Validation Loss Decreased(2.541162--->0.015421) \t Saving The Model\n","Validation Loss Decreased(0.015421--->0.005522) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.09510857648482281 \t Validation Loss: 0.1610674262046814 \n","\n","\n"," Epoch 20 \t Training Loss: 0.05155582570504131 \t Validation Loss: 0.014208137057721615 \n","\n","Validation Loss Decreased(0.005522--->0.004922) \t Saving The Model\n","Validation Loss Decreased(0.004922--->0.004251) \t Saving The Model\n","Validation Loss Decreased(0.004251--->0.003727) \t Saving The Model\n","Validation Loss Decreased(0.003727--->0.003339) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.019345026313593344 \t Validation Loss: 0.003092128419666551 \n","\n","Validation Loss Decreased(0.003339--->0.003092) \t Saving The Model\n","Validation Loss Decreased(0.003092--->0.003007) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.024003113054537346 \t Validation Loss: 0.012116782134398818 \n","\n","\n"," Epoch 50 \t Training Loss: 0.060626556165516376 \t Validation Loss: 0.03179242229089141 \n","\n","\n"," Epoch 60 \t Training Loss: 0.08081891432604087 \t Validation Loss: 0.0054857488721609116 \n","\n","\n"," Epoch 70 \t Training Loss: 0.04335840073430778 \t Validation Loss: 0.025368931936100125 \n","\n","\n"," Epoch 80 \t Training Loss: 0.028811309070858573 \t Validation Loss: 0.01046917773783207 \n","\n","Validation Loss Decreased(0.003007--->0.002905) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.02448474446594316 \t Validation Loss: 0.0030487654730677605 \n","\n","\n"," Epoch 100 \t Training Loss: 0.03345381829421967 \t Validation Loss: 0.013884204439818859 \n","\n","\n"," Epoch 110 \t Training Loss: 0.04916098438219966 \t Validation Loss: 0.04044838389381766 \n","\n","\n"," Epoch 120 \t Training Loss: 0.06081603518188266 \t Validation Loss: 0.05970914661884308 \n","\n","\n"," Epoch 130 \t Training Loss: 0.06988069416755545 \t Validation Loss: 0.07042436487972736 \n","\n","\n"," Epoch 140 \t Training Loss: 0.07041917998243921 \t Validation Loss: 0.07400122657418251 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:57:25,441]\u001b[0m Trial 9 finished with value: 138.87861992836426 and parameters: {'batch_size': 64, 'learning_rate': 0.29406525415923634, 'hidden_size': 32, 'num_layers': 2}. Best is trial 7 with value: 33.45010680557603.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 138.87861992836426\n","\n"," Epoch 0 \t Training Loss: 1.360689515247941 \t Validation Loss: 0.02022482641041279 \n","\n","Validation Loss Decreased(inf--->0.020225) \t Saving The Model\n","Validation Loss Decreased(0.020225--->0.001887) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.00563455639348831 \t Validation Loss: 0.012286832556128502 \n","\n","\n"," Epoch 20 \t Training Loss: 0.0030711407744092867 \t Validation Loss: 0.008410614915192127 \n","\n","\n"," Epoch 30 \t Training Loss: 0.002868502342607826 \t Validation Loss: 0.0077596390619874 \n","\n","\n"," Epoch 40 \t Training Loss: 0.002692326030228287 \t Validation Loss: 0.006917358376085758 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0029457482451107353 \t Validation Loss: 0.008233347907662392 \n","\n","\n"," Epoch 60 \t Training Loss: 0.0027135493292007595 \t Validation Loss: 0.004426032770425081 \n","\n","\n"," Epoch 70 \t Training Loss: 0.0104841249412857 \t Validation Loss: 0.0036715264432132244 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0027069425559602678 \t Validation Loss: 0.009382962249219418 \n","\n","\n"," Epoch 90 \t Training Loss: 0.002335014913114719 \t Validation Loss: 0.007436783984303474 \n","\n","\n"," Epoch 100 \t Training Loss: 0.0022706918825861067 \t Validation Loss: 0.0072807553224265575 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0022927009995328262 \t Validation Loss: 0.00789066031575203 \n","\n","\n"," Epoch 120 \t Training Loss: 0.0033887503086589277 \t Validation Loss: 0.006644912529736757 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0022247346059884876 \t Validation Loss: 0.0064830658957362175 \n","\n","\n"," Epoch 140 \t Training Loss: 0.0025566147232893854 \t Validation Loss: 0.007693339139223099 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:57:33,335]\u001b[0m Trial 10 finished with value: 74.91523374180134 and parameters: {'batch_size': 256, 'learning_rate': 0.06873196655801943, 'hidden_size': 32, 'num_layers': 2}. Best is trial 7 with value: 33.45010680557603.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 74.91523374180134\n","\n"," Epoch 0 \t Training Loss: 0.02094847447006032 \t Validation Loss: 0.003521205624565482 \n","\n","Validation Loss Decreased(inf--->0.003521) \t Saving The Model\n","Validation Loss Decreased(0.003521--->0.003125) \t Saving The Model\n","Validation Loss Decreased(0.003125--->0.003057) \t Saving The Model\n","Validation Loss Decreased(0.003057--->0.003029) \t Saving The Model\n","Validation Loss Decreased(0.003029--->0.002803) \t Saving The Model\n","Validation Loss Decreased(0.002803--->0.002533) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.011989462655037642 \t Validation Loss: 0.0022501382045447826 \n","\n","Validation Loss Decreased(0.002533--->0.002250) \t Saving The Model\n","Validation Loss Decreased(0.002250--->0.001975) \t Saving The Model\n","Validation Loss Decreased(0.001975--->0.001711) \t Saving The Model\n","Validation Loss Decreased(0.001711--->0.001450) \t Saving The Model\n","Validation Loss Decreased(0.001450--->0.001173) \t Saving The Model\n","Validation Loss Decreased(0.001173--->0.000853) \t Saving The Model\n","Validation Loss Decreased(0.000853--->0.000515) \t Saving The Model\n","Validation Loss Decreased(0.000515--->0.000411) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.00114331913573551 \t Validation Loss: 0.0004816227010451257 \n","\n","Validation Loss Decreased(0.000411--->0.000409) \t Saving The Model\n","Validation Loss Decreased(0.000409--->0.000365) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0009923033921950264 \t Validation Loss: 0.000363321480108425 \n","\n","Validation Loss Decreased(0.000365--->0.000363) \t Saving The Model\n","Validation Loss Decreased(0.000363--->0.000351) \t Saving The Model\n","Validation Loss Decreased(0.000351--->0.000345) \t Saving The Model\n","Validation Loss Decreased(0.000345--->0.000335) \t Saving The Model\n","Validation Loss Decreased(0.000335--->0.000333) \t Saving The Model\n","Validation Loss Decreased(0.000333--->0.000329) \t Saving The Model\n","Validation Loss Decreased(0.000329--->0.000323) \t Saving The Model\n","Validation Loss Decreased(0.000323--->0.000317) \t Saving The Model\n","Validation Loss Decreased(0.000317--->0.000310) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0008990476198960096 \t Validation Loss: 0.00030462717404589057 \n","\n","Validation Loss Decreased(0.000310--->0.000305) \t Saving The Model\n","Validation Loss Decreased(0.000305--->0.000301) \t Saving The Model\n","Validation Loss Decreased(0.000301--->0.000298) \t Saving The Model\n","Validation Loss Decreased(0.000298--->0.000294) \t Saving The Model\n","Validation Loss Decreased(0.000294--->0.000290) \t Saving The Model\n","Validation Loss Decreased(0.000290--->0.000285) \t Saving The Model\n","Validation Loss Decreased(0.000285--->0.000281) \t Saving The Model\n","Validation Loss Decreased(0.000281--->0.000277) \t Saving The Model\n","Validation Loss Decreased(0.000277--->0.000273) \t Saving The Model\n","Validation Loss Decreased(0.000273--->0.000269) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0008239159924414707 \t Validation Loss: 0.0002658771991264075 \n","\n","Validation Loss Decreased(0.000269--->0.000266) \t Saving The Model\n","Validation Loss Decreased(0.000266--->0.000262) \t Saving The Model\n","Validation Loss Decreased(0.000262--->0.000259) \t Saving The Model\n","Validation Loss Decreased(0.000259--->0.000255) \t Saving The Model\n","Validation Loss Decreased(0.000255--->0.000252) \t Saving The Model\n","Validation Loss Decreased(0.000252--->0.000249) \t Saving The Model\n","Validation Loss Decreased(0.000249--->0.000246) \t Saving The Model\n","Validation Loss Decreased(0.000246--->0.000243) \t Saving The Model\n","Validation Loss Decreased(0.000243--->0.000240) \t Saving The Model\n","Validation Loss Decreased(0.000240--->0.000237) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0007568236633233028 \t Validation Loss: 0.0002338682534173131 \n","\n","Validation Loss Decreased(0.000237--->0.000234) \t Saving The Model\n","Validation Loss Decreased(0.000234--->0.000231) \t Saving The Model\n","Validation Loss Decreased(0.000231--->0.000228) \t Saving The Model\n","Validation Loss Decreased(0.000228--->0.000226) \t Saving The Model\n","Validation Loss Decreased(0.000226--->0.000223) \t Saving The Model\n","Validation Loss Decreased(0.000223--->0.000220) \t Saving The Model\n","Validation Loss Decreased(0.000220--->0.000218) \t Saving The Model\n","Validation Loss Decreased(0.000218--->0.000215) \t Saving The Model\n","Validation Loss Decreased(0.000215--->0.000213) \t Saving The Model\n","Validation Loss Decreased(0.000213--->0.000210) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0006951984960323898 \t Validation Loss: 0.00020811073773074895 \n","\n","Validation Loss Decreased(0.000210--->0.000208) \t Saving The Model\n","Validation Loss Decreased(0.000208--->0.000206) \t Saving The Model\n","Validation Loss Decreased(0.000206--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000201) \t Saving The Model\n","Validation Loss Decreased(0.000201--->0.000199) \t Saving The Model\n","Validation Loss Decreased(0.000199--->0.000197) \t Saving The Model\n","Validation Loss Decreased(0.000197--->0.000195) \t Saving The Model\n","Validation Loss Decreased(0.000195--->0.000193) \t Saving The Model\n","Validation Loss Decreased(0.000193--->0.000190) \t Saving The Model\n","Validation Loss Decreased(0.000190--->0.000188) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0006372373845806578 \t Validation Loss: 0.00018631559214554727 \n","\n","Validation Loss Decreased(0.000188--->0.000186) \t Saving The Model\n","Validation Loss Decreased(0.000186--->0.000184) \t Saving The Model\n","Validation Loss Decreased(0.000184--->0.000182) \t Saving The Model\n","Validation Loss Decreased(0.000182--->0.000180) \t Saving The Model\n","Validation Loss Decreased(0.000180--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000177) \t Saving The Model\n","Validation Loss Decreased(0.000177--->0.000175) \t Saving The Model\n","Validation Loss Decreased(0.000175--->0.000173) \t Saving The Model\n","Validation Loss Decreased(0.000173--->0.000171) \t Saving The Model\n","Validation Loss Decreased(0.000171--->0.000169) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0005824208728881786 \t Validation Loss: 0.00016746109758969396 \n","\n","Validation Loss Decreased(0.000169--->0.000167) \t Saving The Model\n","Validation Loss Decreased(0.000167--->0.000166) \t Saving The Model\n","Validation Loss Decreased(0.000166--->0.000164) \t Saving The Model\n","Validation Loss Decreased(0.000164--->0.000162) \t Saving The Model\n","Validation Loss Decreased(0.000162--->0.000161) \t Saving The Model\n","Validation Loss Decreased(0.000161--->0.000159) \t Saving The Model\n","Validation Loss Decreased(0.000159--->0.000158) \t Saving The Model\n","Validation Loss Decreased(0.000158--->0.000156) \t Saving The Model\n","Validation Loss Decreased(0.000156--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000153) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0005328615407051984 \t Validation Loss: 0.00015209363482426852 \n","\n","Validation Loss Decreased(0.000153--->0.000152) \t Saving The Model\n","Validation Loss Decreased(0.000152--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000150) \t Saving The Model\n","Validation Loss Decreased(0.000150--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000147) \t Saving The Model\n","Validation Loss Decreased(0.000147--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000143) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0004927862428303342 \t Validation Loss: 0.0001420220942236483 \n","\n","Validation Loss Decreased(0.000143--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000139) \t Saving The Model\n","Validation Loss Decreased(0.000139--->0.000139) \t Saving The Model\n","Validation Loss Decreased(0.000139--->0.000138) \t Saving The Model\n","Validation Loss Decreased(0.000138--->0.000138) \t Saving The Model\n","Validation Loss Decreased(0.000138--->0.000138) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.00046218930947361514 \t Validation Loss: 0.00013730491627939045 \n","\n","Validation Loss Decreased(0.000138--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.00043660131632350385 \t Validation Loss: 0.00013578646758105606 \n","\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.0004140477249165997 \t Validation Loss: 0.00013600854435935616 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:57:38,667]\u001b[0m Trial 11 finished with value: 7.023413212837748 and parameters: {'batch_size': 256, 'learning_rate': 0.00243744167357133, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 7.023413212837748\n","\n"," Epoch 0 \t Training Loss: 0.020430644450243562 \t Validation Loss: 0.003940597176551819 \n","\n","Validation Loss Decreased(inf--->0.003941) \t Saving The Model\n","Validation Loss Decreased(0.003941--->0.003421) \t Saving The Model\n","Validation Loss Decreased(0.003421--->0.003229) \t Saving The Model\n","Validation Loss Decreased(0.003229--->0.003166) \t Saving The Model\n","Validation Loss Decreased(0.003166--->0.003162) \t Saving The Model\n","Validation Loss Decreased(0.003162--->0.003116) \t Saving The Model\n","Validation Loss Decreased(0.003116--->0.003018) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.014343225804623216 \t Validation Loss: 0.002885145600885153 \n","\n","Validation Loss Decreased(0.003018--->0.002885) \t Saving The Model\n","Validation Loss Decreased(0.002885--->0.002726) \t Saving The Model\n","Validation Loss Decreased(0.002726--->0.002551) \t Saving The Model\n","Validation Loss Decreased(0.002551--->0.002368) \t Saving The Model\n","Validation Loss Decreased(0.002368--->0.002179) \t Saving The Model\n","Validation Loss Decreased(0.002179--->0.001984) \t Saving The Model\n","Validation Loss Decreased(0.001984--->0.001781) \t Saving The Model\n","Validation Loss Decreased(0.001781--->0.001565) \t Saving The Model\n","Validation Loss Decreased(0.001565--->0.001330) \t Saving The Model\n","Validation Loss Decreased(0.001330--->0.001076) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.0032805405207909644 \t Validation Loss: 0.0008182974997907877 \n","\n","Validation Loss Decreased(0.001076--->0.000818) \t Saving The Model\n","Validation Loss Decreased(0.000818--->0.000597) \t Saving The Model\n","Validation Loss Decreased(0.000597--->0.000486) \t Saving The Model\n","Validation Loss Decreased(0.000486--->0.000464) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0011172039157827385 \t Validation Loss: 0.00043432676466181874 \n","\n","Validation Loss Decreased(0.000464--->0.000434) \t Saving The Model\n","Validation Loss Decreased(0.000434--->0.000428) \t Saving The Model\n","Validation Loss Decreased(0.000428--->0.000426) \t Saving The Model\n","Validation Loss Decreased(0.000426--->0.000424) \t Saving The Model\n","Validation Loss Decreased(0.000424--->0.000418) \t Saving The Model\n","Validation Loss Decreased(0.000418--->0.000405) \t Saving The Model\n","Validation Loss Decreased(0.000405--->0.000395) \t Saving The Model\n","Validation Loss Decreased(0.000395--->0.000388) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0010217744911642512 \t Validation Loss: 0.00038192610372789204 \n","\n","Validation Loss Decreased(0.000388--->0.000382) \t Saving The Model\n","Validation Loss Decreased(0.000382--->0.000377) \t Saving The Model\n","Validation Loss Decreased(0.000377--->0.000374) \t Saving The Model\n","Validation Loss Decreased(0.000374--->0.000370) \t Saving The Model\n","Validation Loss Decreased(0.000370--->0.000364) \t Saving The Model\n","Validation Loss Decreased(0.000364--->0.000358) \t Saving The Model\n","Validation Loss Decreased(0.000358--->0.000353) \t Saving The Model\n","Validation Loss Decreased(0.000353--->0.000347) \t Saving The Model\n","Validation Loss Decreased(0.000347--->0.000342) \t Saving The Model\n","Validation Loss Decreased(0.000342--->0.000338) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0009434599051019177 \t Validation Loss: 0.00033368298318237066 \n","\n","Validation Loss Decreased(0.000338--->0.000334) \t Saving The Model\n","Validation Loss Decreased(0.000334--->0.000329) \t Saving The Model\n","Validation Loss Decreased(0.000329--->0.000325) \t Saving The Model\n","Validation Loss Decreased(0.000325--->0.000321) \t Saving The Model\n","Validation Loss Decreased(0.000321--->0.000316) \t Saving The Model\n","Validation Loss Decreased(0.000316--->0.000312) \t Saving The Model\n","Validation Loss Decreased(0.000312--->0.000308) \t Saving The Model\n","Validation Loss Decreased(0.000308--->0.000304) \t Saving The Model\n","Validation Loss Decreased(0.000304--->0.000301) \t Saving The Model\n","Validation Loss Decreased(0.000301--->0.000297) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0008754890332056675 \t Validation Loss: 0.00029359542531892657 \n","\n","Validation Loss Decreased(0.000297--->0.000294) \t Saving The Model\n","Validation Loss Decreased(0.000294--->0.000290) \t Saving The Model\n","Validation Loss Decreased(0.000290--->0.000287) \t Saving The Model\n","Validation Loss Decreased(0.000287--->0.000283) \t Saving The Model\n","Validation Loss Decreased(0.000283--->0.000280) \t Saving The Model\n","Validation Loss Decreased(0.000280--->0.000277) \t Saving The Model\n","Validation Loss Decreased(0.000277--->0.000274) \t Saving The Model\n","Validation Loss Decreased(0.000274--->0.000271) \t Saving The Model\n","Validation Loss Decreased(0.000271--->0.000268) \t Saving The Model\n","Validation Loss Decreased(0.000268--->0.000265) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0008145959800458513 \t Validation Loss: 0.0002618386934045702 \n","\n","Validation Loss Decreased(0.000265--->0.000262) \t Saving The Model\n","Validation Loss Decreased(0.000262--->0.000259) \t Saving The Model\n","Validation Loss Decreased(0.000259--->0.000256) \t Saving The Model\n","Validation Loss Decreased(0.000256--->0.000254) \t Saving The Model\n","Validation Loss Decreased(0.000254--->0.000251) \t Saving The Model\n","Validation Loss Decreased(0.000251--->0.000248) \t Saving The Model\n","Validation Loss Decreased(0.000248--->0.000246) \t Saving The Model\n","Validation Loss Decreased(0.000246--->0.000243) \t Saving The Model\n","Validation Loss Decreased(0.000243--->0.000241) \t Saving The Model\n","Validation Loss Decreased(0.000241--->0.000238) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0007587969193991739 \t Validation Loss: 0.0002359884383622557 \n","\n","Validation Loss Decreased(0.000238--->0.000236) \t Saving The Model\n","Validation Loss Decreased(0.000236--->0.000234) \t Saving The Model\n","Validation Loss Decreased(0.000234--->0.000231) \t Saving The Model\n","Validation Loss Decreased(0.000231--->0.000229) \t Saving The Model\n","Validation Loss Decreased(0.000229--->0.000227) \t Saving The Model\n","Validation Loss Decreased(0.000227--->0.000225) \t Saving The Model\n","Validation Loss Decreased(0.000225--->0.000223) \t Saving The Model\n","Validation Loss Decreased(0.000223--->0.000220) \t Saving The Model\n","Validation Loss Decreased(0.000220--->0.000218) \t Saving The Model\n","Validation Loss Decreased(0.000218--->0.000216) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0007066998277878156 \t Validation Loss: 0.0002142024168279022 \n","\n","Validation Loss Decreased(0.000216--->0.000214) \t Saving The Model\n","Validation Loss Decreased(0.000214--->0.000212) \t Saving The Model\n","Validation Loss Decreased(0.000212--->0.000210) \t Saving The Model\n","Validation Loss Decreased(0.000210--->0.000208) \t Saving The Model\n","Validation Loss Decreased(0.000208--->0.000206) \t Saving The Model\n","Validation Loss Decreased(0.000206--->0.000204) \t Saving The Model\n","Validation Loss Decreased(0.000204--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000201) \t Saving The Model\n","Validation Loss Decreased(0.000201--->0.000199) \t Saving The Model\n","Validation Loss Decreased(0.000199--->0.000197) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.000657453646454087 \t Validation Loss: 0.00019529850396793336 \n","\n","Validation Loss Decreased(0.000197--->0.000195) \t Saving The Model\n","Validation Loss Decreased(0.000195--->0.000194) \t Saving The Model\n","Validation Loss Decreased(0.000194--->0.000192) \t Saving The Model\n","Validation Loss Decreased(0.000192--->0.000190) \t Saving The Model\n","Validation Loss Decreased(0.000190--->0.000188) \t Saving The Model\n","Validation Loss Decreased(0.000188--->0.000187) \t Saving The Model\n","Validation Loss Decreased(0.000187--->0.000185) \t Saving The Model\n","Validation Loss Decreased(0.000185--->0.000183) \t Saving The Model\n","Validation Loss Decreased(0.000183--->0.000182) \t Saving The Model\n","Validation Loss Decreased(0.000182--->0.000180) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0006109401729190722 \t Validation Loss: 0.00017868459690362215 \n","\n","Validation Loss Decreased(0.000180--->0.000179) \t Saving The Model\n","Validation Loss Decreased(0.000179--->0.000177) \t Saving The Model\n","Validation Loss Decreased(0.000177--->0.000176) \t Saving The Model\n","Validation Loss Decreased(0.000176--->0.000174) \t Saving The Model\n","Validation Loss Decreased(0.000174--->0.000173) \t Saving The Model\n","Validation Loss Decreased(0.000173--->0.000171) \t Saving The Model\n","Validation Loss Decreased(0.000171--->0.000170) \t Saving The Model\n","Validation Loss Decreased(0.000170--->0.000168) \t Saving The Model\n","Validation Loss Decreased(0.000168--->0.000167) \t Saving The Model\n","Validation Loss Decreased(0.000167--->0.000166) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.000568120365642244 \t Validation Loss: 0.0001644863368710503 \n","\n","Validation Loss Decreased(0.000166--->0.000164) \t Saving The Model\n","Validation Loss Decreased(0.000164--->0.000163) \t Saving The Model\n","Validation Loss Decreased(0.000163--->0.000162) \t Saving The Model\n","Validation Loss Decreased(0.000162--->0.000161) \t Saving The Model\n","Validation Loss Decreased(0.000161--->0.000160) \t Saving The Model\n","Validation Loss Decreased(0.000160--->0.000159) \t Saving The Model\n","Validation Loss Decreased(0.000159--->0.000157) \t Saving The Model\n","Validation Loss Decreased(0.000157--->0.000156) \t Saving The Model\n","Validation Loss Decreased(0.000156--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000154) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0005308849285938777 \t Validation Loss: 0.00015351006004493684 \n","\n","Validation Loss Decreased(0.000154--->0.000154) \t Saving The Model\n","Validation Loss Decreased(0.000154--->0.000153) \t Saving The Model\n","Validation Loss Decreased(0.000153--->0.000152) \t Saving The Model\n","Validation Loss Decreased(0.000152--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000150) \t Saving The Model\n","Validation Loss Decreased(0.000150--->0.000149) \t Saving The Model\n","Validation Loss Decreased(0.000149--->0.000149) \t Saving The Model\n","Validation Loss Decreased(0.000149--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000147) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.0005003428304917179 \t Validation Loss: 0.00014642835594713688 \n","\n","Validation Loss Decreased(0.000147--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:57:44,141]\u001b[0m Trial 12 finished with value: 7.6710563602122095 and parameters: {'batch_size': 256, 'learning_rate': 0.0017693541530254884, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.000144--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n"," \n","SMAPE : 7.6710563602122095\n","\n"," Epoch 0 \t Training Loss: 0.0376974823884666 \t Validation Loss: 0.011514680460095406 \n","\n","Validation Loss Decreased(inf--->0.011515) \t Saving The Model\n","Validation Loss Decreased(0.011515--->0.010238) \t Saving The Model\n","Validation Loss Decreased(0.010238--->0.007360) \t Saving The Model\n","Validation Loss Decreased(0.007360--->0.007205) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.030618369637522846 \t Validation Loss: 0.01142826396971941 \n","\n","Validation Loss Decreased(0.007205--->0.006512) \t Saving The Model\n","Validation Loss Decreased(0.006512--->0.006246) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.017471651313826442 \t Validation Loss: 0.010115246288478374 \n","\n","\n"," Epoch 30 \t Training Loss: 0.013333288254216313 \t Validation Loss: 0.008950460702180862 \n","\n","Validation Loss Decreased(0.006246--->0.004368) \t Saving The Model\n","Validation Loss Decreased(0.004368--->0.004062) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.025669854803709313 \t Validation Loss: 0.021479029208421707 \n","\n","\n"," Epoch 50 \t Training Loss: 0.01671952588367276 \t Validation Loss: 0.007332077249884605 \n","\n","Validation Loss Decreased(0.004062--->0.003114) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.014222733385395259 \t Validation Loss: 0.011404045857489109 \n","\n","\n"," Epoch 70 \t Training Loss: 0.0101208031992428 \t Validation Loss: 0.008334428071975708 \n","\n","\n"," Epoch 80 \t Training Loss: 0.01952932751737535 \t Validation Loss: 0.004398550372570753 \n","\n","\n"," Epoch 90 \t Training Loss: 0.012758038123138249 \t Validation Loss: 0.007841093465685844 \n","\n","\n"," Epoch 100 \t Training Loss: 0.007852579990867525 \t Validation Loss: 0.007380696013569832 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0033712222066242248 \t Validation Loss: 0.005754165351390839 \n","\n","\n"," Epoch 120 \t Training Loss: 0.002379135286901146 \t Validation Loss: 0.006355511490255594 \n","\n","\n"," Epoch 130 \t Training Loss: 0.00223276575707132 \t Validation Loss: 0.005970044061541557 \n","\n","\n"," Epoch 140 \t Training Loss: 0.00202043323952239 \t Validation Loss: 0.006001299247145653 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:58:33,052]\u001b[0m Trial 13 finished with value: 68.09848964595648 and parameters: {'batch_size': 256, 'learning_rate': 0.0010840655996163758, 'hidden_size': 512, 'num_layers': 2}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 68.09848964595648\n","\n"," Epoch 0 \t Training Loss: 0.020461182692088187 \t Validation Loss: 0.00391016760841012 \n","\n","Validation Loss Decreased(inf--->0.003910) \t Saving The Model\n","Validation Loss Decreased(0.003910--->0.003397) \t Saving The Model\n","Validation Loss Decreased(0.003397--->0.003213) \t Saving The Model\n","Validation Loss Decreased(0.003213--->0.003156) \t Saving The Model\n","Validation Loss Decreased(0.003156--->0.003105) \t Saving The Model\n","Validation Loss Decreased(0.003105--->0.002998) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.014217708492651582 \t Validation Loss: 0.0028552524745464325 \n","\n","Validation Loss Decreased(0.002998--->0.002855) \t Saving The Model\n","Validation Loss Decreased(0.002855--->0.002687) \t Saving The Model\n","Validation Loss Decreased(0.002687--->0.002504) \t Saving The Model\n","Validation Loss Decreased(0.002504--->0.002313) \t Saving The Model\n","Validation Loss Decreased(0.002313--->0.002118) \t Saving The Model\n","Validation Loss Decreased(0.002118--->0.001917) \t Saving The Model\n","Validation Loss Decreased(0.001917--->0.001707) \t Saving The Model\n","Validation Loss Decreased(0.001707--->0.001482) \t Saving The Model\n","Validation Loss Decreased(0.001482--->0.001237) \t Saving The Model\n","Validation Loss Decreased(0.001237--->0.000974) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.0026449313736520708 \t Validation Loss: 0.0007187813171185553 \n","\n","Validation Loss Decreased(0.000974--->0.000719) \t Saving The Model\n","Validation Loss Decreased(0.000719--->0.000527) \t Saving The Model\n","Validation Loss Decreased(0.000527--->0.000480) \t Saving The Model\n","Validation Loss Decreased(0.000480--->0.000442) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0011075677430198994 \t Validation Loss: 0.0004233181825838983 \n","\n","Validation Loss Decreased(0.000442--->0.000423) \t Saving The Model\n","Validation Loss Decreased(0.000423--->0.000423) \t Saving The Model\n","Validation Loss Decreased(0.000423--->0.000421) \t Saving The Model\n","Validation Loss Decreased(0.000421--->0.000420) \t Saving The Model\n","Validation Loss Decreased(0.000420--->0.000420) \t Saving The Model\n","Validation Loss Decreased(0.000420--->0.000408) \t Saving The Model\n","Validation Loss Decreased(0.000408--->0.000396) \t Saving The Model\n","Validation Loss Decreased(0.000396--->0.000388) \t Saving The Model\n","Validation Loss Decreased(0.000388--->0.000381) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0010123456740984693 \t Validation Loss: 0.00037561135832220316 \n","\n","Validation Loss Decreased(0.000381--->0.000376) \t Saving The Model\n","Validation Loss Decreased(0.000376--->0.000372) \t Saving The Model\n","Validation Loss Decreased(0.000372--->0.000369) \t Saving The Model\n","Validation Loss Decreased(0.000369--->0.000364) \t Saving The Model\n","Validation Loss Decreased(0.000364--->0.000358) \t Saving The Model\n","Validation Loss Decreased(0.000358--->0.000352) \t Saving The Model\n","Validation Loss Decreased(0.000352--->0.000346) \t Saving The Model\n","Validation Loss Decreased(0.000346--->0.000341) \t Saving The Model\n","Validation Loss Decreased(0.000341--->0.000336) \t Saving The Model\n","Validation Loss Decreased(0.000336--->0.000332) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0009342600642412435 \t Validation Loss: 0.0003281206882093102 \n","\n","Validation Loss Decreased(0.000332--->0.000328) \t Saving The Model\n","Validation Loss Decreased(0.000328--->0.000324) \t Saving The Model\n","Validation Loss Decreased(0.000324--->0.000319) \t Saving The Model\n","Validation Loss Decreased(0.000319--->0.000315) \t Saving The Model\n","Validation Loss Decreased(0.000315--->0.000311) \t Saving The Model\n","Validation Loss Decreased(0.000311--->0.000307) \t Saving The Model\n","Validation Loss Decreased(0.000307--->0.000303) \t Saving The Model\n","Validation Loss Decreased(0.000303--->0.000299) \t Saving The Model\n","Validation Loss Decreased(0.000299--->0.000296) \t Saving The Model\n","Validation Loss Decreased(0.000296--->0.000292) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0008663979406264843 \t Validation Loss: 0.0002885544381570071 \n","\n","Validation Loss Decreased(0.000292--->0.000289) \t Saving The Model\n","Validation Loss Decreased(0.000289--->0.000285) \t Saving The Model\n","Validation Loss Decreased(0.000285--->0.000282) \t Saving The Model\n","Validation Loss Decreased(0.000282--->0.000278) \t Saving The Model\n","Validation Loss Decreased(0.000278--->0.000275) \t Saving The Model\n","Validation Loss Decreased(0.000275--->0.000272) \t Saving The Model\n","Validation Loss Decreased(0.000272--->0.000269) \t Saving The Model\n","Validation Loss Decreased(0.000269--->0.000266) \t Saving The Model\n","Validation Loss Decreased(0.000266--->0.000263) \t Saving The Model\n","Validation Loss Decreased(0.000263--->0.000260) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0008055009438976413 \t Validation Loss: 0.0002573465171735734 \n","\n","Validation Loss Decreased(0.000260--->0.000257) \t Saving The Model\n","Validation Loss Decreased(0.000257--->0.000255) \t Saving The Model\n","Validation Loss Decreased(0.000255--->0.000252) \t Saving The Model\n","Validation Loss Decreased(0.000252--->0.000249) \t Saving The Model\n","Validation Loss Decreased(0.000249--->0.000247) \t Saving The Model\n","Validation Loss Decreased(0.000247--->0.000244) \t Saving The Model\n","Validation Loss Decreased(0.000244--->0.000242) \t Saving The Model\n","Validation Loss Decreased(0.000242--->0.000239) \t Saving The Model\n","Validation Loss Decreased(0.000239--->0.000237) \t Saving The Model\n","Validation Loss Decreased(0.000237--->0.000234) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.000749604589145747 \t Validation Loss: 0.0002318870392628014 \n","\n","Validation Loss Decreased(0.000234--->0.000232) \t Saving The Model\n","Validation Loss Decreased(0.000232--->0.000230) \t Saving The Model\n","Validation Loss Decreased(0.000230--->0.000227) \t Saving The Model\n","Validation Loss Decreased(0.000227--->0.000225) \t Saving The Model\n","Validation Loss Decreased(0.000225--->0.000223) \t Saving The Model\n","Validation Loss Decreased(0.000223--->0.000221) \t Saving The Model\n","Validation Loss Decreased(0.000221--->0.000219) \t Saving The Model\n","Validation Loss Decreased(0.000219--->0.000216) \t Saving The Model\n","Validation Loss Decreased(0.000216--->0.000214) \t Saving The Model\n","Validation Loss Decreased(0.000214--->0.000212) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0006973329709580867 \t Validation Loss: 0.00021037129044998437 \n","\n","Validation Loss Decreased(0.000212--->0.000210) \t Saving The Model\n","Validation Loss Decreased(0.000210--->0.000208) \t Saving The Model\n","Validation Loss Decreased(0.000208--->0.000206) \t Saving The Model\n","Validation Loss Decreased(0.000206--->0.000205) \t Saving The Model\n","Validation Loss Decreased(0.000205--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000201) \t Saving The Model\n","Validation Loss Decreased(0.000201--->0.000199) \t Saving The Model\n","Validation Loss Decreased(0.000199--->0.000197) \t Saving The Model\n","Validation Loss Decreased(0.000197--->0.000195) \t Saving The Model\n","Validation Loss Decreased(0.000195--->0.000193) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0006478905534095247 \t Validation Loss: 0.00019166535639669746 \n","\n","Validation Loss Decreased(0.000193--->0.000192) \t Saving The Model\n","Validation Loss Decreased(0.000192--->0.000190) \t Saving The Model\n","Validation Loss Decreased(0.000190--->0.000188) \t Saving The Model\n","Validation Loss Decreased(0.000188--->0.000187) \t Saving The Model\n","Validation Loss Decreased(0.000187--->0.000185) \t Saving The Model\n","Validation Loss Decreased(0.000185--->0.000183) \t Saving The Model\n","Validation Loss Decreased(0.000183--->0.000182) \t Saving The Model\n","Validation Loss Decreased(0.000182--->0.000180) \t Saving The Model\n","Validation Loss Decreased(0.000180--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000177) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0006013134561726474 \t Validation Loss: 0.0001752650859998539 \n","\n","Validation Loss Decreased(0.000177--->0.000175) \t Saving The Model\n","Validation Loss Decreased(0.000175--->0.000174) \t Saving The Model\n","Validation Loss Decreased(0.000174--->0.000172) \t Saving The Model\n","Validation Loss Decreased(0.000172--->0.000171) \t Saving The Model\n","Validation Loss Decreased(0.000171--->0.000169) \t Saving The Model\n","Validation Loss Decreased(0.000169--->0.000168) \t Saving The Model\n","Validation Loss Decreased(0.000168--->0.000167) \t Saving The Model\n","Validation Loss Decreased(0.000167--->0.000165) \t Saving The Model\n","Validation Loss Decreased(0.000165--->0.000164) \t Saving The Model\n","Validation Loss Decreased(0.000164--->0.000163) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0005588362555499771 \t Validation Loss: 0.00016145953850355 \n","\n","Validation Loss Decreased(0.000163--->0.000161) \t Saving The Model\n","Validation Loss Decreased(0.000161--->0.000160) \t Saving The Model\n","Validation Loss Decreased(0.000160--->0.000159) \t Saving The Model\n","Validation Loss Decreased(0.000159--->0.000158) \t Saving The Model\n","Validation Loss Decreased(0.000158--->0.000157) \t Saving The Model\n","Validation Loss Decreased(0.000157--->0.000156) \t Saving The Model\n","Validation Loss Decreased(0.000156--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000154) \t Saving The Model\n","Validation Loss Decreased(0.000154--->0.000153) \t Saving The Model\n","Validation Loss Decreased(0.000153--->0.000152) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0005224947444730788 \t Validation Loss: 0.0001511659793322906 \n","\n","Validation Loss Decreased(0.000152--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000150) \t Saving The Model\n","Validation Loss Decreased(0.000150--->0.000150) \t Saving The Model\n","Validation Loss Decreased(0.000150--->0.000149) \t Saving The Model\n","Validation Loss Decreased(0.000149--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000147) \t Saving The Model\n","Validation Loss Decreased(0.000147--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000145) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.000492972593747254 \t Validation Loss: 0.00014489212480839342 \n","\n","Validation Loss Decreased(0.000145--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000142) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:58:38,405]\u001b[0m Trial 14 finished with value: 7.600648154986115 and parameters: {'batch_size': 256, 'learning_rate': 0.0018104657931356074, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n"," \n","SMAPE : 7.600648154986115\n","\n"," Epoch 0 \t Training Loss: 0.023337301856372505 \t Validation Loss: 0.004281409550458193 \n","\n","Validation Loss Decreased(inf--->0.004281) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.021398214914370328 \t Validation Loss: 0.0047963447868824005 \n","\n","\n"," Epoch 20 \t Training Loss: 0.020328092679847032 \t Validation Loss: 0.005513011943548918 \n","\n","\n"," Epoch 30 \t Training Loss: 0.01969782344531268 \t Validation Loss: 0.006155448965728283 \n","\n","\n"," Epoch 40 \t Training Loss: 0.019294802623335272 \t Validation Loss: 0.006659628823399544 \n","\n","\n"," Epoch 50 \t Training Loss: 0.019005838898010552 \t Validation Loss: 0.0070274039171636105 \n","\n","\n"," Epoch 60 \t Training Loss: 0.01877111312933266 \t Validation Loss: 0.007283995393663645 \n","\n","\n"," Epoch 70 \t Training Loss: 0.01855869876453653 \t Validation Loss: 0.007458132691681385 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0183509488706477 \t Validation Loss: 0.007574126590043306 \n","\n","\n"," Epoch 90 \t Training Loss: 0.01813732594018802 \t Validation Loss: 0.007650140207260847 \n","\n","\n"," Epoch 100 \t Training Loss: 0.017910695867612958 \t Validation Loss: 0.007698815315961838 \n","\n","\n"," Epoch 110 \t Training Loss: 0.017665339750237763 \t Validation Loss: 0.007728659547865391 \n","\n","\n"," Epoch 120 \t Training Loss: 0.017395860166288912 \t Validation Loss: 0.007745188660919666 \n","\n","\n"," Epoch 130 \t Training Loss: 0.017096392170060426 \t Validation Loss: 0.007751899771392345 \n","\n","\n"," Epoch 140 \t Training Loss: 0.016759944090154022 \t Validation Loss: 0.007750897202640772 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:58:46,299]\u001b[0m Trial 15 finished with value: 65.82442955569304 and parameters: {'batch_size': 256, 'learning_rate': 0.00011911095214186437, 'hidden_size': 16, 'num_layers': 2}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 65.82442955569304\n","\n"," Epoch 0 \t Training Loss: 0.022600098080667003 \t Validation Loss: 0.004776684567332268 \n","\n","Validation Loss Decreased(inf--->0.004777) \t Saving The Model\n","Validation Loss Decreased(0.004777--->0.004125) \t Saving The Model\n","Validation Loss Decreased(0.004125--->0.003783) \t Saving The Model\n","Validation Loss Decreased(0.003783--->0.003589) \t Saving The Model\n","Validation Loss Decreased(0.003589--->0.003473) \t Saving The Model\n","Validation Loss Decreased(0.003473--->0.003399) \t Saving The Model\n","Validation Loss Decreased(0.003399--->0.003348) \t Saving The Model\n","Validation Loss Decreased(0.003348--->0.003305) \t Saving The Model\n","Validation Loss Decreased(0.003305--->0.003262) \t Saving The Model\n","Validation Loss Decreased(0.003262--->0.003217) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.018109074062002555 \t Validation Loss: 0.0031654549529775977 \n","\n","Validation Loss Decreased(0.003217--->0.003165) \t Saving The Model\n","Validation Loss Decreased(0.003165--->0.003108) \t Saving The Model\n","Validation Loss Decreased(0.003108--->0.003046) \t Saving The Model\n","Validation Loss Decreased(0.003046--->0.002978) \t Saving The Model\n","Validation Loss Decreased(0.002978--->0.002905) \t Saving The Model\n","Validation Loss Decreased(0.002905--->0.002827) \t Saving The Model\n","Validation Loss Decreased(0.002827--->0.002745) \t Saving The Model\n","Validation Loss Decreased(0.002745--->0.002657) \t Saving The Model\n","Validation Loss Decreased(0.002657--->0.002563) \t Saving The Model\n","Validation Loss Decreased(0.002563--->0.002463) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.013693134899118118 \t Validation Loss: 0.0023555299849249423 \n","\n","Validation Loss Decreased(0.002463--->0.002356) \t Saving The Model\n","Validation Loss Decreased(0.002356--->0.002240) \t Saving The Model\n","Validation Loss Decreased(0.002240--->0.002115) \t Saving The Model\n","Validation Loss Decreased(0.002115--->0.001980) \t Saving The Model\n","Validation Loss Decreased(0.001980--->0.001834) \t Saving The Model\n","Validation Loss Decreased(0.001834--->0.001678) \t Saving The Model\n","Validation Loss Decreased(0.001678--->0.001511) \t Saving The Model\n","Validation Loss Decreased(0.001511--->0.001336) \t Saving The Model\n","Validation Loss Decreased(0.001336--->0.001160) \t Saving The Model\n","Validation Loss Decreased(0.001160--->0.000991) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0028835540163397256 \t Validation Loss: 0.0008451264002360404 \n","\n","Validation Loss Decreased(0.000991--->0.000845) \t Saving The Model\n","Validation Loss Decreased(0.000845--->0.000740) \t Saving The Model\n","Validation Loss Decreased(0.000740--->0.000712) \t Saving The Model\n","Validation Loss Decreased(0.000712--->0.000690) \t Saving The Model\n","Validation Loss Decreased(0.000690--->0.000682) \t Saving The Model\n","Validation Loss Decreased(0.000682--->0.000677) \t Saving The Model\n","Validation Loss Decreased(0.000677--->0.000660) \t Saving The Model\n","Validation Loss Decreased(0.000660--->0.000645) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.001433801387715253 \t Validation Loss: 0.0006365418958012015 \n","\n","Validation Loss Decreased(0.000645--->0.000637) \t Saving The Model\n","Validation Loss Decreased(0.000637--->0.000627) \t Saving The Model\n","Validation Loss Decreased(0.000627--->0.000615) \t Saving The Model\n","Validation Loss Decreased(0.000615--->0.000606) \t Saving The Model\n","Validation Loss Decreased(0.000606--->0.000596) \t Saving The Model\n","Validation Loss Decreased(0.000596--->0.000586) \t Saving The Model\n","Validation Loss Decreased(0.000586--->0.000576) \t Saving The Model\n","Validation Loss Decreased(0.000576--->0.000567) \t Saving The Model\n","Validation Loss Decreased(0.000567--->0.000558) \t Saving The Model\n","Validation Loss Decreased(0.000558--->0.000549) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.001313652591176963 \t Validation Loss: 0.0005401583621278405 \n","\n","Validation Loss Decreased(0.000549--->0.000540) \t Saving The Model\n","Validation Loss Decreased(0.000540--->0.000532) \t Saving The Model\n","Validation Loss Decreased(0.000532--->0.000524) \t Saving The Model\n","Validation Loss Decreased(0.000524--->0.000516) \t Saving The Model\n","Validation Loss Decreased(0.000516--->0.000508) \t Saving The Model\n","Validation Loss Decreased(0.000508--->0.000500) \t Saving The Model\n","Validation Loss Decreased(0.000500--->0.000493) \t Saving The Model\n","Validation Loss Decreased(0.000493--->0.000486) \t Saving The Model\n","Validation Loss Decreased(0.000486--->0.000479) \t Saving The Model\n","Validation Loss Decreased(0.000479--->0.000472) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0012175969573685766 \t Validation Loss: 0.00046539616596419364 \n","\n","Validation Loss Decreased(0.000472--->0.000465) \t Saving The Model\n","Validation Loss Decreased(0.000465--->0.000459) \t Saving The Model\n","Validation Loss Decreased(0.000459--->0.000453) \t Saving The Model\n","Validation Loss Decreased(0.000453--->0.000447) \t Saving The Model\n","Validation Loss Decreased(0.000447--->0.000441) \t Saving The Model\n","Validation Loss Decreased(0.000441--->0.000435) \t Saving The Model\n","Validation Loss Decreased(0.000435--->0.000429) \t Saving The Model\n","Validation Loss Decreased(0.000429--->0.000424) \t Saving The Model\n","Validation Loss Decreased(0.000424--->0.000418) \t Saving The Model\n","Validation Loss Decreased(0.000418--->0.000413) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0011372236289649404 \t Validation Loss: 0.0004078437923453748 \n","\n","Validation Loss Decreased(0.000413--->0.000408) \t Saving The Model\n","Validation Loss Decreased(0.000408--->0.000403) \t Saving The Model\n","Validation Loss Decreased(0.000403--->0.000398) \t Saving The Model\n","Validation Loss Decreased(0.000398--->0.000393) \t Saving The Model\n","Validation Loss Decreased(0.000393--->0.000389) \t Saving The Model\n","Validation Loss Decreased(0.000389--->0.000384) \t Saving The Model\n","Validation Loss Decreased(0.000384--->0.000380) \t Saving The Model\n","Validation Loss Decreased(0.000380--->0.000375) \t Saving The Model\n","Validation Loss Decreased(0.000375--->0.000371) \t Saving The Model\n","Validation Loss Decreased(0.000371--->0.000367) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0010673782931657375 \t Validation Loss: 0.00036274998274166137 \n","\n","Validation Loss Decreased(0.000367--->0.000363) \t Saving The Model\n","Validation Loss Decreased(0.000363--->0.000359) \t Saving The Model\n","Validation Loss Decreased(0.000359--->0.000355) \t Saving The Model\n","Validation Loss Decreased(0.000355--->0.000351) \t Saving The Model\n","Validation Loss Decreased(0.000351--->0.000347) \t Saving The Model\n","Validation Loss Decreased(0.000347--->0.000344) \t Saving The Model\n","Validation Loss Decreased(0.000344--->0.000340) \t Saving The Model\n","Validation Loss Decreased(0.000340--->0.000337) \t Saving The Model\n","Validation Loss Decreased(0.000337--->0.000333) \t Saving The Model\n","Validation Loss Decreased(0.000333--->0.000330) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0010048625471037148 \t Validation Loss: 0.0003266366838943213 \n","\n","Validation Loss Decreased(0.000330--->0.000327) \t Saving The Model\n","Validation Loss Decreased(0.000327--->0.000323) \t Saving The Model\n","Validation Loss Decreased(0.000323--->0.000320) \t Saving The Model\n","Validation Loss Decreased(0.000320--->0.000317) \t Saving The Model\n","Validation Loss Decreased(0.000317--->0.000314) \t Saving The Model\n","Validation Loss Decreased(0.000314--->0.000311) \t Saving The Model\n","Validation Loss Decreased(0.000311--->0.000308) \t Saving The Model\n","Validation Loss Decreased(0.000308--->0.000305) \t Saving The Model\n","Validation Loss Decreased(0.000305--->0.000303) \t Saving The Model\n","Validation Loss Decreased(0.000303--->0.000300) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0009476330880196267 \t Validation Loss: 0.00029701649327762425 \n","\n","Validation Loss Decreased(0.000300--->0.000297) \t Saving The Model\n","Validation Loss Decreased(0.000297--->0.000294) \t Saving The Model\n","Validation Loss Decreased(0.000294--->0.000292) \t Saving The Model\n","Validation Loss Decreased(0.000292--->0.000289) \t Saving The Model\n","Validation Loss Decreased(0.000289--->0.000287) \t Saving The Model\n","Validation Loss Decreased(0.000287--->0.000284) \t Saving The Model\n","Validation Loss Decreased(0.000284--->0.000282) \t Saving The Model\n","Validation Loss Decreased(0.000282--->0.000279) \t Saving The Model\n","Validation Loss Decreased(0.000279--->0.000277) \t Saving The Model\n","Validation Loss Decreased(0.000277--->0.000274) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.000894338927017608 \t Validation Loss: 0.0002721287601161748 \n","\n","Validation Loss Decreased(0.000274--->0.000272) \t Saving The Model\n","Validation Loss Decreased(0.000272--->0.000270) \t Saving The Model\n","Validation Loss Decreased(0.000270--->0.000268) \t Saving The Model\n","Validation Loss Decreased(0.000268--->0.000265) \t Saving The Model\n","Validation Loss Decreased(0.000265--->0.000263) \t Saving The Model\n","Validation Loss Decreased(0.000263--->0.000261) \t Saving The Model\n","Validation Loss Decreased(0.000261--->0.000259) \t Saving The Model\n","Validation Loss Decreased(0.000259--->0.000257) \t Saving The Model\n","Validation Loss Decreased(0.000257--->0.000255) \t Saving The Model\n","Validation Loss Decreased(0.000255--->0.000253) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0008440847442605966 \t Validation Loss: 0.00025073068536585197 \n","\n","Validation Loss Decreased(0.000253--->0.000251) \t Saving The Model\n","Validation Loss Decreased(0.000251--->0.000249) \t Saving The Model\n","Validation Loss Decreased(0.000249--->0.000247) \t Saving The Model\n","Validation Loss Decreased(0.000247--->0.000245) \t Saving The Model\n","Validation Loss Decreased(0.000245--->0.000243) \t Saving The Model\n","Validation Loss Decreased(0.000243--->0.000241) \t Saving The Model\n","Validation Loss Decreased(0.000241--->0.000239) \t Saving The Model\n","Validation Loss Decreased(0.000239--->0.000237) \t Saving The Model\n","Validation Loss Decreased(0.000237--->0.000236) \t Saving The Model\n","Validation Loss Decreased(0.000236--->0.000234) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0007963435047193863 \t Validation Loss: 0.00023197226983029395 \n","\n","Validation Loss Decreased(0.000234--->0.000232) \t Saving The Model\n","Validation Loss Decreased(0.000232--->0.000230) \t Saving The Model\n","Validation Loss Decreased(0.000230--->0.000228) \t Saving The Model\n","Validation Loss Decreased(0.000228--->0.000227) \t Saving The Model\n","Validation Loss Decreased(0.000227--->0.000225) \t Saving The Model\n","Validation Loss Decreased(0.000225--->0.000223) \t Saving The Model\n","Validation Loss Decreased(0.000223--->0.000222) \t Saving The Model\n","Validation Loss Decreased(0.000222--->0.000220) \t Saving The Model\n","Validation Loss Decreased(0.000220--->0.000219) \t Saving The Model\n","Validation Loss Decreased(0.000219--->0.000217) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.0007509821386650271 \t Validation Loss: 0.00021535328414756805 \n","\n","Validation Loss Decreased(0.000217--->0.000215) \t Saving The Model\n","Validation Loss Decreased(0.000215--->0.000214) \t Saving The Model\n","Validation Loss Decreased(0.000214--->0.000212) \t Saving The Model\n","Validation Loss Decreased(0.000212--->0.000211) \t Saving The Model\n","Validation Loss Decreased(0.000211--->0.000209) \t Saving The Model\n","Validation Loss Decreased(0.000209--->0.000208) \t Saving The Model\n","Validation Loss Decreased(0.000208--->0.000206) \t Saving The Model\n","Validation Loss Decreased(0.000206--->0.000205) \t Saving The Model\n","Validation Loss Decreased(0.000205--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000202) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:58:54,082]\u001b[0m Trial 16 finished with value: 9.717228741628936 and parameters: {'batch_size': 128, 'learning_rate': 0.0007365177864819695, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 9.717228741628936\n","\n"," Epoch 0 \t Training Loss: 0.07807017024606466 \t Validation Loss: 0.031304843723773956 \n","\n","Validation Loss Decreased(inf--->0.031305) \t Saving The Model\n","Validation Loss Decreased(0.031305--->0.008534) \t Saving The Model\n","Validation Loss Decreased(0.008534--->0.007490) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.021645712971803732 \t Validation Loss: 0.006898697931319475 \n","\n","Validation Loss Decreased(0.007490--->0.006899) \t Saving The Model\n","Validation Loss Decreased(0.006899--->0.005984) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.010362251370679587 \t Validation Loss: 0.009098555892705917 \n","\n","Validation Loss Decreased(0.005984--->0.005526) \t Saving The Model\n","Validation Loss Decreased(0.005526--->0.004577) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.017496184795163572 \t Validation Loss: 0.007514478638768196 \n","\n","\n"," Epoch 40 \t Training Loss: 0.009594074741471559 \t Validation Loss: 0.007962759584188461 \n","\n","Validation Loss Decreased(0.004577--->0.001695) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.003119527013041079 \t Validation Loss: 0.007864811457693577 \n","\n","\n"," Epoch 60 \t Training Loss: 0.0034595143661135808 \t Validation Loss: 0.008624166250228882 \n","\n","\n"," Epoch 70 \t Training Loss: 0.0027585321222431958 \t Validation Loss: 0.00602823356166482 \n","\n","\n"," Epoch 80 \t Training Loss: 0.007887682906584814 \t Validation Loss: 0.004446892533451319 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0027256806642981246 \t Validation Loss: 0.007836109027266502 \n","\n","\n"," Epoch 100 \t Training Loss: 0.002375955620664172 \t Validation Loss: 0.007177997380495071 \n","\n","\n"," Epoch 110 \t Training Loss: 0.01029472192749381 \t Validation Loss: 0.009754774160683155 \n","\n","\n"," Epoch 120 \t Training Loss: 0.0030827784503344446 \t Validation Loss: 0.008481889963150024 \n","\n","\n"," Epoch 130 \t Training Loss: 0.007210051757283509 \t Validation Loss: 0.00981979537755251 \n","\n","\n"," Epoch 140 \t Training Loss: 0.003007256716955453 \t Validation Loss: 0.00840742327272892 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 02:59:43,165]\u001b[0m Trial 17 finished with value: 98.97780946325891 and parameters: {'batch_size': 256, 'learning_rate': 0.004011458282217028, 'hidden_size': 512, 'num_layers': 2}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 98.97780946325891\n","\n"," Epoch 0 \t Training Loss: 21.228917613625526 \t Validation Loss: 12.509788513183594 \n","\n","Validation Loss Decreased(inf--->12.509789) \t Saving The Model\n","Validation Loss Decreased(12.509789--->6.274574) \t Saving The Model\n","Validation Loss Decreased(6.274574--->1.059762) \t Saving The Model\n","Validation Loss Decreased(1.059762--->0.897352) \t Saving The Model\n","Validation Loss Decreased(0.897352--->0.308266) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 6.929881930351257 \t Validation Loss: 1.2558940649032593 \n","\n","Validation Loss Decreased(0.308266--->0.181670) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 2.135420262813568 \t Validation Loss: 1.9266468286514282 \n","\n","Validation Loss Decreased(0.181670--->0.136595) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.3949918672442436 \t Validation Loss: 0.237365260720253 \n","\n","Validation Loss Decreased(0.136595--->0.125633) \t Saving The Model\n","Validation Loss Decreased(0.125633--->0.073810) \t Saving The Model\n","Validation Loss Decreased(0.073810--->0.068174) \t Saving The Model\n","Validation Loss Decreased(0.068174--->0.064108) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.06903629098087549 \t Validation Loss: 0.06471648067235947 \n","\n","Validation Loss Decreased(0.064108--->0.059984) \t Saving The Model\n","Validation Loss Decreased(0.059984--->0.058728) \t Saving The Model\n","Validation Loss Decreased(0.058728--->0.049680) \t Saving The Model\n","Validation Loss Decreased(0.049680--->0.049452) \t Saving The Model\n","Validation Loss Decreased(0.049452--->0.047723) \t Saving The Model\n","Validation Loss Decreased(0.047723--->0.046868) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.044382656924426556 \t Validation Loss: 0.041081324219703674 \n","\n","Validation Loss Decreased(0.046868--->0.041081) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.04218830168247223 \t Validation Loss: 0.049582187086343765 \n","\n","Validation Loss Decreased(0.041081--->0.040977) \t Saving The Model\n","Validation Loss Decreased(0.040977--->0.039061) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.03580336086452007 \t Validation Loss: 0.04471238702535629 \n","\n","Validation Loss Decreased(0.039061--->0.035145) \t Saving The Model\n","Validation Loss Decreased(0.035145--->0.034417) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.03372020833194256 \t Validation Loss: 0.03466569632291794 \n","\n","Validation Loss Decreased(0.034417--->0.033348) \t Saving The Model\n","Validation Loss Decreased(0.033348--->0.030103) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.026815474499017 \t Validation Loss: 0.03940587490797043 \n","\n","\n"," Epoch 100 \t Training Loss: 0.026941771153360605 \t Validation Loss: 0.03861042484641075 \n","\n","\n"," Epoch 110 \t Training Loss: 0.02538066217675805 \t Validation Loss: 0.031234094873070717 \n","\n","Validation Loss Decreased(0.030103--->0.026696) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0245357695966959 \t Validation Loss: 0.031866323202848434 \n","\n","\n"," Epoch 130 \t Training Loss: 0.023926568450406194 \t Validation Loss: 0.0271449014544487 \n","\n","\n"," Epoch 140 \t Training Loss: 0.026442420668900013 \t Validation Loss: 0.03140677139163017 \n","\n","Validation Loss Decreased(0.026696--->0.026464) \t Saving The Model\n","Validation Loss Decreased(0.026464--->0.023015) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:00:10,006]\u001b[0m Trial 18 finished with value: 99.05855152559121 and parameters: {'batch_size': 256, 'learning_rate': 0.025547960067989626, 'hidden_size': 256, 'num_layers': 3}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 99.05855152559121\n","\n"," Epoch 0 \t Training Loss: 0.030754232575418428 \t Validation Loss: 0.004567455034703016 \n","\n","Validation Loss Decreased(inf--->0.004567) \t Saving The Model\n","Validation Loss Decreased(0.004567--->0.002964) \t Saving The Model\n","Validation Loss Decreased(0.002964--->0.002867) \t Saving The Model\n","Validation Loss Decreased(0.002867--->0.000797) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.014315226348116994 \t Validation Loss: 0.002341603394597769 \n","\n","Validation Loss Decreased(0.000797--->0.000336) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.004884657384536695 \t Validation Loss: 0.007187822367995977 \n","\n","\n"," Epoch 30 \t Training Loss: 0.0016282675642287359 \t Validation Loss: 0.0004780819872394204 \n","\n","Validation Loss Decreased(0.000336--->0.000334) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0010467243337188847 \t Validation Loss: 0.00020917422079946846 \n","\n","Validation Loss Decreased(0.000334--->0.000209) \t Saving The Model\n","Validation Loss Decreased(0.000209--->0.000202) \t Saving The Model\n","Validation Loss Decreased(0.000202--->0.000186) \t Saving The Model\n","Validation Loss Decreased(0.000186--->0.000180) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.000814079561678227 \t Validation Loss: 0.00018261375953443348 \n","\n","\n"," Epoch 60 \t Training Loss: 0.0007602751556987641 \t Validation Loss: 0.0001813549897633493 \n","\n","Validation Loss Decreased(0.000180--->0.000179) \t Saving The Model\n","Validation Loss Decreased(0.000179--->0.000179) \t Saving The Model\n","Validation Loss Decreased(0.000179--->0.000179) \t Saving The Model\n","Validation Loss Decreased(0.000179--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000178) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.000708615974872373 \t Validation Loss: 0.000177772220922634 \n","\n","Validation Loss Decreased(0.000178--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000177) \t Saving The Model\n","Validation Loss Decreased(0.000177--->0.000177) \t Saving The Model\n","Validation Loss Decreased(0.000177--->0.000177) \t Saving The Model\n","Validation Loss Decreased(0.000177--->0.000177) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0006564558480022242 \t Validation Loss: 0.00017752591520547867 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0006043553021299886 \t Validation Loss: 0.00018229526176583022 \n","\n","\n"," Epoch 100 \t Training Loss: 0.0005603261597570963 \t Validation Loss: 0.00019232835620641708 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0005386609409470111 \t Validation Loss: 0.00019848473311867565 \n","\n","\n"," Epoch 120 \t Training Loss: 0.0005296716499287868 \t Validation Loss: 0.00019579572835937142 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0005232520361460047 \t Validation Loss: 0.00019323528977110982 \n","\n","\n"," Epoch 140 \t Training Loss: 0.000517565778864082 \t Validation Loss: 0.00019090647401753813 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:01:20,508]\u001b[0m Trial 19 finished with value: 8.755615723445256 and parameters: {'batch_size': 256, 'learning_rate': 0.0003833180129562478, 'hidden_size': 1024, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 8.755615723445256\n","\n"," Epoch 0 \t Training Loss: 0.06976201904139348 \t Validation Loss: 0.03971212916076183 \n","\n","Validation Loss Decreased(inf--->0.039712) \t Saving The Model\n","Validation Loss Decreased(0.039712--->0.004338) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.012978409117619907 \t Validation Loss: 0.011673680040985346 \n","\n","\n"," Epoch 20 \t Training Loss: 0.004375089874624142 \t Validation Loss: 0.0096958689391613 \n","\n","\n"," Epoch 30 \t Training Loss: 0.004249941827895652 \t Validation Loss: 0.009323224425315857 \n","\n","\n"," Epoch 40 \t Training Loss: 0.004233377207128797 \t Validation Loss: 0.009035264141857624 \n","\n","\n"," Epoch 50 \t Training Loss: 0.004201944227263864 \t Validation Loss: 0.008626386988908052 \n","\n","\n"," Epoch 60 \t Training Loss: 0.004139461932936683 \t Validation Loss: 0.00794128654524684 \n","\n","\n"," Epoch 70 \t Training Loss: 0.004757786834878581 \t Validation Loss: 0.009090743493288755 \n","\n","\n"," Epoch 80 \t Training Loss: 0.003967066681070719 \t Validation Loss: 0.008580850437283516 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0042958960319603124 \t Validation Loss: 0.009341938653960824 \n","\n","\n"," Epoch 100 \t Training Loss: 0.0037509963689704557 \t Validation Loss: 0.00817582942545414 \n","\n","\n"," Epoch 110 \t Training Loss: 0.005036218998221946 \t Validation Loss: 0.005625138059258461 \n","\n","\n"," Epoch 120 \t Training Loss: 0.008136505211171294 \t Validation Loss: 0.007198564941063523 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0033406008868561393 \t Validation Loss: 0.008448120206594467 \n","\n","\n"," Epoch 140 \t Training Loss: 0.003361105501036426 \t Validation Loss: 0.008584081195294857 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:01:37,807]\u001b[0m Trial 20 finished with value: 76.31448470621373 and parameters: {'batch_size': 128, 'learning_rate': 0.02037667907590272, 'hidden_size': 16, 'num_layers': 3}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 76.31448470621373\n","\n"," Epoch 0 \t Training Loss: 0.020493665884714574 \t Validation Loss: 0.0038787173107266426 \n","\n","Validation Loss Decreased(inf--->0.003879) \t Saving The Model\n","Validation Loss Decreased(0.003879--->0.003372) \t Saving The Model\n","Validation Loss Decreased(0.003372--->0.003196) \t Saving The Model\n","Validation Loss Decreased(0.003196--->0.003147) \t Saving The Model\n","Validation Loss Decreased(0.003147--->0.003093) \t Saving The Model\n","Validation Loss Decreased(0.003093--->0.002976) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.01408273505512625 \t Validation Loss: 0.00282205524854362 \n","\n","Validation Loss Decreased(0.002976--->0.002822) \t Saving The Model\n","Validation Loss Decreased(0.002822--->0.002644) \t Saving The Model\n","Validation Loss Decreased(0.002644--->0.002453) \t Saving The Model\n","Validation Loss Decreased(0.002453--->0.002255) \t Saving The Model\n","Validation Loss Decreased(0.002255--->0.002054) \t Saving The Model\n","Validation Loss Decreased(0.002054--->0.001846) \t Saving The Model\n","Validation Loss Decreased(0.001846--->0.001628) \t Saving The Model\n","Validation Loss Decreased(0.001628--->0.001393) \t Saving The Model\n","Validation Loss Decreased(0.001393--->0.001137) \t Saving The Model\n","Validation Loss Decreased(0.001137--->0.000868) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.0020527564083749894 \t Validation Loss: 0.0006228119018487632 \n","\n","Validation Loss Decreased(0.000868--->0.000623) \t Saving The Model\n","Validation Loss Decreased(0.000623--->0.000482) \t Saving The Model\n","Validation Loss Decreased(0.000482--->0.000457) \t Saving The Model\n","Validation Loss Decreased(0.000457--->0.000426) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0011048029045923613 \t Validation Loss: 0.0004192618653178215 \n","\n","Validation Loss Decreased(0.000426--->0.000419) \t Saving The Model\n","Validation Loss Decreased(0.000419--->0.000417) \t Saving The Model\n","Validation Loss Decreased(0.000417--->0.000416) \t Saving The Model\n","Validation Loss Decreased(0.000416--->0.000410) \t Saving The Model\n","Validation Loss Decreased(0.000410--->0.000397) \t Saving The Model\n","Validation Loss Decreased(0.000397--->0.000388) \t Saving The Model\n","Validation Loss Decreased(0.000388--->0.000380) \t Saving The Model\n","Validation Loss Decreased(0.000380--->0.000374) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.001002770599370706 \t Validation Loss: 0.00037028329097665846 \n","\n","Validation Loss Decreased(0.000374--->0.000370) \t Saving The Model\n","Validation Loss Decreased(0.000370--->0.000367) \t Saving The Model\n","Validation Loss Decreased(0.000367--->0.000363) \t Saving The Model\n","Validation Loss Decreased(0.000363--->0.000357) \t Saving The Model\n","Validation Loss Decreased(0.000357--->0.000351) \t Saving The Model\n","Validation Loss Decreased(0.000351--->0.000345) \t Saving The Model\n","Validation Loss Decreased(0.000345--->0.000340) \t Saving The Model\n","Validation Loss Decreased(0.000340--->0.000335) \t Saving The Model\n","Validation Loss Decreased(0.000335--->0.000331) \t Saving The Model\n","Validation Loss Decreased(0.000331--->0.000327) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0009249908798665274 \t Validation Loss: 0.00032250393996946514 \n","\n","Validation Loss Decreased(0.000327--->0.000323) \t Saving The Model\n","Validation Loss Decreased(0.000323--->0.000318) \t Saving The Model\n","Validation Loss Decreased(0.000318--->0.000314) \t Saving The Model\n","Validation Loss Decreased(0.000314--->0.000310) \t Saving The Model\n","Validation Loss Decreased(0.000310--->0.000305) \t Saving The Model\n","Validation Loss Decreased(0.000305--->0.000302) \t Saving The Model\n","Validation Loss Decreased(0.000302--->0.000298) \t Saving The Model\n","Validation Loss Decreased(0.000298--->0.000294) \t Saving The Model\n","Validation Loss Decreased(0.000294--->0.000291) \t Saving The Model\n","Validation Loss Decreased(0.000291--->0.000287) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0008571878643124364 \t Validation Loss: 0.00028353059315122664 \n","\n","Validation Loss Decreased(0.000287--->0.000284) \t Saving The Model\n","Validation Loss Decreased(0.000284--->0.000280) \t Saving The Model\n","Validation Loss Decreased(0.000280--->0.000277) \t Saving The Model\n","Validation Loss Decreased(0.000277--->0.000274) \t Saving The Model\n","Validation Loss Decreased(0.000274--->0.000270) \t Saving The Model\n","Validation Loss Decreased(0.000270--->0.000267) \t Saving The Model\n","Validation Loss Decreased(0.000267--->0.000264) \t Saving The Model\n","Validation Loss Decreased(0.000264--->0.000261) \t Saving The Model\n","Validation Loss Decreased(0.000261--->0.000258) \t Saving The Model\n","Validation Loss Decreased(0.000258--->0.000256) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0007962523250171216 \t Validation Loss: 0.0002528685436118394 \n","\n","Validation Loss Decreased(0.000256--->0.000253) \t Saving The Model\n","Validation Loss Decreased(0.000253--->0.000250) \t Saving The Model\n","Validation Loss Decreased(0.000250--->0.000247) \t Saving The Model\n","Validation Loss Decreased(0.000247--->0.000245) \t Saving The Model\n","Validation Loss Decreased(0.000245--->0.000242) \t Saving The Model\n","Validation Loss Decreased(0.000242--->0.000240) \t Saving The Model\n","Validation Loss Decreased(0.000240--->0.000237) \t Saving The Model\n","Validation Loss Decreased(0.000237--->0.000235) \t Saving The Model\n","Validation Loss Decreased(0.000235--->0.000232) \t Saving The Model\n","Validation Loss Decreased(0.000232--->0.000230) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0007402250466839178 \t Validation Loss: 0.00022778099810238928 \n","\n","Validation Loss Decreased(0.000230--->0.000228) \t Saving The Model\n","Validation Loss Decreased(0.000228--->0.000226) \t Saving The Model\n","Validation Loss Decreased(0.000226--->0.000223) \t Saving The Model\n","Validation Loss Decreased(0.000223--->0.000221) \t Saving The Model\n","Validation Loss Decreased(0.000221--->0.000219) \t Saving The Model\n","Validation Loss Decreased(0.000219--->0.000217) \t Saving The Model\n","Validation Loss Decreased(0.000217--->0.000215) \t Saving The Model\n","Validation Loss Decreased(0.000215--->0.000213) \t Saving The Model\n","Validation Loss Decreased(0.000213--->0.000211) \t Saving The Model\n","Validation Loss Decreased(0.000211--->0.000209) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0006877468995298841 \t Validation Loss: 0.0002065207518171519 \n","\n","Validation Loss Decreased(0.000209--->0.000207) \t Saving The Model\n","Validation Loss Decreased(0.000207--->0.000205) \t Saving The Model\n","Validation Loss Decreased(0.000205--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000201) \t Saving The Model\n","Validation Loss Decreased(0.000201--->0.000199) \t Saving The Model\n","Validation Loss Decreased(0.000199--->0.000197) \t Saving The Model\n","Validation Loss Decreased(0.000197--->0.000195) \t Saving The Model\n","Validation Loss Decreased(0.000195--->0.000193) \t Saving The Model\n","Validation Loss Decreased(0.000193--->0.000192) \t Saving The Model\n","Validation Loss Decreased(0.000192--->0.000190) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0006380909517247346 \t Validation Loss: 0.000188005855306983 \n","\n","Validation Loss Decreased(0.000190--->0.000188) \t Saving The Model\n","Validation Loss Decreased(0.000188--->0.000186) \t Saving The Model\n","Validation Loss Decreased(0.000186--->0.000185) \t Saving The Model\n","Validation Loss Decreased(0.000185--->0.000183) \t Saving The Model\n","Validation Loss Decreased(0.000183--->0.000181) \t Saving The Model\n","Validation Loss Decreased(0.000181--->0.000180) \t Saving The Model\n","Validation Loss Decreased(0.000180--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000176) \t Saving The Model\n","Validation Loss Decreased(0.000176--->0.000175) \t Saving The Model\n","Validation Loss Decreased(0.000175--->0.000173) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0005914813227718696 \t Validation Loss: 0.0001718424173304811 \n","\n","Validation Loss Decreased(0.000173--->0.000172) \t Saving The Model\n","Validation Loss Decreased(0.000172--->0.000170) \t Saving The Model\n","Validation Loss Decreased(0.000170--->0.000169) \t Saving The Model\n","Validation Loss Decreased(0.000169--->0.000168) \t Saving The Model\n","Validation Loss Decreased(0.000168--->0.000166) \t Saving The Model\n","Validation Loss Decreased(0.000166--->0.000165) \t Saving The Model\n","Validation Loss Decreased(0.000165--->0.000163) \t Saving The Model\n","Validation Loss Decreased(0.000163--->0.000162) \t Saving The Model\n","Validation Loss Decreased(0.000162--->0.000161) \t Saving The Model\n","Validation Loss Decreased(0.000161--->0.000160) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0005494594606716419 \t Validation Loss: 0.00015849729243200272 \n","\n","Validation Loss Decreased(0.000160--->0.000158) \t Saving The Model\n","Validation Loss Decreased(0.000158--->0.000157) \t Saving The Model\n","Validation Loss Decreased(0.000157--->0.000156) \t Saving The Model\n","Validation Loss Decreased(0.000156--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000154) \t Saving The Model\n","Validation Loss Decreased(0.000154--->0.000153) \t Saving The Model\n","Validation Loss Decreased(0.000153--->0.000152) \t Saving The Model\n","Validation Loss Decreased(0.000152--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000150) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0005141306028235704 \t Validation Loss: 0.0001489657734055072 \n","\n","Validation Loss Decreased(0.000150--->0.000149) \t Saving The Model\n","Validation Loss Decreased(0.000149--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000147) \t Saving The Model\n","Validation Loss Decreased(0.000147--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.00048560419691057177 \t Validation Loss: 0.00014351095887832344 \n","\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000141) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:01:43,224]\u001b[0m Trial 21 finished with value: 7.535636463005223 and parameters: {'batch_size': 256, 'learning_rate': 0.001853923926093596, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.000141--->0.000141) \t Saving The Model\n"," \n","SMAPE : 7.535636463005223\n","\n"," Epoch 0 \t Training Loss: 0.020560587930958718 \t Validation Loss: 0.0038167594466358423 \n","\n","Validation Loss Decreased(inf--->0.003817) \t Saving The Model\n","Validation Loss Decreased(0.003817--->0.003325) \t Saving The Model\n","Validation Loss Decreased(0.003325--->0.003166) \t Saving The Model\n","Validation Loss Decreased(0.003166--->0.003132) \t Saving The Model\n","Validation Loss Decreased(0.003132--->0.003064) \t Saving The Model\n","Validation Loss Decreased(0.003064--->0.002925) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.013799495063722134 \t Validation Loss: 0.0027493592351675034 \n","\n","Validation Loss Decreased(0.002925--->0.002749) \t Saving The Model\n","Validation Loss Decreased(0.002749--->0.002552) \t Saving The Model\n","Validation Loss Decreased(0.002552--->0.002345) \t Saving The Model\n","Validation Loss Decreased(0.002345--->0.002134) \t Saving The Model\n","Validation Loss Decreased(0.002134--->0.001920) \t Saving The Model\n","Validation Loss Decreased(0.001920--->0.001700) \t Saving The Model\n","Validation Loss Decreased(0.001700--->0.001465) \t Saving The Model\n","Validation Loss Decreased(0.001465--->0.001210) \t Saving The Model\n","Validation Loss Decreased(0.001210--->0.000933) \t Saving The Model\n","Validation Loss Decreased(0.000933--->0.000664) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.0013094369169266429 \t Validation Loss: 0.00048392059397883713 \n","\n","Validation Loss Decreased(0.000664--->0.000484) \t Saving The Model\n","Validation Loss Decreased(0.000484--->0.000483) \t Saving The Model\n","Validation Loss Decreased(0.000483--->0.000454) \t Saving The Model\n","Validation Loss Decreased(0.000454--->0.000418) \t Saving The Model\n","Validation Loss Decreased(0.000418--->0.000410) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0010945321409963071 \t Validation Loss: 0.00040918891318142414 \n","\n","Validation Loss Decreased(0.000410--->0.000409) \t Saving The Model\n","Validation Loss Decreased(0.000409--->0.000409) \t Saving The Model\n","Validation Loss Decreased(0.000409--->0.000403) \t Saving The Model\n","Validation Loss Decreased(0.000403--->0.000390) \t Saving The Model\n","Validation Loss Decreased(0.000390--->0.000380) \t Saving The Model\n","Validation Loss Decreased(0.000380--->0.000373) \t Saving The Model\n","Validation Loss Decreased(0.000373--->0.000367) \t Saving The Model\n","Validation Loss Decreased(0.000367--->0.000363) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.000983726322374423 \t Validation Loss: 0.0003608076658565551 \n","\n","Validation Loss Decreased(0.000363--->0.000361) \t Saving The Model\n","Validation Loss Decreased(0.000361--->0.000356) \t Saving The Model\n","Validation Loss Decreased(0.000356--->0.000350) \t Saving The Model\n","Validation Loss Decreased(0.000350--->0.000344) \t Saving The Model\n","Validation Loss Decreased(0.000344--->0.000338) \t Saving The Model\n","Validation Loss Decreased(0.000338--->0.000333) \t Saving The Model\n","Validation Loss Decreased(0.000333--->0.000328) \t Saving The Model\n","Validation Loss Decreased(0.000328--->0.000324) \t Saving The Model\n","Validation Loss Decreased(0.000324--->0.000320) \t Saving The Model\n","Validation Loss Decreased(0.000320--->0.000316) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0009066410457307938 \t Validation Loss: 0.0003115120925940573 \n","\n","Validation Loss Decreased(0.000316--->0.000312) \t Saving The Model\n","Validation Loss Decreased(0.000312--->0.000307) \t Saving The Model\n","Validation Loss Decreased(0.000307--->0.000303) \t Saving The Model\n","Validation Loss Decreased(0.000303--->0.000299) \t Saving The Model\n","Validation Loss Decreased(0.000299--->0.000295) \t Saving The Model\n","Validation Loss Decreased(0.000295--->0.000291) \t Saving The Model\n","Validation Loss Decreased(0.000291--->0.000288) \t Saving The Model\n","Validation Loss Decreased(0.000288--->0.000284) \t Saving The Model\n","Validation Loss Decreased(0.000284--->0.000281) \t Saving The Model\n","Validation Loss Decreased(0.000281--->0.000277) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0008389556569454726 \t Validation Loss: 0.0002738243783824146 \n","\n","Validation Loss Decreased(0.000277--->0.000274) \t Saving The Model\n","Validation Loss Decreased(0.000274--->0.000271) \t Saving The Model\n","Validation Loss Decreased(0.000271--->0.000267) \t Saving The Model\n","Validation Loss Decreased(0.000267--->0.000264) \t Saving The Model\n","Validation Loss Decreased(0.000264--->0.000261) \t Saving The Model\n","Validation Loss Decreased(0.000261--->0.000258) \t Saving The Model\n","Validation Loss Decreased(0.000258--->0.000255) \t Saving The Model\n","Validation Loss Decreased(0.000255--->0.000252) \t Saving The Model\n","Validation Loss Decreased(0.000252--->0.000250) \t Saving The Model\n","Validation Loss Decreased(0.000250--->0.000247) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0007779362804285483 \t Validation Loss: 0.0002441892575006932 \n","\n","Validation Loss Decreased(0.000247--->0.000244) \t Saving The Model\n","Validation Loss Decreased(0.000244--->0.000242) \t Saving The Model\n","Validation Loss Decreased(0.000242--->0.000239) \t Saving The Model\n","Validation Loss Decreased(0.000239--->0.000236) \t Saving The Model\n","Validation Loss Decreased(0.000236--->0.000234) \t Saving The Model\n","Validation Loss Decreased(0.000234--->0.000231) \t Saving The Model\n","Validation Loss Decreased(0.000231--->0.000229) \t Saving The Model\n","Validation Loss Decreased(0.000229--->0.000227) \t Saving The Model\n","Validation Loss Decreased(0.000227--->0.000224) \t Saving The Model\n","Validation Loss Decreased(0.000224--->0.000222) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0007216287503979402 \t Validation Loss: 0.00021979758457746357 \n","\n","Validation Loss Decreased(0.000222--->0.000220) \t Saving The Model\n","Validation Loss Decreased(0.000220--->0.000218) \t Saving The Model\n","Validation Loss Decreased(0.000218--->0.000215) \t Saving The Model\n","Validation Loss Decreased(0.000215--->0.000213) \t Saving The Model\n","Validation Loss Decreased(0.000213--->0.000211) \t Saving The Model\n","Validation Loss Decreased(0.000211--->0.000209) \t Saving The Model\n","Validation Loss Decreased(0.000209--->0.000207) \t Saving The Model\n","Validation Loss Decreased(0.000207--->0.000205) \t Saving The Model\n","Validation Loss Decreased(0.000205--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000201) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0006687253226118628 \t Validation Loss: 0.00019901589257642627 \n","\n","Validation Loss Decreased(0.000201--->0.000199) \t Saving The Model\n","Validation Loss Decreased(0.000199--->0.000197) \t Saving The Model\n","Validation Loss Decreased(0.000197--->0.000195) \t Saving The Model\n","Validation Loss Decreased(0.000195--->0.000193) \t Saving The Model\n","Validation Loss Decreased(0.000193--->0.000191) \t Saving The Model\n","Validation Loss Decreased(0.000191--->0.000190) \t Saving The Model\n","Validation Loss Decreased(0.000190--->0.000188) \t Saving The Model\n","Validation Loss Decreased(0.000188--->0.000186) \t Saving The Model\n","Validation Loss Decreased(0.000186--->0.000184) \t Saving The Model\n","Validation Loss Decreased(0.000184--->0.000183) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0006186907912706374 \t Validation Loss: 0.00018088408978655934 \n","\n","Validation Loss Decreased(0.000183--->0.000181) \t Saving The Model\n","Validation Loss Decreased(0.000181--->0.000179) \t Saving The Model\n","Validation Loss Decreased(0.000179--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000176) \t Saving The Model\n","Validation Loss Decreased(0.000176--->0.000174) \t Saving The Model\n","Validation Loss Decreased(0.000174--->0.000173) \t Saving The Model\n","Validation Loss Decreased(0.000173--->0.000171) \t Saving The Model\n","Validation Loss Decreased(0.000171--->0.000170) \t Saving The Model\n","Validation Loss Decreased(0.000170--->0.000168) \t Saving The Model\n","Validation Loss Decreased(0.000168--->0.000167) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0005722357982449466 \t Validation Loss: 0.00016528590640518814 \n","\n","Validation Loss Decreased(0.000167--->0.000165) \t Saving The Model\n","Validation Loss Decreased(0.000165--->0.000164) \t Saving The Model\n","Validation Loss Decreased(0.000164--->0.000163) \t Saving The Model\n","Validation Loss Decreased(0.000163--->0.000161) \t Saving The Model\n","Validation Loss Decreased(0.000161--->0.000160) \t Saving The Model\n","Validation Loss Decreased(0.000160--->0.000159) \t Saving The Model\n","Validation Loss Decreased(0.000159--->0.000157) \t Saving The Model\n","Validation Loss Decreased(0.000157--->0.000156) \t Saving The Model\n","Validation Loss Decreased(0.000156--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000154) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0005315321322996169 \t Validation Loss: 0.00015306616842281073 \n","\n","Validation Loss Decreased(0.000154--->0.000153) \t Saving The Model\n","Validation Loss Decreased(0.000153--->0.000152) \t Saving The Model\n","Validation Loss Decreased(0.000152--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000150) \t Saving The Model\n","Validation Loss Decreased(0.000150--->0.000149) \t Saving The Model\n","Validation Loss Decreased(0.000149--->0.000149) \t Saving The Model\n","Validation Loss Decreased(0.000149--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000147) \t Saving The Model\n","Validation Loss Decreased(0.000147--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000146) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0004984248616892728 \t Validation Loss: 0.00014519289834424853 \n","\n","Validation Loss Decreased(0.000146--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000143) \t Saving The Model\n","Validation Loss Decreased(0.000143--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000142) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.000471601734716387 \t Validation Loss: 0.0001412631681887433 \n","\n","Validation Loss Decreased(0.000142--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:01:48,740]\u001b[0m Trial 22 finished with value: 7.420230700405184 and parameters: {'batch_size': 256, 'learning_rate': 0.0019426165290821775, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n"," \n","SMAPE : 7.420230700405184\n","\n"," Epoch 0 \t Training Loss: 0.025975258788093925 \t Validation Loss: 0.0055712563917040825 \n","\n","Validation Loss Decreased(inf--->0.005571) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.01381274580489844 \t Validation Loss: 0.008133355528116226 \n","\n","\n"," Epoch 20 \t Training Loss: 0.003418389882426709 \t Validation Loss: 0.007772123906761408 \n","\n","\n"," Epoch 30 \t Training Loss: 0.00263979679584736 \t Validation Loss: 0.007882226258516312 \n","\n","\n"," Epoch 40 \t Training Loss: 0.0024878742697183043 \t Validation Loss: 0.00763020059093833 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0023583155270898715 \t Validation Loss: 0.00726376473903656 \n","\n","\n"," Epoch 60 \t Training Loss: 0.002242230490082875 \t Validation Loss: 0.006992135662585497 \n","\n","\n"," Epoch 70 \t Training Loss: 0.002112893656885717 \t Validation Loss: 0.006869933567941189 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0038922403182368726 \t Validation Loss: 0.00878913700580597 \n","\n","Validation Loss Decreased(0.005571--->0.005232) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0023101649712771177 \t Validation Loss: 0.0075188023038208485 \n","\n","\n"," Epoch 100 \t Training Loss: 0.00218705135921482 \t Validation Loss: 0.006900655571371317 \n","\n","\n"," Epoch 110 \t Training Loss: 0.002089540612359997 \t Validation Loss: 0.006743048783391714 \n","\n","\n"," Epoch 120 \t Training Loss: 0.002001133601879701 \t Validation Loss: 0.006673277821391821 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0018962581307278015 \t Validation Loss: 0.006779422517865896 \n","\n","Validation Loss Decreased(0.005232--->0.004441) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.006110622169217095 \t Validation Loss: 0.006230553612112999 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:01:56,692]\u001b[0m Trial 23 finished with value: 76.46287008437963 and parameters: {'batch_size': 256, 'learning_rate': 0.004497108022945475, 'hidden_size': 16, 'num_layers': 2}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 76.46287008437963\n","\n"," Epoch 0 \t Training Loss: 0.019690864835865796 \t Validation Loss: 0.005042002070695162 \n","\n","Validation Loss Decreased(inf--->0.005042) \t Saving The Model\n","Validation Loss Decreased(0.005042--->0.004577) \t Saving The Model\n","Validation Loss Decreased(0.004577--->0.004262) \t Saving The Model\n","Validation Loss Decreased(0.004262--->0.004032) \t Saving The Model\n","Validation Loss Decreased(0.004032--->0.003861) \t Saving The Model\n","Validation Loss Decreased(0.003861--->0.003730) \t Saving The Model\n","Validation Loss Decreased(0.003730--->0.003630) \t Saving The Model\n","Validation Loss Decreased(0.003630--->0.003553) \t Saving The Model\n","Validation Loss Decreased(0.003553--->0.003493) \t Saving The Model\n","Validation Loss Decreased(0.003493--->0.003444) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.017214625142514706 \t Validation Loss: 0.0034040934406220913 \n","\n","Validation Loss Decreased(0.003444--->0.003404) \t Saving The Model\n","Validation Loss Decreased(0.003404--->0.003370) \t Saving The Model\n","Validation Loss Decreased(0.003370--->0.003339) \t Saving The Model\n","Validation Loss Decreased(0.003339--->0.003309) \t Saving The Model\n","Validation Loss Decreased(0.003309--->0.003280) \t Saving The Model\n","Validation Loss Decreased(0.003280--->0.003250) \t Saving The Model\n","Validation Loss Decreased(0.003250--->0.003219) \t Saving The Model\n","Validation Loss Decreased(0.003219--->0.003186) \t Saving The Model\n","Validation Loss Decreased(0.003186--->0.003151) \t Saving The Model\n","Validation Loss Decreased(0.003151--->0.003114) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.015443159791175276 \t Validation Loss: 0.0030740753281861544 \n","\n","Validation Loss Decreased(0.003114--->0.003074) \t Saving The Model\n","Validation Loss Decreased(0.003074--->0.003032) \t Saving The Model\n","Validation Loss Decreased(0.003032--->0.002988) \t Saving The Model\n","Validation Loss Decreased(0.002988--->0.002941) \t Saving The Model\n","Validation Loss Decreased(0.002941--->0.002893) \t Saving The Model\n","Validation Loss Decreased(0.002893--->0.002842) \t Saving The Model\n","Validation Loss Decreased(0.002842--->0.002788) \t Saving The Model\n","Validation Loss Decreased(0.002788--->0.002732) \t Saving The Model\n","Validation Loss Decreased(0.002732--->0.002674) \t Saving The Model\n","Validation Loss Decreased(0.002674--->0.002613) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.012865123222582042 \t Validation Loss: 0.0025498962495476007 \n","\n","Validation Loss Decreased(0.002613--->0.002550) \t Saving The Model\n","Validation Loss Decreased(0.002550--->0.002483) \t Saving The Model\n","Validation Loss Decreased(0.002483--->0.002414) \t Saving The Model\n","Validation Loss Decreased(0.002414--->0.002341) \t Saving The Model\n","Validation Loss Decreased(0.002341--->0.002265) \t Saving The Model\n","Validation Loss Decreased(0.002265--->0.002185) \t Saving The Model\n","Validation Loss Decreased(0.002185--->0.002101) \t Saving The Model\n","Validation Loss Decreased(0.002101--->0.002013) \t Saving The Model\n","Validation Loss Decreased(0.002013--->0.001921) \t Saving The Model\n","Validation Loss Decreased(0.001921--->0.001824) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.008383182663237676 \t Validation Loss: 0.0017231027595698833 \n","\n","Validation Loss Decreased(0.001824--->0.001723) \t Saving The Model\n","Validation Loss Decreased(0.001723--->0.001618) \t Saving The Model\n","Validation Loss Decreased(0.001618--->0.001508) \t Saving The Model\n","Validation Loss Decreased(0.001508--->0.001395) \t Saving The Model\n","Validation Loss Decreased(0.001395--->0.001280) \t Saving The Model\n","Validation Loss Decreased(0.001280--->0.001163) \t Saving The Model\n","Validation Loss Decreased(0.001163--->0.001049) \t Saving The Model\n","Validation Loss Decreased(0.001049--->0.000940) \t Saving The Model\n","Validation Loss Decreased(0.000940--->0.000840) \t Saving The Model\n","Validation Loss Decreased(0.000840--->0.000754) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.001614111810340546 \t Validation Loss: 0.0006884562899358571 \n","\n","Validation Loss Decreased(0.000754--->0.000688) \t Saving The Model\n","Validation Loss Decreased(0.000688--->0.000647) \t Saving The Model\n","Validation Loss Decreased(0.000647--->0.000631) \t Saving The Model\n","Validation Loss Decreased(0.000631--->0.000630) \t Saving The Model\n","Validation Loss Decreased(0.000630--->0.000623) \t Saving The Model\n","Validation Loss Decreased(0.000623--->0.000619) \t Saving The Model\n","Validation Loss Decreased(0.000619--->0.000617) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0013241345259302761 \t Validation Loss: 0.0006154588772915304 \n","\n","Validation Loss Decreased(0.000617--->0.000615) \t Saving The Model\n","Validation Loss Decreased(0.000615--->0.000611) \t Saving The Model\n","Validation Loss Decreased(0.000611--->0.000604) \t Saving The Model\n","Validation Loss Decreased(0.000604--->0.000596) \t Saving The Model\n","Validation Loss Decreased(0.000596--->0.000589) \t Saving The Model\n","Validation Loss Decreased(0.000589--->0.000584) \t Saving The Model\n","Validation Loss Decreased(0.000584--->0.000579) \t Saving The Model\n","Validation Loss Decreased(0.000579--->0.000575) \t Saving The Model\n","Validation Loss Decreased(0.000575--->0.000570) \t Saving The Model\n","Validation Loss Decreased(0.000570--->0.000565) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0012484402213885915 \t Validation Loss: 0.0005601263255812228 \n","\n","Validation Loss Decreased(0.000565--->0.000560) \t Saving The Model\n","Validation Loss Decreased(0.000560--->0.000556) \t Saving The Model\n","Validation Loss Decreased(0.000556--->0.000551) \t Saving The Model\n","Validation Loss Decreased(0.000551--->0.000546) \t Saving The Model\n","Validation Loss Decreased(0.000546--->0.000541) \t Saving The Model\n","Validation Loss Decreased(0.000541--->0.000536) \t Saving The Model\n","Validation Loss Decreased(0.000536--->0.000531) \t Saving The Model\n","Validation Loss Decreased(0.000531--->0.000526) \t Saving The Model\n","Validation Loss Decreased(0.000526--->0.000522) \t Saving The Model\n","Validation Loss Decreased(0.000522--->0.000517) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0011880063393618912 \t Validation Loss: 0.0005122924922034144 \n","\n","Validation Loss Decreased(0.000517--->0.000512) \t Saving The Model\n","Validation Loss Decreased(0.000512--->0.000508) \t Saving The Model\n","Validation Loss Decreased(0.000508--->0.000503) \t Saving The Model\n","Validation Loss Decreased(0.000503--->0.000499) \t Saving The Model\n","Validation Loss Decreased(0.000499--->0.000495) \t Saving The Model\n","Validation Loss Decreased(0.000495--->0.000490) \t Saving The Model\n","Validation Loss Decreased(0.000490--->0.000486) \t Saving The Model\n","Validation Loss Decreased(0.000486--->0.000482) \t Saving The Model\n","Validation Loss Decreased(0.000482--->0.000478) \t Saving The Model\n","Validation Loss Decreased(0.000478--->0.000473) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0011345480634190608 \t Validation Loss: 0.0004694123927038163 \n","\n","Validation Loss Decreased(0.000473--->0.000469) \t Saving The Model\n","Validation Loss Decreased(0.000469--->0.000465) \t Saving The Model\n","Validation Loss Decreased(0.000465--->0.000461) \t Saving The Model\n","Validation Loss Decreased(0.000461--->0.000458) \t Saving The Model\n","Validation Loss Decreased(0.000458--->0.000454) \t Saving The Model\n","Validation Loss Decreased(0.000454--->0.000450) \t Saving The Model\n","Validation Loss Decreased(0.000450--->0.000446) \t Saving The Model\n","Validation Loss Decreased(0.000446--->0.000443) \t Saving The Model\n","Validation Loss Decreased(0.000443--->0.000439) \t Saving The Model\n","Validation Loss Decreased(0.000439--->0.000436) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0010865400290640537 \t Validation Loss: 0.00043205151450820267 \n","\n","Validation Loss Decreased(0.000436--->0.000432) \t Saving The Model\n","Validation Loss Decreased(0.000432--->0.000429) \t Saving The Model\n","Validation Loss Decreased(0.000429--->0.000425) \t Saving The Model\n","Validation Loss Decreased(0.000425--->0.000422) \t Saving The Model\n","Validation Loss Decreased(0.000422--->0.000419) \t Saving The Model\n","Validation Loss Decreased(0.000419--->0.000415) \t Saving The Model\n","Validation Loss Decreased(0.000415--->0.000412) \t Saving The Model\n","Validation Loss Decreased(0.000412--->0.000409) \t Saving The Model\n","Validation Loss Decreased(0.000409--->0.000406) \t Saving The Model\n","Validation Loss Decreased(0.000406--->0.000403) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0010428447731101187 \t Validation Loss: 0.00039972574450075626 \n","\n","Validation Loss Decreased(0.000403--->0.000400) \t Saving The Model\n","Validation Loss Decreased(0.000400--->0.000397) \t Saving The Model\n","Validation Loss Decreased(0.000397--->0.000394) \t Saving The Model\n","Validation Loss Decreased(0.000394--->0.000391) \t Saving The Model\n","Validation Loss Decreased(0.000391--->0.000388) \t Saving The Model\n","Validation Loss Decreased(0.000388--->0.000385) \t Saving The Model\n","Validation Loss Decreased(0.000385--->0.000382) \t Saving The Model\n","Validation Loss Decreased(0.000382--->0.000380) \t Saving The Model\n","Validation Loss Decreased(0.000380--->0.000377) \t Saving The Model\n","Validation Loss Decreased(0.000377--->0.000374) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0010025962601503124 \t Validation Loss: 0.00037168554263189435 \n","\n","Validation Loss Decreased(0.000374--->0.000372) \t Saving The Model\n","Validation Loss Decreased(0.000372--->0.000369) \t Saving The Model\n","Validation Loss Decreased(0.000369--->0.000367) \t Saving The Model\n","Validation Loss Decreased(0.000367--->0.000364) \t Saving The Model\n","Validation Loss Decreased(0.000364--->0.000362) \t Saving The Model\n","Validation Loss Decreased(0.000362--->0.000359) \t Saving The Model\n","Validation Loss Decreased(0.000359--->0.000357) \t Saving The Model\n","Validation Loss Decreased(0.000357--->0.000354) \t Saving The Model\n","Validation Loss Decreased(0.000354--->0.000352) \t Saving The Model\n","Validation Loss Decreased(0.000352--->0.000350) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0009651339369156631 \t Validation Loss: 0.00034722022246569395 \n","\n","Validation Loss Decreased(0.000350--->0.000347) \t Saving The Model\n","Validation Loss Decreased(0.000347--->0.000345) \t Saving The Model\n","Validation Loss Decreased(0.000345--->0.000343) \t Saving The Model\n","Validation Loss Decreased(0.000343--->0.000340) \t Saving The Model\n","Validation Loss Decreased(0.000340--->0.000338) \t Saving The Model\n","Validation Loss Decreased(0.000338--->0.000336) \t Saving The Model\n","Validation Loss Decreased(0.000336--->0.000334) \t Saving The Model\n","Validation Loss Decreased(0.000334--->0.000332) \t Saving The Model\n","Validation Loss Decreased(0.000332--->0.000330) \t Saving The Model\n","Validation Loss Decreased(0.000330--->0.000328) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.0009299534922320163 \t Validation Loss: 0.00032571973861195147 \n","\n","Validation Loss Decreased(0.000328--->0.000326) \t Saving The Model\n","Validation Loss Decreased(0.000326--->0.000324) \t Saving The Model\n","Validation Loss Decreased(0.000324--->0.000322) \t Saving The Model\n","Validation Loss Decreased(0.000322--->0.000320) \t Saving The Model\n","Validation Loss Decreased(0.000320--->0.000318) \t Saving The Model\n","Validation Loss Decreased(0.000318--->0.000316) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-17 03:02:02,097]\u001b[0m Trial 24 finished with value: 12.921025228029492 and parameters: {'batch_size': 256, 'learning_rate': 0.0006705430330850013, 'hidden_size': 16, 'num_layers': 1}. Best is trial 11 with value: 7.023413212837748.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.000316--->0.000314) \t Saving The Model\n","Validation Loss Decreased(0.000314--->0.000312) \t Saving The Model\n","Validation Loss Decreased(0.000312--->0.000310) \t Saving The Model\n","Validation Loss Decreased(0.000310--->0.000308) \t Saving The Model\n"," \n","SMAPE : 12.921025228029492\n"]}]},{"cell_type":"code","source":["joblib.dump(study, '/content/drive/MyDrive/Colab Notebooks/실무인증/Data/lstm_optuna_key_220617_01.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiIa9yM6A1k4","executionInfo":{"status":"ok","timestamp":1655434939470,"user_tz":-540,"elapsed":404,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"bcf27491-f6e2-41f2-d297-dc864fca6f59"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/실무인증/Data/lstm_optuna_key_220617_01.pkl']"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["study = joblib.load(\"/content/drive/MyDrive/Colab Notebooks/실무인증/Data/lstm_optuna_key_220615_01.pkl\")\n","print(\"Best trial until now:\")\n","print(\" Value: \", study.best_trial.value)\n","print(\" Params: \")\n","for key, value in study.best_trial.params.items():\n","    print(f\"    {key}: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"268hIGwJVkq9","executionInfo":{"status":"ok","timestamp":1655363662253,"user_tz":-540,"elapsed":2,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"744e9b4e-8358-444c-92f9-a52128804eab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best trial until now:\n"," Value:  5.477795508716941\n"," Params: \n","    max_epochs: 50\n","    learning_rate: 0.005659123649008913\n","    hidden_size: 256\n"]}]},{"cell_type":"code","source":["optuna.visualization.matplotlib.plot_param_importances(study)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"b-6nLQBPVoZ9","executionInfo":{"status":"ok","timestamp":1655434945933,"user_tz":-540,"elapsed":1262,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"b715ac1a-0039-4750-925f-4a0b2237930f"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(\"Number of finished trials: \", len(study.trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))\n","\n","\n","# optuna.visualization.matplotlib.plot_param_importances(study)\n","# plt.show()\n","\n","\n","optuna.visualization.matplotlib.plot_optimization_history(study)\n","plt.show()"],"metadata":{"id":"2SWG-vK0huDm","colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"status":"ok","timestamp":1655434952030,"user_tz":-540,"elapsed":616,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"da1642c4-c796-4724-ce08-1e8d31db75fe"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of finished trials:  25\n","Best trial:\n","  Value:  7.023413212837748\n","  Params: \n","    batch_size: 256\n","    learning_rate: 0.00243744167357133\n","    hidden_size: 16\n","    num_layers: 1\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## Plot Results"],"metadata":{"id":"4UVQqgCk_NVo"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","lookback_length = 90\n","forecast_length = max_prediction_length\n","log_interval = 10\n","num_classes = 1 \n","num_epochs = 150 \n","\n","\n","\n","x, y = sliding_windows(training_data, lookback_length, forecast_length)\n","\n","train_dl, val_dl, input_size = get_data_loader(x, y[:, 0, :],  study.best_params['batch_size'])\n","model = LSTM(num_classes=num_classes, input_size=input_size, hidden_size=study.best_params['hidden_size'], num_layers=study.best_params['num_layers'])\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","optimizer = optim.Adam(model.parameters(), lr=study.best_params['learning_rate'])\n","criterion = torch.nn.MSELoss()\n","best_loss = train(log_interval, model, train_dl, val_dl, optimizer, criterion,  num_epochs)\n","\n","# print('best loss for the trial = ', best_loss)\n","predict_data = []\n","actual_data = []\n","\n","# x_pred = x[-1:, :, :]  # last observed input sequence\n","# y_pred = y[-1]         # last observed target value\n","\n","# 여기서 x는 (sample, lookback_length, 1)의 크기를 지님. 따라서, 제일 앞의 시점을 제거하려면, x[:, -1, :]이 되어야 함\n","x_pred = x  # Inference에 사용할 lookback data를 x_pred로 지정. 앞으로 x_pred를 하나씩 옮겨 가면서 inference를 할 예정\n","\n","# print('-----------------------y shape before loop = ', y.shape)\n","for j, i in enumerate(range(max_prediction_length - 1)):\n","\n","    # feed the last forecast back to the model as an input\n","    # print(f'j = {j}')\n","    # print(f'y shape = {y.shape}')\n","    # print(f'Before update data = {x_pred.shape} & y_pred = {y_pred.shape}, expand_dim shape = {np.expand_dims(y[:, j, :], 1).shape}')\n","    x_pred = np.append(x_pred[:, 1:, :], np.expand_dims(y[:, j, :], 1), axis=1)\n","\n","    # print(f'After update data = {x_pred.shape}')\n","    xt_pred = torch.Tensor(x_pred)\n","\n","    if torch.cuda.is_available():\n","        xt_pred = xt_pred.cuda()\n","\n","    # generate the next forecast\n","    yt_pred = model(xt_pred)\n","\n","    # print(f'model result yt_pred = {yt_pred.shape}')\n","    # tensor to array\n","    # x_pred = xt_pred.cpu().detach().numpy()\n","    y_pred = yt_pred.cpu().detach().numpy()\n","\n","    # save the forecast\n","    predict_data.append(y_pred)\n","    print(f'predict_data : {predict_data}')\n","    # save actual data\n","    actual_data.append(y[:, j, :])\n","\n","# print(f'After loop predict_data = {len(predict_data)} & predict_data = {len(predict_data)}')\n","# transform the forecasts back to the original scale\n","predict_data_np = np.array(predict_data).reshape(-1, 1)\n","# predict_data = scaler.inverse_transform(predict_data) # actual_data는 scale되어 있는 데이터임\n","actual_data_np = np.array(actual_data).reshape(-1, 1)\n","\n","# # print(f'predict_data = {predict_data[:3]}, actual data = {actual_data[:3]}')\n","# # print(f'actual size = {actual.shape}, predict = {predict_data.shape}')\n","# SMAPE = smape(actual_data, predict_data)\n","# print(f' \\nSMAPE : {SMAPE}')\n","\n","\n","# pred_df = pd.concat([data_p[scale_cols], pd.DataFrame(predict_data, columns=['trade_price'])], axis=0).reset_index(drop=True)\n","# pred_df = pred_df.rename(columns = {'trade_price':'pred_price'})\n","# data_df = pd.concat([data, pred_df], axis=1)\n","\n","# plt.plot(data_df['pred_price'][len(data_df)-lookback_length*3:], label='prediction')\n","# plt.plot(data_df['trade_price'][len(data_df)-lookback_length*3:], label='actual')\n","# plt.suptitle('Timeseries Prediction')\n","# plt.suptitle('Timeseries Prediction')\n","# plt.axvline(x = len(data_df) - len(predict_data), c = 'g', linestyle = '--')\n","# plt.legend()\n","# plt.show();\n","\n","\n","np.expand_dims(y[:, 9, :], 1).shape\n"],"metadata":{"id":"rNdPlJCASiX0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655436793493,"user_tz":-540,"elapsed":6082,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"e98f77cf-94fc-4ceb-b714-e4729221960a"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0 \t Training Loss: 0.02094847447006032 \t Validation Loss: 0.003521205624565482 \n","\n","Validation Loss Decreased(inf--->0.003521) \t Saving The Model\n","Validation Loss Decreased(0.003521--->0.003125) \t Saving The Model\n","Validation Loss Decreased(0.003125--->0.003057) \t Saving The Model\n","Validation Loss Decreased(0.003057--->0.003029) \t Saving The Model\n","Validation Loss Decreased(0.003029--->0.002803) \t Saving The Model\n","Validation Loss Decreased(0.002803--->0.002533) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.011989462655037642 \t Validation Loss: 0.0022501382045447826 \n","\n","Validation Loss Decreased(0.002533--->0.002250) \t Saving The Model\n","Validation Loss Decreased(0.002250--->0.001975) \t Saving The Model\n","Validation Loss Decreased(0.001975--->0.001711) \t Saving The Model\n","Validation Loss Decreased(0.001711--->0.001450) \t Saving The Model\n","Validation Loss Decreased(0.001450--->0.001173) \t Saving The Model\n","Validation Loss Decreased(0.001173--->0.000853) \t Saving The Model\n","Validation Loss Decreased(0.000853--->0.000515) \t Saving The Model\n","Validation Loss Decreased(0.000515--->0.000411) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.00114331913573551 \t Validation Loss: 0.0004816227010451257 \n","\n","Validation Loss Decreased(0.000411--->0.000409) \t Saving The Model\n","Validation Loss Decreased(0.000409--->0.000365) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.0009923033921950264 \t Validation Loss: 0.000363321480108425 \n","\n","Validation Loss Decreased(0.000365--->0.000363) \t Saving The Model\n","Validation Loss Decreased(0.000363--->0.000351) \t Saving The Model\n","Validation Loss Decreased(0.000351--->0.000345) \t Saving The Model\n","Validation Loss Decreased(0.000345--->0.000335) \t Saving The Model\n","Validation Loss Decreased(0.000335--->0.000333) \t Saving The Model\n","Validation Loss Decreased(0.000333--->0.000329) \t Saving The Model\n","Validation Loss Decreased(0.000329--->0.000323) \t Saving The Model\n","Validation Loss Decreased(0.000323--->0.000317) \t Saving The Model\n","Validation Loss Decreased(0.000317--->0.000310) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0008990476198960096 \t Validation Loss: 0.00030462717404589057 \n","\n","Validation Loss Decreased(0.000310--->0.000305) \t Saving The Model\n","Validation Loss Decreased(0.000305--->0.000301) \t Saving The Model\n","Validation Loss Decreased(0.000301--->0.000298) \t Saving The Model\n","Validation Loss Decreased(0.000298--->0.000294) \t Saving The Model\n","Validation Loss Decreased(0.000294--->0.000290) \t Saving The Model\n","Validation Loss Decreased(0.000290--->0.000285) \t Saving The Model\n","Validation Loss Decreased(0.000285--->0.000281) \t Saving The Model\n","Validation Loss Decreased(0.000281--->0.000277) \t Saving The Model\n","Validation Loss Decreased(0.000277--->0.000273) \t Saving The Model\n","Validation Loss Decreased(0.000273--->0.000269) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.0008239159924414707 \t Validation Loss: 0.0002658771991264075 \n","\n","Validation Loss Decreased(0.000269--->0.000266) \t Saving The Model\n","Validation Loss Decreased(0.000266--->0.000262) \t Saving The Model\n","Validation Loss Decreased(0.000262--->0.000259) \t Saving The Model\n","Validation Loss Decreased(0.000259--->0.000255) \t Saving The Model\n","Validation Loss Decreased(0.000255--->0.000252) \t Saving The Model\n","Validation Loss Decreased(0.000252--->0.000249) \t Saving The Model\n","Validation Loss Decreased(0.000249--->0.000246) \t Saving The Model\n","Validation Loss Decreased(0.000246--->0.000243) \t Saving The Model\n","Validation Loss Decreased(0.000243--->0.000240) \t Saving The Model\n","Validation Loss Decreased(0.000240--->0.000237) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0007568236633233028 \t Validation Loss: 0.0002338682534173131 \n","\n","Validation Loss Decreased(0.000237--->0.000234) \t Saving The Model\n","Validation Loss Decreased(0.000234--->0.000231) \t Saving The Model\n","Validation Loss Decreased(0.000231--->0.000228) \t Saving The Model\n","Validation Loss Decreased(0.000228--->0.000226) \t Saving The Model\n","Validation Loss Decreased(0.000226--->0.000223) \t Saving The Model\n","Validation Loss Decreased(0.000223--->0.000220) \t Saving The Model\n","Validation Loss Decreased(0.000220--->0.000218) \t Saving The Model\n","Validation Loss Decreased(0.000218--->0.000215) \t Saving The Model\n","Validation Loss Decreased(0.000215--->0.000213) \t Saving The Model\n","Validation Loss Decreased(0.000213--->0.000210) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0006951984960323898 \t Validation Loss: 0.00020811073773074895 \n","\n","Validation Loss Decreased(0.000210--->0.000208) \t Saving The Model\n","Validation Loss Decreased(0.000208--->0.000206) \t Saving The Model\n","Validation Loss Decreased(0.000206--->0.000203) \t Saving The Model\n","Validation Loss Decreased(0.000203--->0.000201) \t Saving The Model\n","Validation Loss Decreased(0.000201--->0.000199) \t Saving The Model\n","Validation Loss Decreased(0.000199--->0.000197) \t Saving The Model\n","Validation Loss Decreased(0.000197--->0.000195) \t Saving The Model\n","Validation Loss Decreased(0.000195--->0.000193) \t Saving The Model\n","Validation Loss Decreased(0.000193--->0.000190) \t Saving The Model\n","Validation Loss Decreased(0.000190--->0.000188) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0006372373845806578 \t Validation Loss: 0.00018631559214554727 \n","\n","Validation Loss Decreased(0.000188--->0.000186) \t Saving The Model\n","Validation Loss Decreased(0.000186--->0.000184) \t Saving The Model\n","Validation Loss Decreased(0.000184--->0.000182) \t Saving The Model\n","Validation Loss Decreased(0.000182--->0.000180) \t Saving The Model\n","Validation Loss Decreased(0.000180--->0.000178) \t Saving The Model\n","Validation Loss Decreased(0.000178--->0.000177) \t Saving The Model\n","Validation Loss Decreased(0.000177--->0.000175) \t Saving The Model\n","Validation Loss Decreased(0.000175--->0.000173) \t Saving The Model\n","Validation Loss Decreased(0.000173--->0.000171) \t Saving The Model\n","Validation Loss Decreased(0.000171--->0.000169) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0005824208728881786 \t Validation Loss: 0.00016746109758969396 \n","\n","Validation Loss Decreased(0.000169--->0.000167) \t Saving The Model\n","Validation Loss Decreased(0.000167--->0.000166) \t Saving The Model\n","Validation Loss Decreased(0.000166--->0.000164) \t Saving The Model\n","Validation Loss Decreased(0.000164--->0.000162) \t Saving The Model\n","Validation Loss Decreased(0.000162--->0.000161) \t Saving The Model\n","Validation Loss Decreased(0.000161--->0.000159) \t Saving The Model\n","Validation Loss Decreased(0.000159--->0.000158) \t Saving The Model\n","Validation Loss Decreased(0.000158--->0.000156) \t Saving The Model\n","Validation Loss Decreased(0.000156--->0.000155) \t Saving The Model\n","Validation Loss Decreased(0.000155--->0.000153) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0005328615407051984 \t Validation Loss: 0.00015209363482426852 \n","\n","Validation Loss Decreased(0.000153--->0.000152) \t Saving The Model\n","Validation Loss Decreased(0.000152--->0.000151) \t Saving The Model\n","Validation Loss Decreased(0.000151--->0.000150) \t Saving The Model\n","Validation Loss Decreased(0.000150--->0.000148) \t Saving The Model\n","Validation Loss Decreased(0.000148--->0.000147) \t Saving The Model\n","Validation Loss Decreased(0.000147--->0.000146) \t Saving The Model\n","Validation Loss Decreased(0.000146--->0.000145) \t Saving The Model\n","Validation Loss Decreased(0.000145--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000144) \t Saving The Model\n","Validation Loss Decreased(0.000144--->0.000143) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0004927862428303342 \t Validation Loss: 0.0001420220942236483 \n","\n","Validation Loss Decreased(0.000143--->0.000142) \t Saving The Model\n","Validation Loss Decreased(0.000142--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000141) \t Saving The Model\n","Validation Loss Decreased(0.000141--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000140) \t Saving The Model\n","Validation Loss Decreased(0.000140--->0.000139) \t Saving The Model\n","Validation Loss Decreased(0.000139--->0.000139) \t Saving The Model\n","Validation Loss Decreased(0.000139--->0.000138) \t Saving The Model\n","Validation Loss Decreased(0.000138--->0.000138) \t Saving The Model\n","Validation Loss Decreased(0.000138--->0.000138) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.00046218930947361514 \t Validation Loss: 0.00013730491627939045 \n","\n","Validation Loss Decreased(0.000138--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000137) \t Saving The Model\n","Validation Loss Decreased(0.000137--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.00043660131632350385 \t Validation Loss: 0.00013578646758105606 \n","\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","Validation Loss Decreased(0.000136--->0.000136) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.0004140477249165997 \t Validation Loss: 0.00013600854435935616 \n","\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32), array([[0.70255566],\n","       [0.76759464],\n","       [0.8287168 ],\n","       ...,\n","       [0.29656252],\n","       [0.29286078],\n","       [0.29090726]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32), array([[0.70255566],\n","       [0.76759464],\n","       [0.8287168 ],\n","       ...,\n","       [0.29656252],\n","       [0.29286078],\n","       [0.29090726]], dtype=float32), array([[0.76759464],\n","       [0.8287168 ],\n","       [0.9037006 ],\n","       ...,\n","       [0.29286078],\n","       [0.29090726],\n","       [0.29273996]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32), array([[0.70255566],\n","       [0.76759464],\n","       [0.8287168 ],\n","       ...,\n","       [0.29656252],\n","       [0.29286078],\n","       [0.29090726]], dtype=float32), array([[0.76759464],\n","       [0.8287168 ],\n","       [0.9037006 ],\n","       ...,\n","       [0.29286078],\n","       [0.29090726],\n","       [0.29273996]], dtype=float32), array([[0.8287168 ],\n","       [0.9037006 ],\n","       [0.89204234],\n","       ...,\n","       [0.29090726],\n","       [0.29273996],\n","       [0.30436617]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32), array([[0.70255566],\n","       [0.76759464],\n","       [0.8287168 ],\n","       ...,\n","       [0.29656252],\n","       [0.29286078],\n","       [0.29090726]], dtype=float32), array([[0.76759464],\n","       [0.8287168 ],\n","       [0.9037006 ],\n","       ...,\n","       [0.29286078],\n","       [0.29090726],\n","       [0.29273996]], dtype=float32), array([[0.8287168 ],\n","       [0.9037006 ],\n","       [0.89204234],\n","       ...,\n","       [0.29090726],\n","       [0.29273996],\n","       [0.30436617]], dtype=float32), array([[0.9037006 ],\n","       [0.89204234],\n","       [0.8440214 ],\n","       ...,\n","       [0.29273996],\n","       [0.30436617],\n","       [0.32328013]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32), array([[0.70255566],\n","       [0.76759464],\n","       [0.8287168 ],\n","       ...,\n","       [0.29656252],\n","       [0.29286078],\n","       [0.29090726]], dtype=float32), array([[0.76759464],\n","       [0.8287168 ],\n","       [0.9037006 ],\n","       ...,\n","       [0.29286078],\n","       [0.29090726],\n","       [0.29273996]], dtype=float32), array([[0.8287168 ],\n","       [0.9037006 ],\n","       [0.89204234],\n","       ...,\n","       [0.29090726],\n","       [0.29273996],\n","       [0.30436617]], dtype=float32), array([[0.9037006 ],\n","       [0.89204234],\n","       [0.8440214 ],\n","       ...,\n","       [0.29273996],\n","       [0.30436617],\n","       [0.32328013]], dtype=float32), array([[0.89204234],\n","       [0.8440214 ],\n","       [0.8347767 ],\n","       ...,\n","       [0.30436617],\n","       [0.32328013],\n","       [0.33881465]], dtype=float32)]\n","predict_data : [array([[0.45641658],\n","       [0.45631212],\n","       [0.45854193],\n","       ...,\n","       [0.2570045 ],\n","       [0.2512182 ],\n","       [0.25107014]], dtype=float32), array([[0.45631212],\n","       [0.45854193],\n","       [0.4622982 ],\n","       ...,\n","       [0.2512182 ],\n","       [0.25107014],\n","       [0.25490427]], dtype=float32), array([[0.45854193],\n","       [0.4622982 ],\n","       [0.45245516],\n","       ...,\n","       [0.25107014],\n","       [0.25490427],\n","       [0.2583068 ]], dtype=float32), array([[0.4622982 ],\n","       [0.45245516],\n","       [0.4453719 ],\n","       ...,\n","       [0.25490427],\n","       [0.2583068 ],\n","       [0.26166654]], dtype=float32), array([[0.45245516],\n","       [0.4453719 ],\n","       [0.4444368 ],\n","       ...,\n","       [0.2583068 ],\n","       [0.26166654],\n","       [0.27096108]], dtype=float32), array([[0.4453719 ],\n","       [0.4444368 ],\n","       [0.44754055],\n","       ...,\n","       [0.26166654],\n","       [0.27096108],\n","       [0.2811151 ]], dtype=float32), array([[0.4444368 ],\n","       [0.44754055],\n","       [0.4562377 ],\n","       ...,\n","       [0.27096108],\n","       [0.2811151 ],\n","       [0.29130867]], dtype=float32), array([[0.44754055],\n","       [0.4562377 ],\n","       [0.48407462],\n","       ...,\n","       [0.2811151 ],\n","       [0.29130867],\n","       [0.3006214 ]], dtype=float32), array([[0.4562377 ],\n","       [0.48407462],\n","       [0.5281116 ],\n","       ...,\n","       [0.29130867],\n","       [0.3006214 ],\n","       [0.30413836]], dtype=float32), array([[0.48407462],\n","       [0.5281116 ],\n","       [0.5880315 ],\n","       ...,\n","       [0.3006214 ],\n","       [0.30413836],\n","       [0.3012889 ]], dtype=float32), array([[0.5281116 ],\n","       [0.5880315 ],\n","       [0.65351665],\n","       ...,\n","       [0.30413836],\n","       [0.3012889 ],\n","       [0.30303407]], dtype=float32), array([[0.5880315 ],\n","       [0.65351665],\n","       [0.70255566],\n","       ...,\n","       [0.3012889 ],\n","       [0.30303407],\n","       [0.29656252]], dtype=float32), array([[0.65351665],\n","       [0.70255566],\n","       [0.76759464],\n","       ...,\n","       [0.30303407],\n","       [0.29656252],\n","       [0.29286078]], dtype=float32), array([[0.70255566],\n","       [0.76759464],\n","       [0.8287168 ],\n","       ...,\n","       [0.29656252],\n","       [0.29286078],\n","       [0.29090726]], dtype=float32), array([[0.76759464],\n","       [0.8287168 ],\n","       [0.9037006 ],\n","       ...,\n","       [0.29286078],\n","       [0.29090726],\n","       [0.29273996]], dtype=float32), array([[0.8287168 ],\n","       [0.9037006 ],\n","       [0.89204234],\n","       ...,\n","       [0.29090726],\n","       [0.29273996],\n","       [0.30436617]], dtype=float32), array([[0.9037006 ],\n","       [0.89204234],\n","       [0.8440214 ],\n","       ...,\n","       [0.29273996],\n","       [0.30436617],\n","       [0.32328013]], dtype=float32), array([[0.89204234],\n","       [0.8440214 ],\n","       [0.8347767 ],\n","       ...,\n","       [0.30436617],\n","       [0.32328013],\n","       [0.33881465]], dtype=float32), array([[0.8440214 ],\n","       [0.8347767 ],\n","       [0.8520896 ],\n","       ...,\n","       [0.32328013],\n","       [0.33881465],\n","       [0.35272655]], dtype=float32)]\n"]},{"output_type":"execute_result","data":{"text/plain":["(1085, 1, 1)"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["np.expand_dims(y[:, 9, :], 1).shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KU1bOLTuZLW","executionInfo":{"status":"ok","timestamp":1655436843511,"user_tz":-540,"elapsed":508,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"3c9e5375-b011-4850-da0b-deddd6917752"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1085, 1, 1)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["np.array(predict_data).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TBOTjgwFp7f_","executionInfo":{"status":"ok","timestamp":1655436451013,"user_tz":-540,"elapsed":603,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"1022db3b-fce6-47f2-b24b-a24a98295dde"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(19, 1085, 1)"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["pred_df = pd.concat([data_p[scale_cols], pd.DataFrame(predict_data, columns=['trade_price'])], axis=0).reset_index(drop=True)"],"metadata":{"id":"QTdpk9Ftpp7O","executionInfo":{"status":"ok","timestamp":1655435601696,"user_tz":-540,"elapsed":447,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["pred_df = pd.concat([data_p[scale_cols], pd.DataFrame(predict_data, columns=['trade_price'])], axis=0).reset_index(drop=True)\n","pred_df = pred_df.rename(columns = {'trade_price':'pred_price'})\n","data_df = pd.concat([data, pred_df], axis=1)\n","\n","plt.plot(data_df['pred_price'][len(data_df)-lookback_length*3:], label='prediction')\n","plt.plot(data_df['trade_price'][len(data_df)-lookback_length*3:], label='actual')\n","plt.suptitle('Timeseries Prediction')\n","plt.axvline(x = len(data_df) - len(predict_data), c = 'g', linestyle = '--')\n","plt.legend()\n","plt.show();"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"BPIzb9vppSSG","executionInfo":{"status":"ok","timestamp":1655435510237,"user_tz":-540,"elapsed":534,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"4a0d8fe8-0e32-4bec-8100-44549f1f5e12"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["plt.plot(data_df['pred_price'][len(data_df)-lookback_length*3:], label='prediction')\n","plt.plot(data_df['trade_price'][len(data_df)-lookback_length*3:], label='actual')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"DGGGq5PDpX7E","executionInfo":{"status":"ok","timestamp":1655435543803,"user_tz":-540,"elapsed":451,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"f0d1abb9-a15a-4357-bbb2-3d174cb0a800"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f78902cb250>]"]},"metadata":{},"execution_count":26},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["plt.plot(predict_data, label='predict')\n","plt.plot(actual, label='actual')\n","plt.suptitle('Timeseries Prediction')\n","plt.legend()\n","plt.show();"],"metadata":{"id":"tfOoGb4JQOoi","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"error","timestamp":1655435274845,"user_tz":-540,"elapsed":689,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"564dc310-6121-4b0b-9f87-0c7cf0bf2d64"},"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-feb292548c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Timeseries Prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'actual' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZwUxd3/Pz0zuwvLXuwuN3gt4I2wLh7EC1mNJzFGJRrjQaJRlHjGqNGEREmIkejzKD45Hh4TNb8n5BCTeOZZEVHQiFFQ8eBUroVld/a+d7p/f3T3THf1MV1dPVvTs/V5vXixO9vv+VZVV3+7jm9VSYqiKBASEhISCr0ivBMgJCQkJBSMhEMXEhISyhEJhy4kJCSUIxIOXUhISChHJBy6kJCQUI5IOHQhISGhHFGMp/G9e/f64iorK9HY2BhwanJPopy8SZSTN4ly8qZMl9P48eMd/yZa6EJCQkI5IuHQhYSEhHJEwqELCQkJ5YiEQxcSEhLKEQmHLiQkJJQjEg5dSEhIKEckHLqQkJBQjih0Dl2RZXS/+jyUgQHfvLy2Lgv4fn+8okBe+yp/vp833+efXxcA39fLwK/iz/cy8G+9FgDf44sHAPltNj6XFV20aNEiXsbb29upGeWtVej9zcNAXh6kqUfT82+vhvLkfwCxGKSpx9Dz/3odypOPAtEopMN98O+sgfI/jwKRCKTDj6Xn310LZfkvAQDSEdNcry0sLERXV5f5w/fWQfnvpYCiQDryOGr7eO8tlZdlf/zGf0H57cPAQD+ko2bQ8x+sh/KbXwD9fZCO9sO/C+U3DwF9vZCOrgbgUE5O2vQelF/9HOjtgXTM8fT2P94A5VdLgO4uSMfW0POfbIDyX0uArk5I03zwn30I5YmfAh1tkI47gQotLCxE14b1UJYtBtpbqXkAwJZNKt/WAmn6idS4svUTKI89ALTEIc04id7+IIiqPvlQcXGx499C10JHZ4f6f3urP74rKL7FJ9+p/t/ml+9g4hVG+0q3zjf7tK9V9Fa/fCcb383Ks6Ufmn3FL9/TrfIt8XDyWsua3X6TPz7HFT6HLkmsX6D+p8hs9v0e9KQn3+85Ucz51+0z5l/2mYEII898+4MqP7/pZ64AbHxg9Zcx/az1z6eUrk4kbrgYyqb3mb4nWxVehy77rRDa/74P3mN0CJJe5LweSJ33h7N71IDEenAio0NSmBPg1zzj/edef9nwpPzmf/fnQGIA8vMrGBOQnQqfQ2eukLxbaIw8s/2gHPLQPIqWvfgCeiH7VSSgFrrfHhZzD8OnWYtys/6G0KHrYn7F+8N4t/CT4vVCYDUfVBMtpApqyMhvD5V5yINzD5NVgTVoslPhc+hBORTmLitjhfY9ZBTUkAmvLntAcyBD9YXAXP7a/8zVn1cLm/MLIcsVQoce1KQQo/2wtjCCyj/vMWRevC5OHUR2h8rYwmYeMoow8pyHTLNc4XPogbUQeUdZ8B5D5zWGqpvn/EBxcygBNUiYhww5D5lwm0NiM5vtCqFD18StQgU0qcVrUkkXryGnnBEvh8g7yiWoKKuhXn8yo/A5dM5DwMF1+XnFsbPyWdLECatDCKr+8RoyDGpSlFuDgL3+Kn29ULK0/oXQoXMeA9TF3MLya5i3R9dxThW6WT2rUVn/hj9eW6GovPumLzy5wvG9t3zy2grRje/44lMO2W+Ui54QzmGLvFv4PnmlqwPyTZdC+ccf2exnSCF06Nr/vCdVuIVNZskD4Ves5d9Qz8Y37mPk97Px8QNsfGAtfF5DNtr/vHqIrHx7GwBA+ddqtu/JkMLn0IOS3wod0WfpfdoNauk7tx4r5x7OuElsdsdMZOSdT1z3pMC2ruA1qco5jj2pkPYwM6wQOnTeLVTOLZywT2oxOjRp1Bj1Bz87/QGQKkerP/jZ6RCAVKHZ97PTYxDiPYcRWJAP5yFTVvG276DwOXTeDikp3jeU7wuN214mvF+IrBo1NpjvCa1DDGjrAV7ibT+NwufQk+K8lwTz8xBOhxzcFATvrQv43H+pfJT6Q5q97DMm3iutAxuy88nx7uFkWCF26LwU1MIQPuI/J5wlQ0b+E6D9z8l+YJOa4ay/gTWRs3TIhFXhc+jMcbhg5DlXKN5L13MgjlgV7xYm50lNVvFuoXPLP+8XmrtC7NCHaBw2q/2gDlhgzD/3MXje98+3eK+jYFS2zIGF9v67K+blog0bNuDJJ5+ELMuYM2cOLrroItPfGxsbsWzZMnR2dkKWZVxxxRWorq7OSIK5i3sDlXOUTgANRIWBF+Is3kMuzOZze8gmrUOXZRnLly/Hfffdh4qKCtxzzz2oqanBxImpeN6//vWvOPnkk3H22Wdj9+7d+NnPfpZBhx5UHC2vsENWMUaZ8E4+qzhPanJf2JMtIwahfX5yW2mHXLZu3YqxY8dizJgxiMVimDVrFtavX2+6RpKk5CnXXV1dGDlyZGZSG4R41yfeKzWZxbuFxv0GMuI5kn7e+6HzFvf7aK+0LfR4PI6Kiork7xUVFdiyZYvpmksvvRQPPvggXn75ZfT29uL++++3/a66ujrU1dUBAJYsWYLKykrqBHcXF6ENQEHBMJT64YuKNb7AH1+s8/m++J7iYrQCyC8oQJkfvkTlC/LT87FYzFLGPcUlGp/v077GFzDyPu33lpSgBUB+fj5G+uJLNT4vyduVkyNfqvGxmC/7fftK0QwgLy8P5X74/Rof88k3lql8NErNx2IxlJWVIQ4glhdDhQ/7/S0HVD4a9cWzpN9kP+Yv/QP9PWgCEI1EHOsMTX0KWp7G0NNp7dq1OOOMM3DhhRdi8+bNeOyxx7B06VJEIuYOQG1tLWpra5O/NzY2UtuSOzoAAL09PT75do3v5WO/XbXf1+vPvqLxvR74yspKyzU0vK39tjY2vl0rvz42+33MfF+StyuntHx/vz/7ra0AgP7+Pp98GyOv26dPf2VlJVpaVH6gf4DJ/sAAa/n55FtaVPuMfCKRcORp6pMfjR/vvP1E2iGX8vJyNDU1JX9vampCeXm56ZpVq1bh5JNPBgBMnToV/f39aNccR/YpS4YMwjrkwr2rKYY8wq0sef5yVGkdelVVFerr69HQ0ICBgQGsW7cONTXmfTAqKyvx0UcfAQB2796N/v5+lJSUZCbFoVduVyjPCnvYGe/0cws7DOh7eN+/HA2zSjvkEo1GMX/+fCxevBiyLGP27NmYNGkSVqxYgaqqKtTU1OCqq67Cr3/9a7zwwgsAgAULFkDK2JtwiDtE3tuHsmqI376siVJhFbconSyJUuP+QrKXpzH06upqSxjivHnzkj9PnDgRDzzwQLApy7j43hD2hTVDtIUYlMKefl4K7ExcVvuMytH7H76VokmFcy8KdvOsLXS+5pPitnWDTy7Jc956gVXcW7icV2pnSw8nQwqfQ+c+ZBDyB5q7eL9ReD/RnNPPu4WbLXup5OjzFz6HHpS4b67EqNB3GUOeft776XPffpjNPPf92H3jvBsE7gqvQ+e9uRAv8d7POrAj0FjF+4XM+cQn7kMmnM3zeqFmeUMqfA49u1+QFMruipFx8Xoh8O7y866/gdkP6V4uvBtkGVb4HHq2iHfYVFiVLXHMWd7SyrjE9rX+lOUvBOHQQyfRwhFiEHMPh7d9Uf/cFF6HHtIXfFJhbaHoCnv6WTXU888q3pPKrMrS+x9Ch877DR/QfuTM9UFMqnER70lp3ke4MSug+hP2KLUMKYQOna/4LwzK7grlWbxeKNwnRUOeft4a4tlPpxA79CE6KaMrrOnPlRcSq3hPKvIKWw1K3Ks/9wTYKnwOPag4XlZxGzJhtRvyIQ9dvB1iWIcsmLdOCMp+aPd+YOQzq/A5dN4KqsvMu4XDW2GNQ+fdw+D9QtcV1vunK0efP+HQhfyJdxw+7xZ26B0C73UUYlIzExq6Dj20D2SODJlwU8jjsFnFOw6dd/nrCu3z764h6NAZww6TEtt3CjGI9xwAd94vyDvsNrs1BB06q7KkQnGPw2WNw+cdJcF5czfe5c8szvWPe/3ha95J4XPovLusvMV9UkkMGfAVb/shV8irfzqFz6EzKmveB7xbGENdod3PO1vs8/qCLOnhZqmGnENnVtgnxXgrqDMpeYl3DydbehjchqwY7ed42GN4HTq3SZmAxHtSKuxhe7zth11+yy+oYuc1h5BKgE8uuxtk4XPo3MeQNbHO0vtW2FuIjAqshcz7CDfODZLQvhD1SVFGPkcVPofOXbldITIvzlEKQ33IKzCFdMglMPG2b6/wOnRxQ9nEq4UY9rC5rFHI089tL5dg8GxVCB0656XHvDcnCor3Ld4tXNY4blbzWbIfOe85GN7rILjV/8C/KFCF0KGzKjtvhGflypBBjraQsl+i4AHwfyFnSKF16OxL91kTwPuG8u5ych9zCbe415+hqiwJqsiQQuvQmcW7y87sUTlVLN4VOvRDRozKlvcA7xdSjrawWTV0HTo3hdyhBCZePYyAHmTecdzM4jWGzrn8uYctZ1ahc+hhX2gYnEK6dJp3C18X9xYmK897yI1R3Muf96RqZhQ6h84s5hcC76XDYR/y0MT7gfarsKY71xT2BkWGNPQcOm9lTX3ivDCEWWGPQw9p2GBQPLfzCLQKKMt87GdY4XPo2TLmwtshDPUWCu8xbO4OkVW8x8A5876V3c9P+Bw6s7IkbIn38+xbnCt0aMstSzTUyy+5MMlvCz27C3AIOnRdvFpYWfKG5xalkCUnzjCLV/llS7mFfMgnW4oxYMW8XLRhwwY8+eSTkGUZc+bMwUUXXWS5Zt26dfjzn/8MSZJw8MEH45Zbbgk8saqypIXNrNycZRcSGhyFfcgrM0rr0GVZxvLly3HfffehoqIC99xzD2pqajBx4sTkNfX19XjuuefwwAMPoKioCK2trRlNtFAWKOwPxFBPP/cWLif7yXyHvIftoLRDLlu3bsXYsWMxZswYxGIxzJo1C+vXrzdd8+qrr+LLX/4yioqKAAClpaWZSa1R3Cp0ltxQ3v6I2yG/WdIy4+1QeVcAXi+EwO5fMF+TbUrbQo/H46ioqEj+XlFRgS1btpiu2bt3LwDg/vvvhyzLuPTSSzF9+nTLd9XV1aGurg4AsGTJElRWVlInuKekBK0ACvILUOaLL9b4fF98b0kJWgDk5+dhpB++VOPzfPIlpZ7tx2IxSxn3lhQz2lfTn5eXh3Jf+S9l4vv2laIZQF4s5o9vUPlYLIYKjbcrJ0e+0crTqD/egDiAaDTqq/73tzay8e3NKh+JUPOxWAwjy8rQBCASpecBYKC7Q+V92AeAgd4ujZd88YlEHxoBRCT44yPQeGf7NPUpaHkaQ08nWZZRX1+PH/3oR4jH4/jRj36Ehx9+GCNGjDBdV1tbi9ra2uTvjY2N1LaU9jYAQG9frz++rV3j+3zyqv2+vn5/fKvO+7Xf6tl+ZWWl5Zqk/X629Pf75lv58i0qP2Dg7crJmW9R+YEBn+lX+YRfvkXnE2x8gp6vrKxEs8bLPnjVfrPKyz755mY2+/E4G9+qp1925Gnqkx+NHz/e8W9ph1zKy8vR1NSU/L2pqQnl5eWWa2pqahCLxTB69GiMGzcO9fX1DEn2oL27/HH6iMnenWz8ns/98UH19Xzb17RrBxu/m7N9Zn47H/v6kEG9z/qra/8eNr5xPxvPWo01x+hbbS1sfDvjPB+r/QwprUOvqqpCfX09GhoaMDAwgHXr1qGmpsZ0zQknnIBNmzYBANra2lBfX48xY8ZkJsW6WCv0gX1sfJzxDdzZ4Y/THyTfD4T2BR1tPnGN72z3aV9Tl9/8a/b7+3wa1njmlYKclC2TuR0i8CEblXbIJRqNYv78+Vi8eDFkWcbs2bMxadIkrFixAlVVVaipqcFxxx2HjRs34rbbbkMkEsGVV16J4uLiDCU5SyYl/Up/Hndu45oMbsoWhzRUFVT5JxLBfI9QoPI0hl5dXY3q6mrTZ/PmzUv+LEkSrr76alx99dXBpi4jCvkLQUhISMhBQ3ilKC/xPouS0Xyuxnt51hDP/1DvYWV5/sPn0Hmv9Mzu+xkC8Y7f5qywp18oqxU+hz7UJRwCm0TxCeWwhqBDD/kKO/YEcLYvJCSUKYXPoXN3iENcovj5KuzlL57fjCp8Dp27OE9Khv15EOkXEsqYhp5DH+oP5JBvIQ31/AvlskLo0MP+QIa9ic7bvhCbxP3LZYXQoXNW2Fu4vJMf9vITEspiDT2Hztuh8LYfdonyExJyVPgcuv48HzyZ7XsOOoyNn3AwGz9ukj9Od2h+eV1jJ7DZHz2OzX7FaDa+rCL9NXbS01/s8xAWnS8s8sfrKhjOxsfy2HgpfI9+dii7GxRD965ya+nxrhC87QelsOeD1yHTQX+PUDZpCDv0QQeJr+H8QDPPzYY8/bwTwPsIOjF0lZMKoUPPkigRvw9EYA2skD/Q3F8I4kzMkBkW8qAQOnRNQ76F4/eFwnu3xqC+SPBCQqTC69DDqqxxqJzEO/28GwJCQhlU+Bx68oFkHPLgPWTCXSHvofC2zypecxghLzYhd4XPoQcl3vuq+xX3FibnMeBsCfLgPYchHLOQjYauQx+yYYtCoVbYeyZCGVXoHHqyPu/c7vcb1P/2fMGWgPpdPu1r2rebkd/DxjfUs/GN+32CWvk1N7LZb2th4zvb2fiebp+glv+Bfjb7iszGC+WkQufQQy/eZ3ryHmpgFXf7fM0LcVaW33/h0KmV5XdUSEjIkxTGxoEiZ18vKYQOPeyTUbztcxbvFrZQYGJ2iLwdKuuwlXDoAai/jwlX+hnHLsOuoe5Qsyj/Q9IhGpPMap83n4XzGKFz6MrTy9j4P/6WNQWMePY4FH/inf7g7HNxiEaTvFuIrHyC1X6CL8+a/gSj/QwodA4dAwNsfG9PMOngJt4ONaXQtzB98YY087BvLDNfDo2RD7KFzeoQeb8QWPkMKHwOnXEfcumSq9nsMzsx4888HGKKCX+Xm/WB5NzlZm7h8uZ5lD/nF2qQfAYUPodeVMLGGw4G8OMQTQ6Zi0Pk3UI0/My9y83ZPmsLU+FsP/Tlz7v+iRY6s6TTvpz8OZxjmNnUwuD9QLO28FmHHEI+Bs19DJ53DymE9TfDCp1Dj5xwWuoXLm94UxOdg32DeI+B8m7h8k4/b4cmWuiMPGf7GVDoHLpJXByiwSP6qRCMk1qmUSIeDzTvSTmjeDzQrPlnLj+Dwlh+QdrnXX5iyCVghb3LxWVSLouGHIZ6lzvsPPOQTwjvX5BzWBlQuB0665AJjxZWkC003l1uLvaNPSQeY/AG8W5hhv7+h33ISbTQAxbvbQB48IrDzz4kc9gb3RQmxGqetYXJaJ93/WMV7731udtnNZ89a0J0hduhh77LybnLyPpC4V7+rC+0EHb5WXneYa+mtISw/ILkMyBPDn3Dhg245ZZbsHDhQjz33HOO17399tu47LLLsG3btsAS6CrmFhpjC5O1hcvdPmeHyvuBZi3/sJcf7x4qj+fHxHMuvwworUOXZRnLly/Hvffei0ceeQRr167F7t3Wwxm6u7vx0ksvYcqUKRlJqH3iOD+QvFsozA8U7/yH3KH6ckhZlH7m+s/boYa8/DKgtA5969atGDt2LMaMGYNYLIZZs2Zh/fr1lutWrFiBr3zlK8jLy7P5lgyJ+wPJ6pAGv0KZTPpKPyMfpEPj/UIfii3cbCo/7i+U7Guhx9JdEI/HUVFRkfy9oqICW7ZsMV2zfft2NDY2orq6Gn//+98dv6uurg51dXUAgCVLlqCystJXovXDz8pHliFK+R1dRUXQDx/zw3cXFaONhS828GV++KIkP7KsDDEXPhaLWcq4p6QYrR55OzHzxSUpvpSe7y0pgX743MiyUnq+1MBr9u3KyUl9paVoZrDft9/Al/rgG1N8WVkp8ij5/pYDiOt8KR0fi8VQNrLMNw8AA11taEryJfR8b6eB92F/oJeJT0CGfnhiWYl9+mnqU9BK69DTSZZlPPXUU1iwYEHaa2tra1FbW5v8vbGR7VzJeGMjJESpGLm9w8xLdEUgt6fOoow3NkKK0PVI5Pa25M/xpkZI0Xw6vi1lv7mpCVKswPHayspKSxmb+UZIecOo7Cuk/fzhdLwh/83xJkgFhZT2DXxTHNKwIjq+lbA/vMi2nJz51uTPzU1NkIYXU9o38PEmSIV0exMpLamzVFuamiCNKPXPx5sgFZV5ZisrK9HSbOTjkIrpnmGF5Eto+ebkzy3xOKTSQebj8eTPLfE4JJt6Q1Of/Gj8+PGOf0s75FJeXo6mpqbk701NTSgvL0/+3tPTg127duHHP/4xbrrpJmzZsgUPPfTQoEyMyvdcB4UhUkG+93ootLGshm6W/IPv0PNG+z+4AQrDdsDyfTdAoT5s2JD++xew8T9cQH9giHFO94c3QaE9sMTIL7oZSl8vHW+QvGghG//j70LpZeFvgcKwnbP8wK2M/G1Qerr88w8y8ovvgNLNwP/0DihdnQz8nVC6OtJf6MQvuYuJz4TSOvSqqirU19ejoaEBAwMDWLduHWpqapJ/LywsxPLly7Fs2TIsW7YMU6ZMwV133YWqqqqMJjypPrYTjJj3R6c9/Z0cdmN4IAAArBWqoz39Na58W/pr3NTWkv4aN7U2p7/GJOIGtMTtL/OqZsaWWPwAG9/YwMjvT3+NmxrqGfm9bPz+PWx8vTXAw11E/dm7k81+wEo73hCNRjF//nwsXrwYsixj9uzZmDRpElasWIGqqiqTc+ejwV6cQF7Pe3EDI897cQj3iaVBvgGW6hP2xTWMAO/6w/2M4WDlaQC5uroa1dXVps/mzZtne+2iRYuYE0Ul5grFaI+2QliuZ+RZ7Q/1FwLvF+pgr1bMuRcK7/Rnl0cP90pRIAtiwdnw0FVI1hdC4Pnl/UAOcZ57+fE2Lxx6sBr0xS2kQ6OdFGXtcgZdgYbYA2m5fpB7SOm+j5pnw/nzvMsvt4Zcwu/Qea/25L24gdZ+4EMujMqyLmv2S5SXkLPC79BpHTLpQAadJ37nPmREx1v8L3X6s2zIhPekIO/8826ihz39lLzS3w/lk41QmpvSX+xDQ8+hs/KWFu5g22fkWe2T4t3DCNkDHbh53g6Vdw+L++2n/ILOdsi/vB/KxncYDdsr/A59KJ6akkt86IbMhvoYerbln/cL2Wf9kSQ2uw4KpUMvvftnqV+476nNOOTAu4dAzTOmnxTrHARr+bMeY8fboYR+TD3s6aeUnt3M+PNwOnRTaQy2QyUV+hYq5yEjcS4oIx+2Fmqw5gc/bJeVz6xHD6VDlyKGZA92C5W1hWyZVB3kFmrgLXzeLWzeDon3FrycGzRDrYUdVI9KDLkYxdJCJ8S7hc8ax869hSh4KuValBTvsN3BntRkXulNdzmtwunQIwE69EGPUgn6hcA4qTfo6Q+4hU07qZ1tDjXsPPchQ8b7P+hbZ4gWuo0YHDrrA2l5QQ/xsEXeUULcHcpQd6hDPP9+h1yFQzfI2EIPewtbOFRKe4w8q31SImw2ZHzQc2B+eeHQUzK+3QY97C1gfrAnxQKbpdfEu/x5T2qGPf2ht884hs1rHYIIWzRIMiR70Fs4nN/w2TYpypz/kLVQyROWaPNPnvBEa588YYo6/az2iRO2ws7TBiWw2k9oPMNJZ24KqUNnGHIJerc87g51kCcVLfwgp5/zpKa8+kUmXvl8C/EBpf1XnmWyL/9zJROv1BGHwLPylA5VefUfbPZfe56RZ7z/q19S/yfzEZBC6dAliWPYIvMYLmMLNejNxXgPOQz6SlvZ/fc0ipx6NpN96WDiaMbBnsMhj1ykdajkkYu0Dq2PsE/LdxNniNLmv5st/eSRkwq1fe3ISTIfASmUDh2BOnTOceChc6gB86xzAIPtEKNRrvYjp5/DxEsjK5l4S/4p758kES6HNv0ET+1QI+bBa2qejE6h9R86z7zC117hd+hhm9RhnpQhed6TUpQtLGaHzNbCZubJ+0V9/xntR0iHSmdfqj6ZjZ9h5hXa+jv9RMI8G09dftNmEjyl/WOJM5Rpy+/Y49X/515OZ9ejwu/QEwPO19mKuAEJxhYWtX1CtJMjzPZ5p58ob0r7FgdAm36SJye5KHmFlpcZeYv9focLHb+Ayb40osj8QYLOvjR8hPkDWvvDC9n4YYz88OEET1n+BSovjT+IjvOokDr0VLLlFcvp2D5zlIL8x9/S8USUg/y/v6G032vm/9+v2Oz/gZIn8//Mf1HyRPqfXsbGP8XI//4xKlzpJfjf/ScdT9hXnnyUjf8fRn75I5S8+f4r/72Ujae0j34y/7Q8YZ+y/C32f/cflLzZgSuU9Q8DWvrFwiKDjIWxbzcVqvztD+YPGvbS8X/5nfmDxv10/J+IF1D8AB1PvkBa6E4+Uf5AOPDWZjr+qcfNH7S30vHkA9DZTseTDlifZPLKkw6AnCRMx5MOiAxjTMcv/6X5A9oeCumAaYe8fvsw1fUW/jcPMfHyrxn5X/2cjf+vJWz8Ez9l45dpfIYOBgm/QxcSEhIKm0QcekqWmXIhISGhMIl17spB4fSMooEuJCQUZokWukGRcCZbSEhIKJMKpWekDvUSEhISyiaRC7QCUigduil0aOoxbN81+Sg2vuoINv6ww9n4gyez8QdVpb/GTZMOZeMnHsLGs8bzjpvExo+ZwMaPGsuXrxjNxpMrT2lVVs7Gl5Sx8cWlbHxRsT/uiGlsdh0USoceqRiV+oV2cmHykUDlmNTvtEt3Jx9F8JQLk6YQPO1Y2tRjzDxt+NPhxxI8ZfotPB2OI6aZnRBt+o88zmyfVkdNZ+RnmNNPO59zTLXZPu0E/zHHs6X/2Bpz+mmHL6fNNNun5Y87gZ03pj8ao+Onn0TYp2wpzyB4WvvVJwMTDjbvRxWgQunQTYdEb/uUDk4kgFhe6vftn9HxMsHv2EzJy2b+i61s9nduo+PJ/O/awWZ/NyO/5wtKnii/vTvpeDL/9bso7RP8vj1s9inXQVjsH9jHZp+WJ+03NbDZjzdS2pfNTrSZlifst8bp7Zt4unUcSCQyOgcYSofOFIcuy0CM8q1qVCIRbl5m5QMoP5bxQ+75Z0w/7/rHmn7e5R96+zJ9r4BC4XToLG+4RIK+m2SUzJsDK+sAACAASURBVJuX2XjW/DOXH2P6eZcf7/If6vlXwl7+ooVuFUuBKIwtJNYWFjPP2EIIff4586wtNO7lz7mFy5r/IFrIvPkMRbgAYXXoLCtFebcwQ2+fdwuft33e5T/U7RNj4H543vVPDLmYxTRDnA1jaEPZPvcWWsj5DNx/qj3JbexT8az2bV4oVIdU2LSQqXibOSBW+0HKk0PfsGEDbrnlFixcuBDPPfec5e/PP/88brvtNtx55534yU9+ggMH6HYQpBYx5KLQzNTbtBAUmkgDmze0QhPpYMtT7Bhpx9NEetjlfw8Fb/dA7f6czj7RwlJoIm3s7NNE+tjkv58mUoqMcgCg0EQ62ZU/lX0bfsvHdDzZwt28ick+Pv3AO2/XQt/0PiVPOMSP/s3Gf/COd16WrUO+762j4BNsIwxplPabZVnG8uXLce+99+KRRx7B2rVrsXu32QEdcsghWLJkCR5++GGcdNJJeOaZZzKWYACWAvF64GriurlqmBXZQvi/vzsQNnzjfitPHrzrxh/YZ+Vf+qt3vqHeyr/4Zwp+r5V/YYV3fv8eCy//43+98/t2W/nnvNWXxHVz1TBDkn/2KSa+/XePOxA2/J4vrPZX/Dcb73FP/sR1c4Hdn1t5j3vqJ66bq4apEg5NfuYJT/z+r84Cdm63OGSve+InrpurhtmS9sktmd34L7ZaHKrXPe0T180FPt9iGfKQn/S2J3riurlqmLKF97Yne+K6uWqYNM8W+tatWzF27FiMGTMGsVgMs2bNwvr1603XHHPMMSgoKAAATJkyBfE4ZWwnrYhzAXEI3WpJiWyhHDqVL0+52tTCU652Ddq+VHUkHU84BGnq0Wz2p7Dx+cfMoOPJ9FOu+rOkn5Yn7R95HB1POCTpaLr8I2p2G9LR1XQ8af8YSp5wiNK0GocLPdqfdgKb/eNOdLjQyX7mWuhpB+Pi8TgqKiqSv1dUVGDLli2O169atQrTp0+3/VtdXR3q6uoAAEuWLEFlpb9lwxHi1JaS0jIMS/NdA19sg34URMGIIhiPNSguLcXwdPzO7Wz8rs+d+ZKS9PxuZ76opBiFNnwsFkuW8cCeL5z54hJb3mR/7y4X3t6+ia/f7ciPKCrGiHT8vj3OfHF6PtFQD30JCslHY3lp66KZH2HiC4tLUJSOP7AvxRcWEnxxer6xwZEfXlyC4nR80wE2Pm7ghxfC+AQOLy5Ozzc3GewPN/HDiktQko5viTvap+dJ+8Vpebm1GfpAMskXFBej1MAbn7sk39Zi4AtR5tP3pRPD7IpVa9aswfbt27Fo0SLbv9fW1qK2tjb5e2Mj5SovTRXF5nMN21pb0ZHmu+QPU+N0vcRy+/a2NnSm4z/a4My3t6fllU0uvAf7yqbUOCXJd7S1o8uGr6ysTJaxK9/ZYcub7H/yoTPf1cXEd3Z2ojst/1GKJzZn6+zqSs9/auCJY8RkpK+LymepcWqS7+ruQg8N30fwPT3p+c2fONrv7u1Fbzp+q5E3l193b096fltqRXUfab+vLz2/I9UI7Osz2+/p60dfOt6wopq074k3zLP0EfWnp38gPW+YJ+ojyq93IGGqP8bnLskb5qn6+vt9+z4AGD9+vOPf0rb9y8vL0dSUOuasqakJ5eXWDXU++OADrFy5EnfddRfy8hjCiryI6LJIXmb9jZEx5PVewqCM4/Ykn5fPxhcM88C7pJ88ONdOxjIjJ6W88Eb7JF9IHBxsyxvsk13OopL0vJEhw76KPWzQZLRPdJkjXjaIMuaftF9WgbSKONv3tMFVxMV++SiklVv5e9kbxi3/o8al543pJ4ZsMMYD72Z/7EQPvEv+x3vYoE1yqX/jD07Pm/LPcQy9qqoK9fX1aGhowMDAANatW4eaGvOY1Y4dO/Db3/4Wd911F0pLGXcv8yIyysXDBleSsRBJB+5hgy7JWAl98KZKHCV5D2FPxvSTDtXLBl8mnqhQCQ/2Iy48bf7JB8ILb3KIRLX1xEftf4a3+mPmg7Xv6f6xlr8b7+X+u9Uf5vzT1j8e5c9Y/9xeCAEqbdM2Go1i/vz5WLx4MWRZxuzZszFp0iSsWLECVVVVqKmpwTPPPIOenh788pfqAbiVlZX4/ve/n7FEW8J+vNwQyaWF6mXHRrcb4sm+S4X0Yj/i0sKgTj+jfUv5s9pnfSBp009MqjPnn9IhkJP6tLwl/ay8l/sn2f/smXdIix/75BaXtDuu+ik/07aiPuwbY+0zeOKapzH06upqVFebZ6LnzZuX/Pn+++8PNlVpJPlqIaUYZc0r5r9RPhDM/OqXzX+jfMMrr71I8F5aOAZ+1Qv09g2tMmUVESZK2cKyhJnS3j+S92Lf8BArrxH59/RCNvCrifL3whseaGX1S+a/ecm/wR8orxP1x5P9VBlZ6q8n+4b0v/FPwr6H8jfUUQvvxb4hj8raOh/2Dfy6V9nsv7XK8W+e7L/1GjD/tvSMD4VypahFXgrUOK7f1UHPG1v13Z0+eENR93bT88aXWH8fwVO2cMjrKR2S5QXiqYXkshrQC+/20mLkFWb7dA7Vynux72LDi323a5h5D+l3c7qUDtGX/YzyjOUXoHLDoVO+YX3xbtew2metEKGw7/LQULawgrfPyA9K+fO2n8H0D0b9czu20kv9c7IvSezlH6Byw6F7aiGkCjRy492Of/PE30DMD1Dal66/i96+odJI191Jzxvtf/sONvvfut2H/dRDI117K/E3L+WXuka69hZ6+4YWtnTNd01/8tZCN+T/qpsd0+YoY/l/8ybHv3nir1xAb9+Y/ituoLdv4r+T+tyrQzPyX7/ewEfo8z/vW6nPIx554/2/zMBHY/T379L5Bj5KX36XXJP+ep/KEYdO2cIjz4GkbaGTPKV9aZwP+8YKRYZp0VaocRMd/+bJPnkOJ619MkzMk33DAzmOOEeU2r4P3mjfwlPe/wmGMDfPDtFg38RH6O1PPCT1eTRKf/8nGPhIlN4hTjLaj9C/ECYdZuBj9PffeA6uL/sGPuKj/IzpD1jhduj6kmkPFcoUmqZP8Om8W3fMjT/8WH/2JUZej5JI8rRDRiRP90Amx8P1A7ppu9z6cLq+5J82/foGR5OPomjhDZh/jkTU82UjEU/338JHY+qWCVF6h4b+PjX09dCp3lt4Rht9vUB+voGntN/bDeQXqAeM+3DI6O0GCoarB4xHo/RDFl2dwLDh6gHjPlrI6GwHho8AJh6qpZ+y/Dra1LUTEw72/kIwToq2t6qHQ48/iIJP2VfaKI+to1CgK0UHU5H/+H9AXgHkBV+jH8PKL1D5/ALIN37NW5SILT8M8o0X09svKEDkP/5Xs38xfQ8jf1iK95h/xei0hhn5SzzZV4w2CoYj8p9/BPLyId90iTf7RhvDDfzNl9E/kMMLtfufB3nhPOoWPoYXIfKozn/dnDcnGa8pLELkkWdU/pbL6e9/YREiv3xa46+g7yEWFSOy9Ckglgf51m/Q2y8uRWTp74FoHuTbr6Svf0UliDz8pMrf8U36ScWSMkR+ofNXeXv+TPxIRB76HyAag3zn1fT3v2QkIj9frvLfu4Z+yKe0HNLP/huIRiHfdS39kE+ph4VsPhXaFrpUWJRaMWl8ew4MQHn3Tesey/pNP6YaUvkoSIVF6iZJkmR+ew4MQF7vwh89A1LFaI2PWcbw0vJHHgepcgykwhEqT7SwlETCntdtHH4spNHjUnzEI69XqKnHQBo93r/9yUdCGjsB0vBCjTe3sBQ5AXn9G875rzoC0tiJBp4oPwc+2UM57HBI4w/S+Dxn+4STSDrtgydDmkDwAx543SEcVAVp4iEpnmghKrJsyyfzP+lQSAcdRqTfA6/bn3AwpIOqIA3TeUr74w+CdPBklc/Ls9afJE84Kd3GuEmQDp2a4okhG0de/33sBEiHHW7maeyPHgdp8pGQhg235xVFrb9O9keNhTT1aIJPeOcrRkM6/BiNz7fWP0VBz9pXrYvVdL68knozNRqF1qED2kEX5APx4p8h//ohYCOxx7F2TeTqhebPyQr58l+g/OYh4P23HXjzhJrlgX75ryr/3lv2/DU2vNH+P1eq/L/XErxaaSLkhCLR5VX++RyU3zwEZf0b5us0G5H5NrzRft3fVP6dNfbpJ+NnI4T9ur9D+c0voPxrtX36yQnVaMxsf9XzKk/G+moOysJHouYY59detOd1++SEcjRieniV115SeTJW2ZEn6t+al1X+TXOstf6AR67/nsW+iX/jnyr/Bslr9r9DTKiT9edNjV9jjlVP2id58v6ve1XliVj5FE8EBJD1/61VKk+ulXDiSYf69msqv+p5e54MaIgS9/9fq9X6W0dsia2X3433WNNvzP87a1T+//5mb3/Bve78+jfQ+vD9UP5JnBuh21/wA2RSoXboACwPBJq1zajaW83X6YVOrjIkx+DiaXhylSc5htbcpPEt5usSDvbJMVTH9MsO9okx0BZt3502v/Yd+GT+7Xij/TidfXIMV+dbGe23EuOU+kuHXJQWjQEDhvzr45sk72SfjLLQuRbSvlP5E2PIrXFzPkj7trwx/S32vNf7n0x/k/k6r/Vft99s5pMtVtv7Z8eb06+4Pb/G4cS2Vgf7cup6i30D326ffvf6Z0i//tw2E5tvJRye34CVAw7d3MJLLqBxGjIgl/2TFZKaJ2bJ9fU75Doa2YG3TOpoXyA7DFlY7Eft808mwOmBImfpqfPvkU++kOweCOPwgFP6B1x4u/L3WH6WSUE9/eSQh9MDTdQ/p/S72ffCJ5zKj5wUdip/l/Tb1n/zZY733xI2mC7/XuufU/n7rX9uDtmm/jkN+aVrEDrxTvc/YIXfoTvO0js5JLKF4bFCem1hJZfoe7VPOqQ0Di1thbLHPT/Q1Pl3Sj8hJ4dM3j/HF4pbC8su/R7tR6P2EUQW3i3/xvRr/zs6RI/335J/h/tvqX9p0p+uh+HHvpcGjevz46X+6Hy68rPHne+/Q/k5Nmj81h+H8gtY4Xfo5BtWStPC9Vsh3bp8xgkonfds32sLxWnIhbKHYbFPPtCw55263LQ9jLT5T2ffbwvPY5c7bQ/JZw/D65Cfr/zb1H+nFm66+u+YfuchP8XWIXq0H4kQE4j0PQzFLv1e638k4o13u/+2vFP9E0Mu7iJbePqNcNrRLl2XW7+PTjuy2d3QAQb7ZAtBf3DIHf0cu+yEQ05rP00LxSn/bkM+dvbJhorjkAHJa/877ciXroeRtO9UfmlaWPqD6Jj/ND2MpH2v95+0DwfeufwUO/vkDXC5f2aesv6RL7R05R9LV/8c0u91yMep/B2H/Mgearr7l67+pas/YsjFXS1Npt3XkiebaBVKaWmC0tPl3GVrboSy9lUrrzlEpSUOpduFjx8wRVQoX2zTfnLgyRZOU4O6+5rO79qu4STv0GVraoDyth2v5b+1GUpXp3ML8cA+KP963crr6df5AQf7B/ZBecfA795hb9/JITfUmyJqUrzRfodzC3v/HlNET/JkGZ1v03kH+/v2oNd4/x15h/LbtxuKISJJ2aPzkRTf2eF8/+t3mU6Nt/ItGu9Q/nt3miOq9nzhwDvY3/OFOaJLP1knmf809nd/Dmww8HsJvr0VSme7c/nt3mGOSKvX+IhHfhfJ7zLnP8k73P+d24APDGck79X4iM63Qeloc7b/xVbgw3cN9ncT+dd5B/sBK7QLixy1c7vpV/kH3wHyCiCdfg4Qiaihjm5KHnUlpfhYDNLs8wEpYt26l9SOzRqu8ffdCEQkSHMu1D5Pw+tHhen8DxcAsgLprK+on6ezv3mTOf33LwDkBKSzv6p+nK5CfaYdFRfR7d8EDPRBOucSjU9j/+MN5vQvuhno6YF0/qXa96ax/9F7BL8Q6OmCdMHXvfH6w6WVs7zou0BXB6QLL/fG685Bt/+TW4H2Vkhf+Yb6ebry052jPnLzk9uAtmZIX/2mN/v/Xme2/+DtQHMjpK9d7cl+8uWm37/FtwPxxtT+IWnsJ1/Ouv2f3gk0NUC65FrNvvv9TzZOkvbvUHl9/5N09vWXq27/Z98DDuxL7b+Szr4e7qmX/5K7gIb61P4v6ezr4Z66/SV3AQ17IX39Os1+Gl7fWlmvfw99H9i3J7V/TYbH0HPPoevS34h9feq/RIKuu6O/kft61X9ygu7tqne99K1yNftpXyi69DHc7i7td9W+Z17vOupb/coJby80UvpWw3ICkCTLifGO0u10tKv/U48h6nybxjt0edPgyTAypy63k/R06mF8TkMujvZ1Xgu/cxoycOS1DOjhb9T51/g4yVPab2pQ/6ceMmDk9fI7sI+Nb6hX/3casnL+Ao3fq/E+y3/fHvV/2vrjU+Efcpl+kronAylifw7llWft9+yoPtmeT5gPolVe+isw0G+9rnoWMG6S9fN+kv+LfTTO8Q48af/FP9tGs0g1p9ifqUjm/4U/2S6xlmaeat1szI5/foV1okjnR9scWmvh/6heT7xQpBNOB0bbnClJrNRz5E88HRg11oYnVnrqPHke7UlnIDrGJv0WfoUGEPzJs4GK0VZeceBJ+yef6elMUOWFP2m82SFIs+YA5enPJFVe0OwTLVzpS3PszzQlytnR/ilnAXZnsjrxpP1TzwbslsIT7Q7lhT/b2z/1bKB0ZHr+Rd0+wZ92DlBsc2wmyb/kYP90B56Q8uKfbe0HrfA79Ihk62hsna/dIQOSZL+XRL+N87WRJFHYt+Uj5nTpDqPfG29Jv74dgkf7kCSz/fx8Sp5If8FwOj5CpL9whPo/mX+7MtbtG/kRxfb2nfYLkSTzMnn9wGqS116mlh4Omf8S7cBqsvGgvcwtPFl/dedG8tqhJpYhP0kyR+ToB1aTfF9fKr0m+1Fz+vWXA9n46O3Rrrexb0y//nIjy7unO2XPwhuu1Q+sJnm9p0kOuZD3X3+5k9Wlq9PePln+euOEvE96T9OSfuL+640jspz1nqYIW3SXRN5QvbXa2+uNJyu0vjVqX4+3BEQI+3pr36N9K6/bp+Dt0t/rM/1jtd6C3/SP1So0eSqTk6SI/QNFk35j/vXWvt/86w6hh4Y3pF/nPdsnwg4rNYfotfzI/Cd5e/vWFxLxQtUdskP+rS8kovwqtN6GQ/otLySS13srTum3CTs0v5Dc7VtfSKR97YXmdP/t4vjteCf7MRHl4iqlcb8aaaA7Ba1w9S62N35PqpWmtcSSXeR0fPwA0LDXwKstm2QXzxNfb7Wvd9G88Af2pXitZau89BdvfHMT0Lg/xWstU+Xlv3rkG9VIG5J/ZaU3vqVJ4xNm/p9e+TgQb0zxev7JvTSc+NZmyEb7+v2r+5sLZebR0mS1T+4l4sa3xlOhi1rL2nJuqhPf1gK0Nlt5ci8UR74VaGux8uS5q058eyvQ3pri9fyTe7m48R1tqd1A9ftPnrvqpI42oKPdypPnrmqyvJA624GuDiu/hobvhKL36PT8k+emOvEBK/QOPRmV8r4WumXoKiv7dpsujdz7sJXXo1L00DEjX0/wdz9k5bd9ql6rRxf0U/JaVEsyusBkf5eZ//4SK7/lY/VaPXTRwA/s2mHm77LhtaiWZOin0b4ewqbz3/upM69vRmXcuVAPodP5Oxdb+U82qtfqBxeb7BP8HQ9a+Y/fV6997SUrr4cg6vztD1h5LapGeVVzgMb7t5sov9t+bOW1qBrl//5utU+W/602vBYyl3wBufG3/MjKa1E5yRewkd+5zXRp5Ls/tPJayGFyjLvfmS/7wS+svBYymWxAGe1/Qdi/+T4rr0X1KP/4Xxt+q+nSyE3ExlgAlHffVP9/7hmNN9S/z7eYeXJjLSAZMqusfEr9wJh/kic39gKSIb/KX35nTf8OgidPOsuAwu/QNSmffKD+YChQ+XHCgQwvdOE32vCEA3Dh8YkWrmeoUPJjPzFfM2y4M79J4w0VSn50kfmaAjf+fYv95h8TOyPmFzjiyiYtXNCY/0cJB5Dnwn9kw//yfvNFsTw4Sfnw3+oPxvwvJRyAS3dV+ehdq/2HiZ3tXCaklI/+beV/YbOznhP/oY39h4idAV1aZ4oeC23M/8+9O4Akb7S/hOCd5iGc+J+ad4a0bMlr/NsGLdzTWP9/6v2oQ2XDv9QfjPl/0PtRh8pGjTemf7HZvkLOixj/poebuvEuw6C2PJF/xeswJoNyx6Hr8Z/Gm0beQLtoDJ3XW4j9LrxdNInO6/GzJvvExJJdNIvO27SwLRNDdtE4Oq8vznHjjUd/kdLjn/td+IMPc+ZtKrTFgRwyxZlPOhSbVYu6Dj3cmbd5oVj4w1z4j60vZEv+Jx/pzOvx+275n3K0M7/1YytP2tdPiLLT9s/U/93u3+HTnHm9NexSfgXTapx5vTfjlv6jXPYB1xckuU2mHznd+W96eKBLMIF09AxnXg+PdDm9Sjr2eGc+fkDjXewf61J+ASlnHHpSxgIl3qhpFwUFzRMTO4PNKz1dBO9hht1YoS32PfDGB6rHPDEkeQnZMuafmJii5sn0u/QQAuP7Xfg8SvtaZEuSd+lhpXjiqDwjX+CFd3bIklsPU5ebQx3m0sNN2ndxqHoElCvvYr+wiI3XI6jc5BadVuSBZ1TuOXRjuKFhT3HplLO88cYbqocqQYvXpeX1RTnQ4oVpeT1UC1q8shcZxwC7Uw5dOmk2vX2DQ5JOOoOeNzgk6cTT6XnDz9IJp3njjfff0EWXZp5Kb9/QwvbO2zsk6fgv0ds3qnoWGz/jJDb+uBPYeK+tUyf+GJfWsRf+KJfWuRfe6ZQhMjzR6YV0+LEZnxAFcsChJycaikrUPRvsYs0Bx/HjyA3aOGdRsbpnhdM4oROvn6Ayoljbs8RhnM+R1yZaCkeoDtipQji0rpITPcNHqC1ypwrpZF+faBpeqPJEqzA9r41TFwyH0tPtHG7pxOsTZQXDVN4p3C8v34G/P/n9Sm+Pc7iYE7/w/uTfld6e1MpcUg4rFJMTjbE8je+0vc6Z1yY6YzF1jLXLnnfqnSQnSqNRle9st71OcrJ/yyLth4jKd7Tbj/U78fpErySpY8wdbXS8YaJZ6etVG2F222M45f+21DyV0terHpBh15N14g0T5Up/n7oy2O5aR/uG9Pf3qSuD7eZ6MrwpV9LMokWLFg2KJRu1t9tXvnQqLCxEV5f64EnjD1Jbwju3Q1n1D7WFeszxqSW/uvbuQuS8Sy3fJY2fpPHb1HH4vl7gqOmpMTVde75A5PzLrPy4SepD/MVWKG+8ojqkI48DGvebL9y1A5EL5tnzPV3A51vUSJOeLuDwY1NLpnV9sQ0RfT8SEz9RdWI7Nqv7aHR3ApOPSo3pJfmtiMy14cdOVNO84zMo77yhOoSqI6wnrnyxzZnv6wW2fQrlvXXqA33oVOuJNzud+AnqS2TbJ+rEWHurehp9K3Hizq7tzvxAH7D1EygfrlcfyEmHWk9M2rXDnh8zAYX5eej/eIM6sdsaV+cqyBOjdn/uwI9XX+KbN0H5ZIOa73GTUgtJdO35wpmXE8Dmj6B89oFa7mMmWB3z3p32/Ojxak/isw+hbNmk3vdRY029Q3d+HACN3/qJWu8qRltfTPW7UPT1byWfOzMPld/+mVrvyyutL8Z9u+3tjxqnLu757EMoOzarfOlI64t5/x4HfqzqwI18UYm1YdGw15mPRoFPP1CjUg7sA0YUpRZiJfl6Zz4WU/kvtqr7xgwrtDaMDuyz5f2ouNh56Cb0LXQAaoEO9KdaN92dqc18dCnOM+TIy1dfBPpD2N0Fad63zde4nSyel6fa1/f96OyAdPn13vmYxutOtKMN0hU3EOl3jlBALF/l9ZdAZzukb9zofL1d+vsHUvtWtLVA+uYCOn6gP7XTXWszpKtupuQHUjsFtsQhkWevuimWr44X6xuzNTdCuvYWCvv5as9MnxhsaoD0rdvcGRMfU3uGeghs435I377DnSHtK0oyBBYN9ZDIs0vdpLcItRBWHNhnPTvUldfG9jd/pP7f1EAXYqf3fj7VIs3ijbYhfmnt65FmrfFUz8+TfYJvb7UPkUzHayGw6Gi3D/F0kp5+PQS2qyPV8xlk5YZDz8tX34haFIl0+rmWMUuLgzRKd6haFIl0hg3v5iBjmkOaeKh67ZkXQCLGPKUrXfi8fLWVN0nj51wIqfpkwr5L+vNiqkM6eLJ67VlfsfLkC4ZMvyIno1Ckcy6GNIPg9d3mnHggxZ97CaTp5jHb5G57bvyhU9Vrz78M0vQTzby+25+d9Aey6gj12rlXQDqO4C++2hGX9O0OdP6iKyFNm2m+5qIrXexrvBYFI138TSv/lSuceT3/U9UoGOmSqyERESWSTe/MYj/JX2MZc07uVunGH36slv6rrLxN7zbFay+UI9QoGumiK4Fjqs38uV9Lb18bp5YuvBwgIlKkL3/VmY8R/AXzLGPmyd1KbXmt/DVGOu8yS0SNVDs3ffq1NBd+9UrLmLt05gXOfIDKjd0W9RuitRAjJ882TQgCcBybBJByCFoLMTJrjjqea5TD2KTKazdUC92KfGmOOp5q4okusFF6+rWFJJFTz7bGrHZ4sK+1MCOnnm2NmSWHAOx4bSFF5LRz1PFAo8ghCBOfZ+bPONfKe7GvtXClM8617iVCHrptZ19r4UpnnGflySEYg5LRKzp/5gXWuRC3/Ou8tkhMmjPXOhdDHnptl35t62PprItseOLQaTf+7K9a55LIISyj9PRroZfSOV+z8uShySbe3EKXzrvU2qN05c0tbOlCm5ePG6+/UHR+rvXlqZBDkCZeS7/WQpe+coVlHkAhh0CN0ntI2lqQom98B73N5vvlygeo3HDoNuFgErkIyMsDaeTJEC1avmCY+QMXh2KbfnIS1I23s09OQro5BDuenER0dSjWCUcqnrAvSZI1Tc0eHJIbT47pG0WUlcqbHw3F1aHY2Ccm0RRyTsIou/STvJtDsuMlgm9yc2geeFeHaMcTDrHRxaHZ8YTc2LhGTAAADS5JREFUHaq5rtlGk7jZJ8vPblL1wH7rZ7qIum47gU3OqWVIuTHk4iE+2DXszSECwsS7hd15iC92Dfvzkv6TXcIOvdh3C5tk5b2k3y3s04v9U2pdeA/379Sznf/mwX7EhXfMv8GxRE77sjPvlH6DY4mcca4L72DfEFkRme3CO6XfkK7I7PPoecOL0nXIwYk3rIx25Z3yPzwVty7VXuiIO95/Q9y6dJbzkIsjb4g7dx3yCVC50UI3tKaMjk/69h1Q/vU6pMrRqV0I0/EGxytddyeUt1dDqhgNTDzEhU/dUGO8sonXxsdtZagQxrF76frvQXnrNUgVo4CDqzzZN8Yrl975IFpfeQ5SeSVwqMsqTSNvGLuOfOcuyOtWqbzbKktjhTaMHUdu+D7kta9CGlkJVLmssjTaN8QrR264G/LaOpV3WyVp5A1jp5Eb74b8Zh2kkRXJ8WE7mXoThrHPyI33QH7jnyp/hMsqSyNvsBO58R7Ia15Rebc4aGP6p6ZWk0ZuvBvy6xp/dLUNmEp/coBDmwcAtPJ//WWVd4njNvHaPAag3f/VL0EqK3eNQ5fy8lK8YTVw5Pq7IK9+EVJZOaQZJ9qyFvsHpVYjR66/E/JrL0IqHWmZ03G0b3hOI9fdAXnVCyp//CmOvKn8DauxI9++A/Kq5yGVlrmvQzD2EAyrySPzb0/xXtdRMCo3HLph+1Lpy6nJl8iJpwNeFrQYF5CcY+BPOA3wciOMvGHyyDNv2E1fuiAVGhmZeSrgaUGLoSVoCK0c9qUz0eG23DsJGVqChvFLqeYURGtcHoTkhQbeMH4pHf8lRL0sqDG2JA2hXdLxsxA9Pv2CGikWS50tfdE3Up9Xz0LUy4IcwwMdMUx+StUnI1rt7EiS10UN9vWj5gBIM05C1MOCHinPwBsmb6XpJyE63cOCIGP69aPmAEjTT0R0urMjtedTk8/ScScg6mVBkbElf6mRn4nocTPtCGf7hug0adpMRKcx8MfWIOplQZMx/Sb+eETdlvsneUP9NQQPeOYDVG4MuZgOWPCwvJmUcQLKy/JqN/t+eGNIo8sGWJ7sexh+sNpn5I0TYF6Wx1vsG/LvY79oxbgfua8FHKy8Qb4OMDCM+Xo+os+IG/h0Z9bayWgz4mM1oynPPnjjmLOf1ZTGe+aHj7Hyhjqf+cWgrvJUezds2IAnn3wSsixjzpw5uOiii0x/7+/vx+OPP47t27ejuLgYt956K0aPtjmWK1Oy26CfRnYnpvi1b3ccGo19u+PYaOzrB0z4te+yAZkn++OdNyDzZH/CIT7sG14IbkNbjrzB/kEuG5A5yZj+Qyaz2T+YkTcMmfjiDzvC+TovvNsGZk4yNggmH8XGT3XZAM0Lf7jL0J4XuQ3NDYLSvs5lWcby5ctx77334pFHHsHatWuxe7d5n+9Vq1ZhxIgReOyxx3D++efjD3/4Q8YSbCstIkWqnetvvwRt8kU68wJ/vG5/9nls9k8/h83+qWf74vWIHOmUs/zxwzT+S3N82tfSf/Jsn/Y1/qQz2PJ/wun+yj9f42ee6vP+a/zxX/K2AZvFvtarqz7ZH6/3aqefxMYfdwJb+o+tYeOPqfZ+iLkdf9QMf7zeqz7yOH98gEq79H/Lli3YuXMnzj33XEQiEXR2dmLv3r048sjUm3jFihU4//zzUVFRgQkTJmD58uWYOze9cw1i6T8AtVU40A/p/HmpRSI0GjdR5S+Y521HO1JjJxrs++EnAAMD/u2PmaCl/+umcEdLOTlp9HiVv/Dr1nBLLxo1DkgM+OdHj1Pz79v+WIN9DzsCEio+dDK62tsgzfXHY9QYzf7l3nYkJFU5Gkgk/PMVowBZRuTCr3vb0ZBUeaXKz73clXesTyMrAFlRebczA5w0siJl3w9fVgEoOu9hR0ZSpeUAFAZ+pMZfAalwhPfnzqfclv5LiuK2phx4++23sWHDBtxwg7pScc2aNdiyZQu+9a3U5MEdd9yBe++9FxUV6gG1CxcuxOLFi1FSUmL6rrq6OtTVqSfjLFmyBH3kfgkeFYvFMOCyzaaQKlFO3iTKyZtEOXlTpssp36XROqhRLrW1taitTcUTNza6LLZwUWVlpW92KEmUkzeJcvImUU7elOlyGj/e+aCetANW5eXlaGpKrZJrampCeXm54zWJRAJdXV2u3QIhISEhoeCV1qFXVVWhvr4eDQ0NGBgYwLp161BTY47tPP7447F69WoA6hDN0UcfPSibuQsJCQkJpZR2yCUajWL+/PlYvHgxZFnG7NmzMWnSJKxYsQJVVVWoqanBmWeeiccffxwLFy5EUVERbr311sFIu5CQkJCQQWknRTOpvXv3+uLEWJ43iXLyJlFO3iTKyZuyegxdSEhISCgcEg5dSEhIKEckHLqQkJBQjojrGLqQkJCQUHAKZQv97rvv5p2EUEiUkzeJcvImUU7exLOcQunQhYSEhISsEg5dSEhIKEcUSodu3A9GyFminLxJlJM3iXLyJp7lJCZFhYSEhHJEoWyhCwkJCQlZJRy6kJCQUI5oUPdDD0LpzjfNdd10000YNmwYIpEIotEolixZgo6ODjzyyCM4cOAARo0ahdtuuw1FRUVQFAVPPvkk3n//fRQUFGDBggU47DD1zMzVq1fj2WefBQBcfPHFOOOMMzjmil1PPPEE3nvvPZSWlmLp0qUAEGi5bN++HcuWLUNfXx9mzJiBa6+9NpQ7itqV05/+9Ce8+uqryQNpLr/8clRXVwMAVq5ciVWrViESieDaa6/F9OnTATg/hw0NDXj00UfR3t6Oww47DAsXLkTMx8HfvNXY2Ihly5ahpaUFkiShtrYW5513XvbXKSVESiQSys0336zs27dP6e/vV+68805l165dvJM1qFqwYIHS2tpq+uzpp59WVq5cqSiKoqxcuVJ5+umnFUVRlH//+9/K4sWLFVmWlc8++0y55557FEVRlPb2duWmm25S2tvbTT+HWZs2bVK2bdum3H777cnPgiyXu+++W/nss88UWZaVxYsXK++9994g5zAY2ZXTihUrlL/97W+Wa3ft2qXceeedSl9fn7J//37l5ptvVhKJhOtzuHTpUuXNN99UFEVRfv3rXyuvvPLK4GQsYMXjcWXbtm2KoihKV1eX8t3vflfZtWtX1tepUA25bN26FWPHjsWYMWMQi8Uwa9YsrF+/nneyuGv9+vU4/fTTAQCnn356skzeffddnHbaaZAkCVOnTkVnZyeam5uxYcMGTJs2DUVFRSgqKsK0adOwYcMGnllg1lFHHYWioiLTZ0GVS3NzM7q7uzF16lRIkoTTTjsttPXOrpyctH79esyaNQt5eXkYPXo0xo4di61btzo+h4qiYNOmTTjppJMAAGeccUZoy2nkyJHJFvbw4cMxYcIExOPxrK9ToeoLxePx5LmlAFBRUYEtW7ZwTBEfLV68GABw1llnoba2Fq2trRg5ciQAoKysDK2trQDU8qqsrExyFRUViMfjlnIsLy9HPB4fxBwMjoIqF7t6l2vl9corr2DNmjU47LDDcNVVV6GoqAjxeBxTpkxJXmOsJ3bPYXt7OwoLCxGNRi3Xh1kNDQ3YsWMHJk+enPV1KlQOXQh44IEHUF5ejtbWVjz44IOWvZElSQrl2G6mJcrFWWeffTYuueQSAMCKFSvw1FNPYcGCBZxTlR3q6enB0qVLcc0116CwsND0t2ysU6EacvFyvmmuS89vaWkpZs6cia1bt6K0tBTNzc0AgObm5uTkVnl5uWmjfb28yHKMx+M5WY5BlUuu17uysjJEIhFEIhHMmTMH27ZtA2B93tKVR3FxMbq6upBIJEzXh1UDAwNYunQpTj31VJx44okAsr9OhcqheznfNJfV09OD7u7u5M8ffPABDjroINTU1OD1118HALz++uuYOXMmAKCmpgZr1qyBoijYvHkzCgsLMXLkSEyfPh0bN25ER0cHOjo6sHHjxmT0Qi4pqHIZOXIkhg8fjs2bN0NRFKxZsyan6p3uoADgnXfewaRJkwCo5bRu3Tr09/ejoaEB9fX1mDx5suNzKEkSjj76aLz99tsA1OiOsJaToij41a9+hQkTJuCCCy5Ifp7tdSp0K0Xfe+89/P73v0+eb3rxxRfzTtKgaf/+/Xj44YcBAIlEAqeccgouvvhitLe345FHHkFjY6MllGr58uXYuHEj8vPzsWDBAlRVVQEAVq1ahZUrVwJQQ6lmz57NLV9B6NFHH8XHH3+M9vZ2lJaW4rLLLsPMmTMDK5dt27bhiSeeQF9fH6ZPn4758+dnXXfbi+zKadOmTfj8888hSRJGjRqF66+/PjlO/Oyzz+K1115DJBLBNddcgxkzZgBwfg7379+PRx99FB0dHTj00EOxcOFC5OXlccuvX3366af44Q9/iIMOOih5ny+//HJMmTIlq+tU6By6kJCQkJC9QjXkIiQkJCTkLOHQhYSEhHJEwqELCQkJ5YiEQxcSEhLKEQmHLiQkJJQjEg5dSEhIKEckHLqQkJBQjuj/A+ewcy06uZ89AAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":["t1 = pd.DataFrame(predict, columns=['predict'])\n","t2 = pd.DataFrame(actual, columns=['actual'])\n","\n","pd.concat([t1,t2], axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"i06T7d41btYs","executionInfo":{"status":"ok","timestamp":1655364936244,"user_tz":-540,"elapsed":5,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"c1185942-6186-4204-cc7e-6375c14161e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       predict     actual\n","0   3031235.50  3038000.0\n","1   2902678.75  3095000.0\n","2   2805710.75  2884000.0\n","3   2724697.00  2648000.0\n","4   2654125.50  2694000.0\n","5   2593605.75  2733000.0\n","6   2542449.50  2806000.0\n","7   2498977.50  2660000.0\n","8   2461608.75  2695000.0\n","9   2429323.50  2472000.0\n","10  2401422.75  2583000.0\n","11  2377282.00  2538000.0\n","12  2356321.50  2552000.0\n","13  2338057.00  2627000.0\n","14  2322102.75  2533000.0\n","15  2308141.50  2525000.0\n","16  2295901.75  2494000.0\n","17  2285151.50  2314000.0\n","18  2275693.50  2225000.0\n","19  2267361.25  2286000.0"],"text/html":["\n","  <div id=\"df-d442e1be-3c15-45d6-bf60-5e6b2d9cc0bd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predict</th>\n","      <th>actual</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3031235.50</td>\n","      <td>3038000.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2902678.75</td>\n","      <td>3095000.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2805710.75</td>\n","      <td>2884000.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2724697.00</td>\n","      <td>2648000.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2654125.50</td>\n","      <td>2694000.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2593605.75</td>\n","      <td>2733000.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2542449.50</td>\n","      <td>2806000.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2498977.50</td>\n","      <td>2660000.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2461608.75</td>\n","      <td>2695000.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2429323.50</td>\n","      <td>2472000.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2401422.75</td>\n","      <td>2583000.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2377282.00</td>\n","      <td>2538000.0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2356321.50</td>\n","      <td>2552000.0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2338057.00</td>\n","      <td>2627000.0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2322102.75</td>\n","      <td>2533000.0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2308141.50</td>\n","      <td>2525000.0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2295901.75</td>\n","      <td>2494000.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2285151.50</td>\n","      <td>2314000.0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2275693.50</td>\n","      <td>2225000.0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2267361.25</td>\n","      <td>2286000.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d442e1be-3c15-45d6-bf60-5e6b2d9cc0bd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d442e1be-3c15-45d6-bf60-5e6b2d9cc0bd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d442e1be-3c15-45d6-bf60-5e6b2d9cc0bd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["plt.plot(data_df['pred_price'][len(data_df)-lookback_length:], label='prediction', c = 'blue')\n","plt.plot(data_df['trade_price'][len(data_df)-lookback_length:], label='actual', c = 'red')\n","plt.suptitle('Timeseries Prediction')\n","plt.suptitle('Timeseries Prediction')\n","plt.axvline(x = len(data_df) - len(predict), c = 'g', linestyle = '--')\n","plt.legend()\n","plt.show();"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"S88bO8YFekae","executionInfo":{"status":"ok","timestamp":1655365794960,"user_tz":-540,"elapsed":5,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"cb3ca099-42f5-4a33-fed6-4791aee364bb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"oeIbqVgJerQz"},"execution_count":null,"outputs":[]}]}